{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d5be150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %pip install pandas\\n%pip install numpy\\n%pip install scikit-learn\\n%pip install seaborn\\n%pip install matplotlib\\n%pip install skorch #for wrapping pytorch models in sklearn\\n '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" %pip install pandas\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install skorch #for wrapping pytorch models in sklearn\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45464378-7b13-4150-a871-746e983f1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "163134e8-fc53-4a08-81b1-740ea714ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_data = pd.read_csv('sph_dynamic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95691140-3053-4489-b915-af3db738a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = pd.read_csv('sph_static.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75f38196-d822-41d8-8698-7382ec2e0e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id                   0\n",
       "charttime                 0\n",
       "total_protein          6930\n",
       "calcium                 933\n",
       "creatinine              261\n",
       "glucose                 444\n",
       "sodium                  214\n",
       "chloride                241\n",
       "heart_rate             6833\n",
       "sbp                    6895\n",
       "dbp                    6895\n",
       "mbp                    6887\n",
       "resp_rate              6832\n",
       "temperature            6974\n",
       "hemoglobin             1179\n",
       "wbc                    1207\n",
       "alt                    3964\n",
       "ast                    3936\n",
       "alp                    3976\n",
       "bilirubin_total        3957\n",
       "bilirubin_direct       6808\n",
       "bilirubin_indirect     6812\n",
       "ph                     7004\n",
       "lactate                7012\n",
       "pt                     3068\n",
       "urineoutput            6942\n",
       "sofa_respiration       7005\n",
       "sofa_coagulation       7023\n",
       "sofa_liver             7023\n",
       "sofa_cardiovascular    6872\n",
       "sofa_cns               6979\n",
       "sofa_renal             7024\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values in Dynamic Table\n",
    "dynamic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b4c0670c-b56b-40c0-bbf3-8092bdf0c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with more than 80%\n",
    "for col in dynamic_data.columns:\n",
    "    if dynamic_data[col].isnull().sum() > len(dynamic_data)*0.8:\n",
    "        del dynamic_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5463f9bf-a76f-4c9c-85b7-b95c63f1b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id               0\n",
       "charttime             0\n",
       "calcium             933\n",
       "creatinine          261\n",
       "glucose             444\n",
       "sodium              214\n",
       "chloride            241\n",
       "hemoglobin         1179\n",
       "wbc                1207\n",
       "alt                3964\n",
       "ast                3936\n",
       "alp                3976\n",
       "bilirubin_total    3957\n",
       "pt                 3068\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90ba5554-d3a5-4578-ae46-449b46ad3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['alt','ast','alp','bilirubin_total','pt'] are liver function related test results\n",
    "# create a new binary column 'liver_function_test', True/1 means have ever taken liver function test\n",
    "liver_test_result = ['alt','ast','alp','bilirubin_total','pt']\n",
    "def liver_categorize(group):\n",
    "    flag = True\n",
    "    for i in liver_test_result:\n",
    "        if group[i].notnull().any():\n",
    "            flag = False\n",
    "    if flag:\n",
    "        group['liver_function_test'] = False\n",
    "    else:\n",
    "        group['liver_function_test'] = True\n",
    "    return group\n",
    "\n",
    "dynamic_data = dynamic_data.groupby('stay_id').apply(liver_categorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e08376e4-8ca0-4718-ae71-71a6c8ebf390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>calcium</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>hemoglobin</th>\n",
       "      <th>wbc</th>\n",
       "      <th>alt</th>\n",
       "      <th>ast</th>\n",
       "      <th>alp</th>\n",
       "      <th>bilirubin_total</th>\n",
       "      <th>pt</th>\n",
       "      <th>liver_function_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35715575</td>\n",
       "      <td>2148-12-27 18:15:00.000</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>137.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34483718</td>\n",
       "      <td>2118-01-04 03:58:00.000</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>129.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31826892</td>\n",
       "      <td>2163-03-10 19:59:00.000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36154799</td>\n",
       "      <td>2131-12-02 19:14:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32732521</td>\n",
       "      <td>2116-08-12 12:45:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>31292653</td>\n",
       "      <td>2192-03-18 03:14:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>102.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7020</th>\n",
       "      <td>32964221</td>\n",
       "      <td>2127-01-30 10:00:00.000</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>112.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7021</th>\n",
       "      <td>33493321</td>\n",
       "      <td>2142-07-28 06:02:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1</td>\n",
       "      <td>130.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>38658392</td>\n",
       "      <td>2189-05-17 00:13:00.000</td>\n",
       "      <td>7.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>37805633</td>\n",
       "      <td>2172-07-28 21:25:00.000</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>155.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7024 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stay_id                charttime  calcium  creatinine  glucose  sodium  \\\n",
       "0     35715575  2148-12-27 18:15:00.000      8.5         0.9    137.0   138.0   \n",
       "1     34483718  2118-01-04 03:58:00.000      8.2         0.8    129.0   141.0   \n",
       "2     31826892  2163-03-10 19:59:00.000      7.7         0.4    112.0   136.0   \n",
       "3     36154799  2131-12-02 19:14:00.000      NaN         NaN      NaN     NaN   \n",
       "4     32732521  2116-08-12 12:45:00.000      NaN         4.0    135.0   139.0   \n",
       "...        ...                      ...      ...         ...      ...     ...   \n",
       "7019  31292653  2192-03-18 03:14:00.000      NaN         1.4    102.0   137.0   \n",
       "7020  32964221  2127-01-30 10:00:00.000      8.6         0.5    112.0   139.0   \n",
       "7021  33493321  2142-07-28 06:02:00.000      NaN         1.1    130.0   142.0   \n",
       "7022  38658392  2189-05-17 00:13:00.000      7.3         1.0    174.0   133.0   \n",
       "7023  37805633  2172-07-28 21:25:00.000      8.3         0.9    155.0   144.0   \n",
       "\n",
       "      chloride  hemoglobin   wbc   alt   ast    alp  bilirubin_total    pt  \\\n",
       "0        104.0         NaN   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "1        101.0         8.7  11.3   NaN   NaN    NaN              NaN  12.1   \n",
       "2         98.0         NaN   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "3          NaN        12.3   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "4        105.0         NaN   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "...        ...         ...   ...   ...   ...    ...              ...   ...   \n",
       "7019     103.0         8.7   4.9   NaN   NaN    NaN              NaN   NaN   \n",
       "7020     107.0         8.9  14.3  14.0  32.0  148.0              2.6   NaN   \n",
       "7021     105.0         8.4   4.0   NaN   NaN    NaN              NaN   NaN   \n",
       "7022      93.0        13.0  19.5   9.0  18.0   48.0              0.5  13.0   \n",
       "7023     110.0        12.1   5.4   NaN   NaN    NaN              NaN   NaN   \n",
       "\n",
       "      liver_function_test  \n",
       "0                    True  \n",
       "1                    True  \n",
       "2                    True  \n",
       "3                    True  \n",
       "4                    True  \n",
       "...                   ...  \n",
       "7019                 True  \n",
       "7020                 True  \n",
       "7021                 True  \n",
       "7022                 True  \n",
       "7023                False  \n",
       "\n",
       "[7024 rows x 15 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a6d1b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>calcium</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>hemoglobin</th>\n",
       "      <th>wbc</th>\n",
       "      <th>alt</th>\n",
       "      <th>ast</th>\n",
       "      <th>alp</th>\n",
       "      <th>bilirubin_total</th>\n",
       "      <th>pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.024000e+03</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6763.000000</td>\n",
       "      <td>6580.000000</td>\n",
       "      <td>6810.000000</td>\n",
       "      <td>6783.000000</td>\n",
       "      <td>5845.000000</td>\n",
       "      <td>5817.000000</td>\n",
       "      <td>3060.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3048.000000</td>\n",
       "      <td>3067.000000</td>\n",
       "      <td>3956.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.497892e+07</td>\n",
       "      <td>8.389640</td>\n",
       "      <td>1.644019</td>\n",
       "      <td>142.289666</td>\n",
       "      <td>137.309545</td>\n",
       "      <td>102.067079</td>\n",
       "      <td>10.043353</td>\n",
       "      <td>10.973130</td>\n",
       "      <td>161.859150</td>\n",
       "      <td>243.882772</td>\n",
       "      <td>131.933727</td>\n",
       "      <td>4.248745</td>\n",
       "      <td>18.826567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.832325e+06</td>\n",
       "      <td>0.820066</td>\n",
       "      <td>1.839893</td>\n",
       "      <td>89.875986</td>\n",
       "      <td>5.638384</td>\n",
       "      <td>6.731008</td>\n",
       "      <td>2.177573</td>\n",
       "      <td>8.228807</td>\n",
       "      <td>752.898832</td>\n",
       "      <td>1216.527439</td>\n",
       "      <td>123.088598</td>\n",
       "      <td>7.808056</td>\n",
       "      <td>11.588015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000414e+07</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.255070e+07</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.496990e+07</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.746081e+07</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>20.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.999217e+07</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>2970.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>125.200000</td>\n",
       "      <td>15018.000000</td>\n",
       "      <td>28275.000000</td>\n",
       "      <td>1185.000000</td>\n",
       "      <td>52.600000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stay_id      calcium   creatinine      glucose       sodium  \\\n",
       "count  7.024000e+03  6091.000000  6763.000000  6580.000000  6810.000000   \n",
       "mean   3.497892e+07     8.389640     1.644019   142.289666   137.309545   \n",
       "std    2.832325e+06     0.820066     1.839893    89.875986     5.638384   \n",
       "min    3.000414e+07     4.200000     0.100000    30.000000    83.000000   \n",
       "25%    3.255070e+07     7.900000     0.800000   102.000000   134.000000   \n",
       "50%    3.496990e+07     8.400000     1.100000   125.000000   138.000000   \n",
       "75%    3.746081e+07     8.900000     1.800000   159.000000   141.000000   \n",
       "max    3.999217e+07    12.300000    19.700000  2970.000000   185.000000   \n",
       "\n",
       "          chloride   hemoglobin          wbc           alt           ast  \\\n",
       "count  6783.000000  5845.000000  5817.000000   3060.000000   3088.000000   \n",
       "mean    102.067079    10.043353    10.973130    161.859150    243.882772   \n",
       "std       6.731008     2.177573     8.228807    752.898832   1216.527439   \n",
       "min      62.000000     3.900000     0.100000      1.000000      5.000000   \n",
       "25%      98.000000     8.400000     5.900000     17.000000     22.000000   \n",
       "50%     102.000000     9.800000     9.400000     30.000000     42.000000   \n",
       "75%     106.000000    11.400000    14.200000     62.000000     95.000000   \n",
       "max     153.000000    18.400000   125.200000  15018.000000  28275.000000   \n",
       "\n",
       "               alp  bilirubin_total           pt  \n",
       "count  3048.000000      3067.000000  3956.000000  \n",
       "mean    131.933727         4.248745    18.826567  \n",
       "std     123.088598         7.808056    11.588015  \n",
       "min       7.000000         0.100000     9.200000  \n",
       "25%      65.000000         0.500000    12.800000  \n",
       "50%      92.000000         1.000000    14.900000  \n",
       "75%     149.000000         3.600000    20.400000  \n",
       "max    1185.000000        52.600000   150.000000  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c002b8dc-5de6-4b18-8fb0-97c29c1b92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that patients with no relevant results recorded don't have liver issues\n",
    "# so we impute these patients' missing values of these columns with random number in normal range\n",
    "\n",
    "# but i can not find the unit and normal range for them so i drop them first >_<\n",
    "# dynamic_data.drop(['alt','ast','alp','bilirubin_total','pt'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f57b483-f95d-4d87-bc58-8317a9ddafd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id                   0\n",
       "charttime                 0\n",
       "calcium                 933\n",
       "creatinine              261\n",
       "glucose                 444\n",
       "sodium                  214\n",
       "chloride                241\n",
       "hemoglobin             1179\n",
       "wbc                    1207\n",
       "alt                    3964\n",
       "ast                    3936\n",
       "alp                    3976\n",
       "bilirubin_total        3957\n",
       "pt                     3068\n",
       "liver_function_test       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d6278aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: alt, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: ast, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: alp, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: bilirubin_total, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: pt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alt'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'ast'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alp'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'bilirubin_total'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'pt'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "026725c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KNN to impute the rest\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors = 10)\n",
    "dynamic_data.iloc[:,2:] = imputer.fit_transform(dynamic_data.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f54e7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define normal ranges for each column\n",
    "normal_ranges = {\n",
    "    'alt': (5, 40),\n",
    "    'ast': (10, 35),\n",
    "    'alp': (40, 130),\n",
    "    'bilirubin_total': (0.1, 1.0),\n",
    "    'pt': (9.5, 13.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4a9dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = dynamic_data['liver_function_test'] == False\n",
    "n_no_test = mask.sum()\n",
    "def sample_normal(col):\n",
    "    lower = normal_ranges[col][0]\n",
    "    upper = normal_ranges[col][1]\n",
    "    return np.random.normal(loc=(lower+upper)/2, scale=(upper-lower)/6, size=n_no_test)\n",
    "\n",
    "sampled_alt = sample_normal(\"alt\")\n",
    "sampled_ast = sample_normal(\"ast\")\n",
    "sampled_alp = sample_normal(\"alp\")\n",
    "sampled_bilirubin_total = sample_normal(\"bilirubin_total\")\n",
    "sampled_pt = sample_normal(\"pt\")\n",
    "dynamic_data.loc[mask, 'alt'] = sampled_alt\n",
    "dynamic_data.loc[mask, 'ast'] = sampled_ast\n",
    "dynamic_data.loc[mask, 'alp'] = sampled_alp\n",
    "dynamic_data.loc[mask, 'bilirubin_total'] = sampled_bilirubin_total\n",
    "dynamic_data.loc[mask, 'pt'] = sampled_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "073bb8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    349.000000\n",
      "mean      22.284148\n",
      "std        5.455683\n",
      "min       10.439581\n",
      "25%       18.750667\n",
      "50%       22.104411\n",
      "75%       25.732527\n",
      "max       37.124739\n",
      "Name: alt, dtype: float64\n",
      "count    349.000000\n",
      "mean      22.347756\n",
      "std        4.311364\n",
      "min        9.522722\n",
      "25%       19.618432\n",
      "50%       22.187796\n",
      "75%       25.259406\n",
      "max       36.448495\n",
      "Name: ast, dtype: float64\n",
      "count    349.000000\n",
      "mean      85.759828\n",
      "std       15.716488\n",
      "min       30.732171\n",
      "25%       75.381025\n",
      "50%       85.493087\n",
      "75%       96.421282\n",
      "max      130.996418\n",
      "Name: alp, dtype: float64\n",
      "count    349.000000\n",
      "mean       0.555881\n",
      "std        0.147997\n",
      "min        0.088183\n",
      "25%        0.454087\n",
      "50%        0.555208\n",
      "75%        0.652488\n",
      "max        1.004763\n",
      "Name: bilirubin_total, dtype: float64\n",
      "count    349.000000\n",
      "mean      11.519348\n",
      "std        0.664672\n",
      "min        9.296217\n",
      "25%       11.093571\n",
      "50%       11.510098\n",
      "75%       11.961104\n",
      "max       13.245479\n",
      "Name: pt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alt'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'ast'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alp'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'bilirubin_total'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'pt'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b71c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to address the same patinent have differrent results at the same charttime\n",
    "dynamic_data = dynamic_data.groupby(['stay_id','charttime']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86cd74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the gradient\n",
    "def count_gradient(group):\n",
    "    testresult = list(dynamic_data.columns)[2:-1]\n",
    "    for i in testresult:\n",
    "        if len(group) == 1:\n",
    "            group[i+'_grad'] = 0\n",
    "        else:\n",
    "            time_diff = (group['charttime'].iloc[-1] - group['charttime'].iloc[-2]).total_seconds()\n",
    "            group[i+'_grad'] = (group[i].iloc[-1] - group[i].iloc[-2]) / time_diff \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff28e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_data['charttime'] = pd.to_datetime(dynamic_data['charttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e0f0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_data.sort_values(by = ['stay_id','charttime'], inplace = True)\n",
    "dynamic_data = dynamic_data.groupby('stay_id').apply(count_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8ce7aaf6-91bc-4a25-b833-eabac6e3d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the last values of all timepoints for each patient\n",
    "dynamic_data_last = dynamic_data.drop(['charttime'],axis = 1).groupby('stay_id').tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39bb1bfb-2a71-49d1-a941-333d27d8d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dynamic and static table\n",
    "data = static_data.merge(dynamic_data_last, on = 'stay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ebdde6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>icu_intime</th>\n",
       "      <th>vent_start</th>\n",
       "      <th>vent_end</th>\n",
       "      <th>vent_duration</th>\n",
       "      <th>calcium</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>...</th>\n",
       "      <th>glucose_grad</th>\n",
       "      <th>sodium_grad</th>\n",
       "      <th>chloride_grad</th>\n",
       "      <th>hemoglobin_grad</th>\n",
       "      <th>wbc_grad</th>\n",
       "      <th>alt_grad</th>\n",
       "      <th>ast_grad</th>\n",
       "      <th>alp_grad</th>\n",
       "      <th>bilirubin_total_grad</th>\n",
       "      <th>pt_grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30004144</td>\n",
       "      <td>2126-04-04 13:20:25.000</td>\n",
       "      <td>4/5/26 16:00</td>\n",
       "      <td>4/6/26 17:00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.7</td>\n",
       "      <td>133.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>-0.001782</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30005366</td>\n",
       "      <td>2202-12-27 17:36:59.000</td>\n",
       "      <td>12/28/02 14:00</td>\n",
       "      <td>12/28/02 20:00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6.7</td>\n",
       "      <td>41.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30006983</td>\n",
       "      <td>2159-10-12 03:56:42.000</td>\n",
       "      <td>10/12/59 18:00</td>\n",
       "      <td>10/14/59 19:00</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>7.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>-0.000606</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30023204</td>\n",
       "      <td>2124-07-09 16:43:55.000</td>\n",
       "      <td>7/11/24 16:00</td>\n",
       "      <td>7/12/24 16:10</td>\n",
       "      <td>24.166667</td>\n",
       "      <td>8.50</td>\n",
       "      <td>1.4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30031418</td>\n",
       "      <td>2156-03-05 14:11:00.000</td>\n",
       "      <td>3/7/56 22:06</td>\n",
       "      <td>3/8/56 8:00</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>7.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>133.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>39977971</td>\n",
       "      <td>2115-12-11 17:42:45.000</td>\n",
       "      <td>12/12/15 12:00</td>\n",
       "      <td>12/12/15 16:00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.90</td>\n",
       "      <td>2.2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003474</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.002295</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>39982332</td>\n",
       "      <td>2180-03-01 22:35:04.000</td>\n",
       "      <td>3/2/80 19:00</td>\n",
       "      <td>3/3/80 8:00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.82</td>\n",
       "      <td>1.2</td>\n",
       "      <td>119.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>39985110</td>\n",
       "      <td>2141-03-03 05:57:46.000</td>\n",
       "      <td>3/4/41 20:44</td>\n",
       "      <td>3/6/41 4:00</td>\n",
       "      <td>31.266667</td>\n",
       "      <td>10.40</td>\n",
       "      <td>6.8</td>\n",
       "      <td>149.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>-0.001323</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>39986206</td>\n",
       "      <td>2183-06-19 23:25:31.000</td>\n",
       "      <td>6/20/83 22:00</td>\n",
       "      <td>6/30/83 4:00</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>39992167</td>\n",
       "      <td>2114-06-10 19:00:00.000</td>\n",
       "      <td>6/11/14 17:00</td>\n",
       "      <td>6/15/14 5:00</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>136.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.006194</td>\n",
       "      <td>-0.007773</td>\n",
       "      <td>-0.009651</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stay_id               icu_intime      vent_start        vent_end  \\\n",
       "0     30004144  2126-04-04 13:20:25.000    4/5/26 16:00    4/6/26 17:00   \n",
       "1     30005366  2202-12-27 17:36:59.000  12/28/02 14:00  12/28/02 20:00   \n",
       "2     30006983  2159-10-12 03:56:42.000  10/12/59 18:00  10/14/59 19:00   \n",
       "3     30023204  2124-07-09 16:43:55.000   7/11/24 16:00   7/12/24 16:10   \n",
       "4     30031418  2156-03-05 14:11:00.000    3/7/56 22:06     3/8/56 8:00   \n",
       "...        ...                      ...             ...             ...   \n",
       "1918  39977971  2115-12-11 17:42:45.000  12/12/15 12:00  12/12/15 16:00   \n",
       "1919  39982332  2180-03-01 22:35:04.000    3/2/80 19:00     3/3/80 8:00   \n",
       "1920  39985110  2141-03-03 05:57:46.000    3/4/41 20:44     3/6/41 4:00   \n",
       "1921  39986206  2183-06-19 23:25:31.000   6/20/83 22:00    6/30/83 4:00   \n",
       "1922  39992167  2114-06-10 19:00:00.000   6/11/14 17:00    6/15/14 5:00   \n",
       "\n",
       "      vent_duration  calcium  creatinine  glucose  sodium  chloride  ...  \\\n",
       "0         25.000000     6.80         0.7    133.0   135.0     102.0  ...   \n",
       "1          6.000000     8.80         6.7     41.0   139.0     100.0  ...   \n",
       "2         49.000000     7.10         1.0     89.0   136.0     108.0  ...   \n",
       "3         24.166667     8.50         1.4    107.0   131.0     100.0  ...   \n",
       "4          9.900000     7.40         0.4    133.0   139.0     106.0  ...   \n",
       "...             ...      ...         ...      ...     ...       ...  ...   \n",
       "1918       4.000000     8.90         2.2     98.0   132.0      97.0  ...   \n",
       "1919      13.000000     8.82         1.2    119.0   140.0     103.0  ...   \n",
       "1920      31.266667    10.40         6.8    149.0   139.0      98.0  ...   \n",
       "1921     222.000000     7.50         6.0    101.0   139.0     103.0  ...   \n",
       "1922      84.000000     7.50         0.8    136.0   128.0     100.0  ...   \n",
       "\n",
       "      glucose_grad  sodium_grad  chloride_grad  hemoglobin_grad  wbc_grad  \\\n",
       "0         0.001466     0.000000      -0.000077        -0.000093 -0.000023   \n",
       "1         0.000000     0.000000       0.000000         0.000000  0.000000   \n",
       "2        -0.001230     0.000073       0.000110        -0.000015  0.000022   \n",
       "3         0.000403    -0.000050      -0.000201         0.000076 -0.000222   \n",
       "4         0.000000     0.000000       0.000000         0.000000  0.000000   \n",
       "...            ...          ...            ...              ...       ...   \n",
       "1918     -0.003474     0.000100       0.000033        -0.000037 -0.000017   \n",
       "1919      0.000000     0.000000       0.000000         0.000000  0.000000   \n",
       "1920      0.000425     0.000018       0.000000         0.000007  0.000052   \n",
       "1921     -0.000866     0.000000       0.000000         0.000017 -0.000039   \n",
       "1922      0.001674    -0.000218       0.000000         0.000000 -0.000102   \n",
       "\n",
       "      alt_grad  ast_grad  alp_grad  bilirubin_total_grad   pt_grad  \n",
       "0    -0.000216 -0.001782  0.002238              0.000002  0.000093  \n",
       "1     0.000000  0.000000  0.000000              0.000000  0.000000  \n",
       "2     0.000055  0.000275 -0.000606              0.000004  0.000031  \n",
       "3     0.002110  0.001334  0.000534             -0.000062 -0.000226  \n",
       "4     0.000000  0.000000  0.000000              0.000000  0.000000  \n",
       "...        ...       ...       ...                   ...       ...  \n",
       "1918 -0.002295  0.000852 -0.000658             -0.000005  0.000010  \n",
       "1919  0.000000  0.000000  0.000000              0.000000  0.000000  \n",
       "1920  0.000575  0.000778 -0.001323              0.000013  0.000032  \n",
       "1921  0.002264  0.002580  0.003697              0.000011  0.000013  \n",
       "1922 -0.006194 -0.007773 -0.009651              0.000131  0.000022  \n",
       "\n",
       "[1923 rows x 30 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b40ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data type of icu_intime, vent_start and vent_end to date and time format\n",
    "data['icu_intime'] = pd.to_datetime(data['icu_intime'])\n",
    "data['vent_start'] = pd.to_datetime(data['vent_start'], format='%m/%d/%y %H:%M')\n",
    "data['vent_end'] = pd.to_datetime(data['vent_end'], format='%m/%d/%y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2c33d740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id                          int64\n",
       "icu_intime              datetime64[ns]\n",
       "vent_start              datetime64[ns]\n",
       "vent_end                datetime64[ns]\n",
       "vent_duration                  float64\n",
       "calcium                        float64\n",
       "creatinine                     float64\n",
       "glucose                        float64\n",
       "sodium                         float64\n",
       "chloride                       float64\n",
       "hemoglobin                     float64\n",
       "wbc                            float64\n",
       "alt                            float64\n",
       "ast                            float64\n",
       "alp                            float64\n",
       "bilirubin_total                float64\n",
       "pt                             float64\n",
       "liver_function_test            float64\n",
       "calcium_grad                   float64\n",
       "creatinine_grad                float64\n",
       "glucose_grad                   float64\n",
       "sodium_grad                    float64\n",
       "chloride_grad                  float64\n",
       "hemoglobin_grad                float64\n",
       "wbc_grad                       float64\n",
       "alt_grad                       float64\n",
       "ast_grad                       float64\n",
       "alp_grad                       float64\n",
       "bilirubin_total_grad           float64\n",
       "pt_grad                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3da8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "over 72 hours    294\n",
       "4-8 hours        249\n",
       "8-12 hours       217\n",
       "12-16 hours      203\n",
       "24-36 hours      194\n",
       "16-20 hours      182\n",
       "0-4 hours        158\n",
       "48-72 hours      155\n",
       "36-48 hours      148\n",
       "20-24 hours      123\n",
       "Name: vent_duration_category, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorize vent_duration \n",
    "data['vent_duration_category'] = pd.cut(data['vent_duration'], bins=[0,4,8,12,16,20,24,36,48,72,np.inf],\n",
    "                                         labels=['0-4 hours', '4-8 hours', '8-12 hours', '12-16 hours',\n",
    "                                                 '16-20 hours', '20-24 hours', '24-36 hours', '36-48 hours',\n",
    "                                                 '48-72 hours','over 72 hours'])\n",
    "data['vent_duration_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a5fc1e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAJDCAYAAAD961luAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVBElEQVR4nO3dX4ild33H8c+3uwb8VyNmKzZ/MJRoTMEUHaMXirHSmuSioWAhUZQGYQk14qW50gtv6oUgYnRZJARvzEUNGks09EYtxNBsQKMxRJaEJtsISVQsKDRs8u3FjGU6nm/mZHLmzLp5vWBhn+f85swX5sfum2efPU91dwAAgD/0Jwc9AAAAnKnEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMdo3lqrq1qp6sqp8Or1dVfbGqTlbVA1X1ttWPCQAA67fMleXbklz1PK9fneSSrV9Hk3zlxY8FAAAHb9dY7u4fJPnV8yy5NsnXetO9Sc6tqjesakAAADgoq7hn+fwkj287PrV1DgAA/qgdXsF71IJzC5+hXVVHs3mrRl75yle+/dJLL13BtwcAgNn999//dHcf2cvXriKWTyW5cNvxBUmeWLSwu48nOZ4kGxsbfeLEiRV8ewAAmFXVf+71a1dxG8adST669akY70rym+7+xQreFwAADtSuV5ar6utJrkxyXlWdSvKZJC9Lku4+luSuJNckOZnkd0lu2K9hAQBgnXaN5e6+fpfXO8nHVzYRAACcITzBDwAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABkvFclVdVVUPV9XJqrp5weuvqapvV9WPq+rBqrph9aMCAMB67RrLVXUoyS1Jrk5yWZLrq+qyHcs+nuRn3X15kiuTfL6qzlnxrAAAsFbLXFm+IsnJ7n6ku59JcnuSa3es6SSvrqpK8qokv0pyeqWTAgDAmi0Ty+cneXzb8amtc9t9KclbkjyR5CdJPtndz61kQgAAOCDLxHItONc7jj+Q5EdJ/jzJXyX5UlX96R+8UdXRqjpRVSeeeuqpFzgqAACs1zKxfCrJhduOL8jmFeTtbkhyR286meTRJJfufKPuPt7dG929ceTIkb3ODAAAa7FMLN+X5JKqunjrP+1dl+TOHWseS/L+JKmq1yd5c5JHVjkoAACs2+HdFnT36aq6KcndSQ4lubW7H6yqG7deP5bks0luq6qfZPO2jU9199P7ODcAAOy7XWM5Sbr7riR37Th3bNvvn0jyt6sdDQAADpYn+AEAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwGCpWK6qq6rq4ao6WVU3D2uurKofVdWDVfX91Y4JAADrd3i3BVV1KMktSf4myakk91XVnd39s21rzk3y5SRXdfdjVfVn+zQvAACszTJXlq9IcrK7H+nuZ5LcnuTaHWs+lOSO7n4sSbr7ydWOCQAA67dMLJ+f5PFtx6e2zm33piSvrarvVdX9VfXRVQ0IAAAHZdfbMJLUgnO94H3enuT9SV6e5IdVdW93//z/vVHV0SRHk+Siiy564dMCAMAaLXNl+VSSC7cdX5DkiQVrvtvdv+3up5P8IMnlO9+ou49390Z3bxw5cmSvMwMAwFosE8v3Jbmkqi6uqnOSXJfkzh1rvpXkPVV1uKpekeSdSR5a7agAALBeu96G0d2nq+qmJHcnOZTk1u5+sKpu3Hr9WHc/VFXfTfJAkueSfLW7f7qfgwMAwH6r7p23H6/HxsZGnzhx4kC+NwAALx1VdX93b+zlaz3BDwAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZLxXJVXVVVD1fVyaq6+XnWvaOqnq2qD65uRAAAOBi7xnJVHUpyS5Krk1yW5PqqumxY97kkd696SAAAOAjLXFm+IsnJ7n6ku59JcnuSaxes+0SSbyR5coXzAQDAgVkmls9P8vi241Nb5/5PVZ2f5O+THFvdaAAAcLCWieVacK53HH8hyae6+9nnfaOqo1V1oqpOPPXUU0uOCAAAB+PwEmtOJblw2/EFSZ7YsWYjye1VlSTnJbmmqk539ze3L+ru40mOJ8nGxsbO4AYAgDPKMrF8X5JLquriJP+V5LokH9q+oLsv/v3vq+q2JP+6M5QBAOCPza6x3N2nq+qmbH7KxaEkt3b3g1V149br7lMGAOCstMyV5XT3XUnu2nFuYSR39z+++LEAAODgeYIfAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMlorlqrqqqh6uqpNVdfOC1z9cVQ9s/bqnqi5f/agAALBeu8ZyVR1KckuSq5NcluT6qrpsx7JHk7y3u9+a5LNJjq96UAAAWLdlrixfkeRkdz/S3c8kuT3JtdsXdPc93f3rrcN7k1yw2jEBAGD9lonl85M8vu341Na5yceSfOfFDAUAAGeCw0usqQXneuHCqvdlM5bfPbx+NMnRJLnooouWHBEAAA7GMleWTyW5cNvxBUme2Lmoqt6a5KtJru3uXy56o+4+3t0b3b1x5MiRvcwLAABrs0ws35fkkqq6uKrOSXJdkju3L6iqi5LckeQj3f3z1Y8JAADrt+ttGN19uqpuSnJ3kkNJbu3uB6vqxq3XjyX5dJLXJflyVSXJ6e7e2L+xAQBg/1X3wtuP993GxkafOHHiQL43AAAvHVV1/14v5HqCHwAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADJaK5aq6qqoerqqTVXXzgterqr649foDVfW21Y8KAADrtWssV9WhJLckuTrJZUmur6rLdiy7OsklW7+OJvnKiucEAIC1W+bK8hVJTnb3I939TJLbk1y7Y821Sb7Wm+5Ncm5VvWHFswIAwFotE8vnJ3l82/GprXMvdA0AAPxRObzEmlpwrvewJlV1NJu3aSTJ/1TVT5f4/ry0nJfk6YMegjOOfcEi9gWL2Bcs8ua9fuEysXwqyYXbji9I8sQe1qS7jyc5niRVdaK7N17QtJz17AsWsS9YxL5gEfuCRarqxF6/dpnbMO5LcklVXVxV5yS5LsmdO9bcmeSjW5+K8a4kv+nuX+x1KAAAOBPsemW5u09X1U1J7k5yKMmt3f1gVd249fqxJHcluSbJySS/S3LD/o0MAADrscxtGOnuu7IZxNvPHdv2+07y8Rf4vY+/wPW8NNgXLGJfsIh9wSL2BYvseV/UZucCAAA7edw1AAAM9j2WPSqbRZbYFx/e2g8PVNU9VXX5QczJeu22L7ate0dVPVtVH1znfByMZfZFVV1ZVT+qqger6vvrnpH1W+LvkddU1ber6sdb+8L/pzrLVdWtVfXk9NHEe23OfY1lj8pmkSX3xaNJ3tvdb03y2bgH7ay35L74/brPZfM/HXOWW2ZfVNW5Sb6c5O+6+y+T/MO652S9lvzz4uNJftbdlye5Msnntz7Vi7PXbUmuep7X99Sc+31l2aOyWWTXfdHd93T3r7cO783mZ3dzdlvmz4sk+USSbyR5cp3DcWCW2RcfSnJHdz+WJN1tb5z9ltkXneTVVVVJXpXkV0lOr3dM1qm7f5DNn/NkT82537HsUdks8kJ/5h9L8p19nYgzwa77oqrOT/L3SY6Fl4pl/rx4U5LXVtX3qur+qvro2qbjoCyzL76U5C3ZfEjaT5J8srufW894nKH21JxLfXTci7CyR2VzVln6Z15V78tmLL97XyfiTLDMvvhCkk9197ObF4t4CVhmXxxO8vYk70/y8iQ/rKp7u/vn+z0cB2aZffGBJD9K8tdJ/iLJv1XVv3f3f+/zbJy59tSc+x3LK3tUNmeVpX7mVfXWJF9NcnV3/3JNs3FwltkXG0lu3wrl85JcU1Wnu/uba5mQg7Ds3yNPd/dvk/y2qn6Q5PIkYvnstcy+uCHJP289C+JkVT2a5NIk/7GeETkD7ak59/s2DI/KZpFd90VVXZTkjiQfcXXoJWPXfdHdF3f3G7v7jUn+Jck/CeWz3jJ/j3wryXuq6nBVvSLJO5M8tOY5Wa9l9sVj2fzXhlTV65O8Ockja52SM82emnNfryx7VDaLLLkvPp3kdUm+vHUV8XR3bxzUzOy/JfcFLzHL7IvufqiqvpvkgSTPJflqdy/86CjODkv+efHZJLdV1U+y+c/vn+rupw9saPZdVX09m598cl5VnUrymSQvS15cc3qCHwAADDzBDwAABmIZAAAGYhkAAAZiGQAABmIZAAAGYhkAAAZiGQAABmIZAAAG/wvxicdhMCk7owAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#Dropped icu_intime, vent_start and vent_end as they are usable for correlation\n",
    "#Creating correlation matrix to observe the relationship between variables\n",
    "df_corr = data.loc[:,'vent_duration':'pt_grad'].corr()\n",
    "\n",
    "#Setting up plots\n",
    "#f, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "#Setting up lower triangle correlation matrix\n",
    "#mask = np.triu(np.ones(df_corr.shape), k=0).astype(bool)\n",
    "#cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "#sns.heatmap(df_corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "651cd34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting pairplot to assess the relationship and distribution of each variable. \n",
    "#sns.pairplot(data.loc[:,'vent_duration':'pt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9c3036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: (1923, 31)\n",
      "Trimmed Shape: (1293, 31)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Scale the numeric columns using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "# Create a new dataframe with the scaled numeric columns\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=numeric_cols, index=data.index)\n",
    "\n",
    "# Identify outliers with z-scores greater than 2.5 or less than -2.5 (95%)\n",
    "z_scores = (scaled_df - scaled_df.mean()) / scaled_df.std()\n",
    "outliers = data[(z_scores > 2.5).any(axis=1) | (z_scores < -2.5).any(axis=1)]\n",
    "\n",
    "# Remove outliers from the original dataframe\n",
    "trimmed_df = data.drop(outliers.index)\n",
    "\n",
    "print(\"Original Shape:\", data.shape)\n",
    "print(\"Trimmed Shape:\", trimmed_df.shape)\n",
    "#print(sns.pairplot(trimmed_df.loc[:,'vent_duration':'pt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1fd52d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating correlation matrix to observe the relationship between variables\n",
    "trimmed_df_corr = trimmed_df.loc[:,'vent_duration':'pt_grad'].corr()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87069e56",
   "metadata": {},
   "source": [
    "## Splitting Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6de74863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting Datasets into Train and Test\n",
    "X= data.drop(['stay_id', 'icu_intime', 'vent_start', 'vent_end','vent_duration', 'vent_duration_category'], axis=1)\n",
    "y= data['vent_duration_category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split to train & test set (8:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e2b8b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1538, 25)\n",
      "y_train: (1538,)\n",
      "X_test: (385, 25)\n",
      "y_test: (385,)\n"
     ]
    }
   ],
   "source": [
    "# The dimension of the training set\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "\n",
    "# The dimension of the test set\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "095eb7ae",
   "metadata": {},
   "source": [
    "## Normalising Dataset after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "91f10aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean:  0.43181259794187293\n",
      "Training set std:  0.3720752340729298\n",
      "Test set mean:  0.4337456691391151\n",
      "Test set std:  0.3724375836774929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Calculate the normalization parameters on the training set\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Normalize the training set using the calculated parameters\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_train_norm_df = pd.DataFrame(X_train_norm, columns=X_train.columns)\n",
    "\n",
    "# Normalize the test set using the same normalization parameters as the training set\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "X_test_norm_df = pd.DataFrame(X_test_norm, columns=X_test.columns)\n",
    "\n",
    "# Verify that the mean and standard deviation of the training and test sets are similar\n",
    "print(\"Training set mean: \", np.mean(X_train_norm))\n",
    "print(\"Training set std: \", np.std(X_train_norm))\n",
    "print(\"Test set mean: \", np.mean(X_test_norm))\n",
    "print(\"Test set std: \", np.std(X_test_norm))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d07b19",
   "metadata": {},
   "source": [
    "## Feature Selection Using Recursive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8ff292ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       calcium  creatinine   glucose    sodium  chloride  hemoglobin  \\\n",
      "0     0.515625    0.087629  0.229541  0.422535  0.414634    0.379577   \n",
      "1     0.421875    0.061856  0.173653  0.535211  0.548780    0.373239   \n",
      "2     0.531250    0.041237  0.165669  0.450704  0.426829    0.612676   \n",
      "3     0.359375    0.025773  0.207585  0.422535  0.439024    0.549296   \n",
      "4     0.468750    0.036082  0.187625  0.464789  0.451220    0.429577   \n",
      "...        ...         ...       ...       ...       ...         ...   \n",
      "1533  0.359375    0.067010  0.219561  0.478873  0.426829    0.619718   \n",
      "1534  0.546875    0.041237  0.137725  0.464789  0.487805    0.640845   \n",
      "1535  0.453125    0.056701  0.107784  0.408451  0.365854    0.542254   \n",
      "1536  0.531250    0.072165  0.409182  0.450704  0.426829    0.352113   \n",
      "1537  0.421875    0.587629  0.211577  0.450704  0.414634    0.274648   \n",
      "\n",
      "           wbc       alt       ast       alp  ...  glucose_grad  sodium_grad  \\\n",
      "0     0.084607  0.062711  0.116021  0.292140  ...      0.975997     0.860531   \n",
      "1     0.747644  0.000716  0.001852  0.266098  ...      0.972736     0.858824   \n",
      "2     0.117277  0.006817  0.003976  0.206723  ...      0.972736     0.858824   \n",
      "3     0.307853  0.002704  0.002575  0.103220  ...      0.973659     0.855996   \n",
      "4     0.064921  0.001125  0.000645  0.053475  ...      0.971694     0.862710   \n",
      "...        ...       ...       ...       ...  ...           ...          ...   \n",
      "1533  0.158115  0.007469  0.005223  0.198295  ...      0.967897     0.856290   \n",
      "1534  0.115183  0.030305  0.014051  0.075758  ...      0.955540     0.867412   \n",
      "1535  0.167539  0.006952  0.007834  0.185417  ...      0.972159     0.863128   \n",
      "1536  0.081675  0.007612  0.003885  0.205398  ...      0.972736     0.858824   \n",
      "1537  0.169634  0.007588  0.004644  0.177557  ...      0.974981     0.856341   \n",
      "\n",
      "      chloride_grad  hemoglobin_grad  wbc_grad  alt_grad  ast_grad  alp_grad  \\\n",
      "0          0.106019         0.932931  0.472206  0.944679  0.737704  0.982922   \n",
      "1          0.105496         0.936478  0.496896  0.933157  0.691004  0.982700   \n",
      "2          0.105496         0.936478  0.496896  0.933157  0.691004  0.982700   \n",
      "3          0.104630         0.939096  0.520087  0.933196  0.691152  0.982703   \n",
      "4          0.106091         0.933960  0.470179  0.932956  0.690883  0.982580   \n",
      "...             ...              ...       ...       ...       ...       ...   \n",
      "1533       0.103944         0.956285  0.538984  0.919174  0.691150  0.982967   \n",
      "1534       0.110100         0.939571  0.496896  0.933831  0.690290  0.982716   \n",
      "1535       0.106156         0.930722  0.497346  0.934259  0.694067  0.983100   \n",
      "1536       0.105496         0.936478  0.496896  0.933157  0.691004  0.982700   \n",
      "1537       0.103975         0.934946  0.494822  0.935161  0.693419  0.983054   \n",
      "\n",
      "      bilirubin_total_grad   pt_grad  \n",
      "0                 0.962379  0.499254  \n",
      "1                 0.961294  0.476152  \n",
      "2                 0.961294  0.476152  \n",
      "3                 0.961562  0.476534  \n",
      "4                 0.961382  0.476583  \n",
      "...                    ...       ...  \n",
      "1533              0.960356  0.440787  \n",
      "1534              0.961468  0.438947  \n",
      "1535              0.961405  0.472781  \n",
      "1536              0.961294  0.476152  \n",
      "1537              0.961670  0.503989  \n",
      "\n",
      "[1538 rows x 25 columns]\n",
      "426       12-16 hours\n",
      "141         4-8 hours\n",
      "1323        0-4 hours\n",
      "692        8-12 hours\n",
      "1797    over 72 hours\n",
      "            ...      \n",
      "1130    over 72 hours\n",
      "1294      12-16 hours\n",
      "860       20-24 hours\n",
      "1459      12-16 hours\n",
      "1126      48-72 hours\n",
      "Name: vent_duration_category, Length: 1538, dtype: category\n",
      "Categories (10, object): ['0-4 hours' < '4-8 hours' < '8-12 hours' < '12-16 hours' ... '24-36 hours' < '36-48 hours' < '48-72 hours' < 'over 72 hours']\n"
     ]
    }
   ],
   "source": [
    "# listing columns\n",
    "print(X_train_norm_df)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "90bdecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform feature selection using SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_train_selected = selector.fit_transform(X_train_norm_df, y_train_encoded)\n",
    "X_test_selected = selector.transform(X_test_norm_df)\n",
    "\n",
    "X_train.columns[selector.get_support()]\n",
    "m, n = X_train_selected.shape\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7c19b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['calcium', 'creatinine', 'alp', 'pt', 'ast_grad'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# showing the selected features\n",
    "print(X_train.columns[selector.get_support()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e79d673",
   "metadata": {},
   "source": [
    "## DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "71b01dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2.2673\n",
      "Epoch [10/100], Loss: 2.2839\n",
      "Epoch [10/100], Loss: 2.2916\n",
      "Epoch [10/100], Loss: 2.2736\n",
      "Epoch [10/100], Loss: 2.2802\n",
      "Epoch [10/100], Loss: 2.2664\n",
      "Epoch [10/100], Loss: 2.2897\n",
      "Epoch [10/100], Loss: 2.2758\n",
      "Epoch [10/100], Loss: 2.2975\n",
      "Epoch [10/100], Loss: 2.2873\n",
      "Epoch [10/100], Loss: 2.2939\n",
      "Epoch [10/100], Loss: 2.2756\n",
      "Epoch [10/100], Loss: 2.2919\n",
      "Epoch [20/100], Loss: 2.2726\n",
      "Epoch [20/100], Loss: 2.2448\n",
      "Epoch [20/100], Loss: 2.3047\n",
      "Epoch [20/100], Loss: 2.2789\n",
      "Epoch [20/100], Loss: 2.2699\n",
      "Epoch [20/100], Loss: 2.3206\n",
      "Epoch [20/100], Loss: 2.2582\n",
      "Epoch [20/100], Loss: 2.3150\n",
      "Epoch [20/100], Loss: 2.2745\n",
      "Epoch [20/100], Loss: 2.2805\n",
      "Epoch [20/100], Loss: 2.2975\n",
      "Epoch [20/100], Loss: 2.2907\n",
      "Epoch [20/100], Loss: 2.4296\n",
      "Epoch [30/100], Loss: 2.2794\n",
      "Epoch [30/100], Loss: 2.2719\n",
      "Epoch [30/100], Loss: 2.2644\n",
      "Epoch [30/100], Loss: 2.2653\n",
      "Epoch [30/100], Loss: 2.2756\n",
      "Epoch [30/100], Loss: 2.3028\n",
      "Epoch [30/100], Loss: 2.2773\n",
      "Epoch [30/100], Loss: 2.2832\n",
      "Epoch [30/100], Loss: 2.2637\n",
      "Epoch [30/100], Loss: 2.2772\n",
      "Epoch [30/100], Loss: 2.2912\n",
      "Epoch [30/100], Loss: 2.2800\n",
      "Epoch [30/100], Loss: 2.2713\n",
      "Epoch [40/100], Loss: 2.3114\n",
      "Epoch [40/100], Loss: 2.3010\n",
      "Epoch [40/100], Loss: 2.2824\n",
      "Epoch [40/100], Loss: 2.2897\n",
      "Epoch [40/100], Loss: 2.2756\n",
      "Epoch [40/100], Loss: 2.2815\n",
      "Epoch [40/100], Loss: 2.2499\n",
      "Epoch [40/100], Loss: 2.2932\n",
      "Epoch [40/100], Loss: 2.2513\n",
      "Epoch [40/100], Loss: 2.2743\n",
      "Epoch [40/100], Loss: 2.2567\n",
      "Epoch [40/100], Loss: 2.2766\n",
      "Epoch [40/100], Loss: 2.4293\n",
      "Epoch [50/100], Loss: 2.2811\n",
      "Epoch [50/100], Loss: 2.2761\n",
      "Epoch [50/100], Loss: 2.2715\n",
      "Epoch [50/100], Loss: 2.2981\n",
      "Epoch [50/100], Loss: 2.2840\n",
      "Epoch [50/100], Loss: 2.2604\n",
      "Epoch [50/100], Loss: 2.2618\n",
      "Epoch [50/100], Loss: 2.2708\n",
      "Epoch [50/100], Loss: 2.2743\n",
      "Epoch [50/100], Loss: 2.2671\n",
      "Epoch [50/100], Loss: 2.2883\n",
      "Epoch [50/100], Loss: 2.2971\n",
      "Epoch [50/100], Loss: 2.2381\n",
      "Epoch [60/100], Loss: 2.2782\n",
      "Epoch [60/100], Loss: 2.2822\n",
      "Epoch [60/100], Loss: 2.2803\n",
      "Epoch [60/100], Loss: 2.3089\n",
      "Epoch [60/100], Loss: 2.2643\n",
      "Epoch [60/100], Loss: 2.2841\n",
      "Epoch [60/100], Loss: 2.2899\n",
      "Epoch [60/100], Loss: 2.2782\n",
      "Epoch [60/100], Loss: 2.2658\n",
      "Epoch [60/100], Loss: 2.2987\n",
      "Epoch [60/100], Loss: 2.2828\n",
      "Epoch [60/100], Loss: 2.2515\n",
      "Epoch [60/100], Loss: 2.3484\n",
      "Epoch [70/100], Loss: 2.2699\n",
      "Epoch [70/100], Loss: 2.2772\n",
      "Epoch [70/100], Loss: 2.2974\n",
      "Epoch [70/100], Loss: 2.2839\n",
      "Epoch [70/100], Loss: 2.2813\n",
      "Epoch [70/100], Loss: 2.2842\n",
      "Epoch [70/100], Loss: 2.2884\n",
      "Epoch [70/100], Loss: 2.2907\n",
      "Epoch [70/100], Loss: 2.2706\n",
      "Epoch [70/100], Loss: 2.3027\n",
      "Epoch [70/100], Loss: 2.2377\n",
      "Epoch [70/100], Loss: 2.2936\n",
      "Epoch [70/100], Loss: 2.3562\n",
      "Epoch [80/100], Loss: 2.2751\n",
      "Epoch [80/100], Loss: 2.2854\n",
      "Epoch [80/100], Loss: 2.2862\n",
      "Epoch [80/100], Loss: 2.2770\n",
      "Epoch [80/100], Loss: 2.2854\n",
      "Epoch [80/100], Loss: 2.2656\n",
      "Epoch [80/100], Loss: 2.2774\n",
      "Epoch [80/100], Loss: 2.3053\n",
      "Epoch [80/100], Loss: 2.2707\n",
      "Epoch [80/100], Loss: 2.2801\n",
      "Epoch [80/100], Loss: 2.2780\n",
      "Epoch [80/100], Loss: 2.2574\n",
      "Epoch [80/100], Loss: 2.3990\n",
      "Epoch [90/100], Loss: 2.2738\n",
      "Epoch [90/100], Loss: 2.2667\n",
      "Epoch [90/100], Loss: 2.2904\n",
      "Epoch [90/100], Loss: 2.2632\n",
      "Epoch [90/100], Loss: 2.2658\n",
      "Epoch [90/100], Loss: 2.2735\n",
      "Epoch [90/100], Loss: 2.2873\n",
      "Epoch [90/100], Loss: 2.2699\n",
      "Epoch [90/100], Loss: 2.3008\n",
      "Epoch [90/100], Loss: 2.2784\n",
      "Epoch [90/100], Loss: 2.2685\n",
      "Epoch [90/100], Loss: 2.3053\n",
      "Epoch [90/100], Loss: 2.1401\n",
      "Epoch [100/100], Loss: 2.2519\n",
      "Epoch [100/100], Loss: 2.3031\n",
      "Epoch [100/100], Loss: 2.2725\n",
      "Epoch [100/100], Loss: 2.2647\n",
      "Epoch [100/100], Loss: 2.2545\n",
      "Epoch [100/100], Loss: 2.2609\n",
      "Epoch [100/100], Loss: 2.2952\n",
      "Epoch [100/100], Loss: 2.3190\n",
      "Epoch [100/100], Loss: 2.2997\n",
      "Epoch [100/100], Loss: 2.3369\n",
      "Epoch [100/100], Loss: 2.2704\n",
      "Epoch [100/100], Loss: 2.2865\n",
      "Epoch [100/100], Loss: 2.4388\n",
      "Epoch [10/100], Loss: 2.2749\n",
      "Epoch [10/100], Loss: 2.3237\n",
      "Epoch [10/100], Loss: 2.2934\n",
      "Epoch [10/100], Loss: 2.2536\n",
      "Epoch [10/100], Loss: 2.2932\n",
      "Epoch [10/100], Loss: 2.3065\n",
      "Epoch [10/100], Loss: 2.2527\n",
      "Epoch [10/100], Loss: 2.2921\n",
      "Epoch [10/100], Loss: 2.3287\n",
      "Epoch [10/100], Loss: 2.2823\n",
      "Epoch [10/100], Loss: 2.2840\n",
      "Epoch [10/100], Loss: 2.2389\n",
      "Epoch [10/100], Loss: 2.1677\n",
      "Epoch [20/100], Loss: 2.2901\n",
      "Epoch [20/100], Loss: 2.2801\n",
      "Epoch [20/100], Loss: 2.2728\n",
      "Epoch [20/100], Loss: 2.2808\n",
      "Epoch [20/100], Loss: 2.2774\n",
      "Epoch [20/100], Loss: 2.3019\n",
      "Epoch [20/100], Loss: 2.2627\n",
      "Epoch [20/100], Loss: 2.2922\n",
      "Epoch [20/100], Loss: 2.3024\n",
      "Epoch [20/100], Loss: 2.2832\n",
      "Epoch [20/100], Loss: 2.2895\n",
      "Epoch [20/100], Loss: 2.2927\n",
      "Epoch [20/100], Loss: 2.1602\n",
      "Epoch [30/100], Loss: 2.2958\n",
      "Epoch [30/100], Loss: 2.2723\n",
      "Epoch [30/100], Loss: 2.2886\n",
      "Epoch [30/100], Loss: 2.2772\n",
      "Epoch [30/100], Loss: 2.2849\n",
      "Epoch [30/100], Loss: 2.2801\n",
      "Epoch [30/100], Loss: 2.2876\n",
      "Epoch [30/100], Loss: 2.2978\n",
      "Epoch [30/100], Loss: 2.2851\n",
      "Epoch [30/100], Loss: 2.2788\n",
      "Epoch [30/100], Loss: 2.2872\n",
      "Epoch [30/100], Loss: 2.2872\n",
      "Epoch [30/100], Loss: 2.2716\n",
      "Epoch [40/100], Loss: 2.3060\n",
      "Epoch [40/100], Loss: 2.2672\n",
      "Epoch [40/100], Loss: 2.2947\n",
      "Epoch [40/100], Loss: 2.2699\n",
      "Epoch [40/100], Loss: 2.2997\n",
      "Epoch [40/100], Loss: 2.2991\n",
      "Epoch [40/100], Loss: 2.2876\n",
      "Epoch [40/100], Loss: 2.2716\n",
      "Epoch [40/100], Loss: 2.2859\n",
      "Epoch [40/100], Loss: 2.2981\n",
      "Epoch [40/100], Loss: 2.3095\n",
      "Epoch [40/100], Loss: 2.2737\n",
      "Epoch [40/100], Loss: 2.2614\n",
      "Epoch [50/100], Loss: 2.2707\n",
      "Epoch [50/100], Loss: 2.2793\n",
      "Epoch [50/100], Loss: 2.2929\n",
      "Epoch [50/100], Loss: 2.2910\n",
      "Epoch [50/100], Loss: 2.2943\n",
      "Epoch [50/100], Loss: 2.2805\n",
      "Epoch [50/100], Loss: 2.2684\n",
      "Epoch [50/100], Loss: 2.2839\n",
      "Epoch [50/100], Loss: 2.2888\n",
      "Epoch [50/100], Loss: 2.2826\n",
      "Epoch [50/100], Loss: 2.2935\n",
      "Epoch [50/100], Loss: 2.2871\n",
      "Epoch [50/100], Loss: 2.4044\n",
      "Epoch [60/100], Loss: 2.2723\n",
      "Epoch [60/100], Loss: 2.2705\n",
      "Epoch [60/100], Loss: 2.3147\n",
      "Epoch [60/100], Loss: 2.2978\n",
      "Epoch [60/100], Loss: 2.3204\n",
      "Epoch [60/100], Loss: 2.2849\n",
      "Epoch [60/100], Loss: 2.2934\n",
      "Epoch [60/100], Loss: 2.2600\n",
      "Epoch [60/100], Loss: 2.2973\n",
      "Epoch [60/100], Loss: 2.2811\n",
      "Epoch [60/100], Loss: 2.2654\n",
      "Epoch [60/100], Loss: 2.2845\n",
      "Epoch [60/100], Loss: 2.4231\n",
      "Epoch [70/100], Loss: 2.3112\n",
      "Epoch [70/100], Loss: 2.3166\n",
      "Epoch [70/100], Loss: 2.2924\n",
      "Epoch [70/100], Loss: 2.2469\n",
      "Epoch [70/100], Loss: 2.2774\n",
      "Epoch [70/100], Loss: 2.2898\n",
      "Epoch [70/100], Loss: 2.2651\n",
      "Epoch [70/100], Loss: 2.2866\n",
      "Epoch [70/100], Loss: 2.2766\n",
      "Epoch [70/100], Loss: 2.2734\n",
      "Epoch [70/100], Loss: 2.2838\n",
      "Epoch [70/100], Loss: 2.3058\n",
      "Epoch [70/100], Loss: 2.0519\n",
      "Epoch [80/100], Loss: 2.2753\n",
      "Epoch [80/100], Loss: 2.2809\n",
      "Epoch [80/100], Loss: 2.3030\n",
      "Epoch [80/100], Loss: 2.2889\n",
      "Epoch [80/100], Loss: 2.2759\n",
      "Epoch [80/100], Loss: 2.2753\n",
      "Epoch [80/100], Loss: 2.2995\n",
      "Epoch [80/100], Loss: 2.2782\n",
      "Epoch [80/100], Loss: 2.2927\n",
      "Epoch [80/100], Loss: 2.2757\n",
      "Epoch [80/100], Loss: 2.2792\n",
      "Epoch [80/100], Loss: 2.2781\n",
      "Epoch [80/100], Loss: 2.3658\n",
      "Epoch [90/100], Loss: 2.2747\n",
      "Epoch [90/100], Loss: 2.2825\n",
      "Epoch [90/100], Loss: 2.2963\n",
      "Epoch [90/100], Loss: 2.2900\n",
      "Epoch [90/100], Loss: 2.2810\n",
      "Epoch [90/100], Loss: 2.2905\n",
      "Epoch [90/100], Loss: 2.2755\n",
      "Epoch [90/100], Loss: 2.3003\n",
      "Epoch [90/100], Loss: 2.2875\n",
      "Epoch [90/100], Loss: 2.2745\n",
      "Epoch [90/100], Loss: 2.2834\n",
      "Epoch [90/100], Loss: 2.2787\n",
      "Epoch [90/100], Loss: 2.3812\n",
      "Epoch [100/100], Loss: 2.2674\n",
      "Epoch [100/100], Loss: 2.2791\n",
      "Epoch [100/100], Loss: 2.2772\n",
      "Epoch [100/100], Loss: 2.2821\n",
      "Epoch [100/100], Loss: 2.2947\n",
      "Epoch [100/100], Loss: 2.2947\n",
      "Epoch [100/100], Loss: 2.3020\n",
      "Epoch [100/100], Loss: 2.2871\n",
      "Epoch [100/100], Loss: 2.3065\n",
      "Epoch [100/100], Loss: 2.2724\n",
      "Epoch [100/100], Loss: 2.2751\n",
      "Epoch [100/100], Loss: 2.2978\n",
      "Epoch [100/100], Loss: 2.3584\n",
      "Epoch [10/100], Loss: 2.2870\n",
      "Epoch [10/100], Loss: 2.2700\n",
      "Epoch [10/100], Loss: 2.3016\n",
      "Epoch [10/100], Loss: 2.3039\n",
      "Epoch [10/100], Loss: 2.2923\n",
      "Epoch [10/100], Loss: 2.2807\n",
      "Epoch [10/100], Loss: 2.2759\n",
      "Epoch [10/100], Loss: 2.2693\n",
      "Epoch [10/100], Loss: 2.2750\n",
      "Epoch [10/100], Loss: 2.2809\n",
      "Epoch [10/100], Loss: 2.2991\n",
      "Epoch [10/100], Loss: 2.2797\n",
      "Epoch [10/100], Loss: 2.1742\n",
      "Epoch [20/100], Loss: 2.2929\n",
      "Epoch [20/100], Loss: 2.2827\n",
      "Epoch [20/100], Loss: 2.3074\n",
      "Epoch [20/100], Loss: 2.2861\n",
      "Epoch [20/100], Loss: 2.2782\n",
      "Epoch [20/100], Loss: 2.2881\n",
      "Epoch [20/100], Loss: 2.2734\n",
      "Epoch [20/100], Loss: 2.3045\n",
      "Epoch [20/100], Loss: 2.2842\n",
      "Epoch [20/100], Loss: 2.2922\n",
      "Epoch [20/100], Loss: 2.2720\n",
      "Epoch [20/100], Loss: 2.2753\n",
      "Epoch [20/100], Loss: 2.3913\n",
      "Epoch [30/100], Loss: 2.2847\n",
      "Epoch [30/100], Loss: 2.2870\n",
      "Epoch [30/100], Loss: 2.2844\n",
      "Epoch [30/100], Loss: 2.2980\n",
      "Epoch [30/100], Loss: 2.3043\n",
      "Epoch [30/100], Loss: 2.2940\n",
      "Epoch [30/100], Loss: 2.2705\n",
      "Epoch [30/100], Loss: 2.2965\n",
      "Epoch [30/100], Loss: 2.2812\n",
      "Epoch [30/100], Loss: 2.2802\n",
      "Epoch [30/100], Loss: 2.2726\n",
      "Epoch [30/100], Loss: 2.2868\n",
      "Epoch [30/100], Loss: 2.2513\n",
      "Epoch [40/100], Loss: 2.2981\n",
      "Epoch [40/100], Loss: 2.2985\n",
      "Epoch [40/100], Loss: 2.2595\n",
      "Epoch [40/100], Loss: 2.2934\n",
      "Epoch [40/100], Loss: 2.2967\n",
      "Epoch [40/100], Loss: 2.2958\n",
      "Epoch [40/100], Loss: 2.2852\n",
      "Epoch [40/100], Loss: 2.2906\n",
      "Epoch [40/100], Loss: 2.2753\n",
      "Epoch [40/100], Loss: 2.2820\n",
      "Epoch [40/100], Loss: 2.2688\n",
      "Epoch [40/100], Loss: 2.2960\n",
      "Epoch [40/100], Loss: 2.3028\n",
      "Epoch [50/100], Loss: 2.2940\n",
      "Epoch [50/100], Loss: 2.2914\n",
      "Epoch [50/100], Loss: 2.2699\n",
      "Epoch [50/100], Loss: 2.3024\n",
      "Epoch [50/100], Loss: 2.2729\n",
      "Epoch [50/100], Loss: 2.2518\n",
      "Epoch [50/100], Loss: 2.2650\n",
      "Epoch [50/100], Loss: 2.2685\n",
      "Epoch [50/100], Loss: 2.3056\n",
      "Epoch [50/100], Loss: 2.3117\n",
      "Epoch [50/100], Loss: 2.2682\n",
      "Epoch [50/100], Loss: 2.3272\n",
      "Epoch [50/100], Loss: 2.4227\n",
      "Epoch [60/100], Loss: 2.2777\n",
      "Epoch [60/100], Loss: 2.2719\n",
      "Epoch [60/100], Loss: 2.2622\n",
      "Epoch [60/100], Loss: 2.2781\n",
      "Epoch [60/100], Loss: 2.2830\n",
      "Epoch [60/100], Loss: 2.2919\n",
      "Epoch [60/100], Loss: 2.2948\n",
      "Epoch [60/100], Loss: 2.2871\n",
      "Epoch [60/100], Loss: 2.2946\n",
      "Epoch [60/100], Loss: 2.2864\n",
      "Epoch [60/100], Loss: 2.2867\n",
      "Epoch [60/100], Loss: 2.3015\n",
      "Epoch [60/100], Loss: 2.0503\n",
      "Epoch [70/100], Loss: 2.2839\n",
      "Epoch [70/100], Loss: 2.2728\n",
      "Epoch [70/100], Loss: 2.2684\n",
      "Epoch [70/100], Loss: 2.3077\n",
      "Epoch [70/100], Loss: 2.2727\n",
      "Epoch [70/100], Loss: 2.3035\n",
      "Epoch [70/100], Loss: 2.2958\n",
      "Epoch [70/100], Loss: 2.2735\n",
      "Epoch [70/100], Loss: 2.2968\n",
      "Epoch [70/100], Loss: 2.2840\n",
      "Epoch [70/100], Loss: 2.2764\n",
      "Epoch [70/100], Loss: 2.2738\n",
      "Epoch [70/100], Loss: 2.2152\n",
      "Epoch [80/100], Loss: 2.2983\n",
      "Epoch [80/100], Loss: 2.2575\n",
      "Epoch [80/100], Loss: 2.3001\n",
      "Epoch [80/100], Loss: 2.2688\n",
      "Epoch [80/100], Loss: 2.3010\n",
      "Epoch [80/100], Loss: 2.2752\n",
      "Epoch [80/100], Loss: 2.2674\n",
      "Epoch [80/100], Loss: 2.2610\n",
      "Epoch [80/100], Loss: 2.2856\n",
      "Epoch [80/100], Loss: 2.2842\n",
      "Epoch [80/100], Loss: 2.2906\n",
      "Epoch [80/100], Loss: 2.3051\n",
      "Epoch [80/100], Loss: 2.2783\n",
      "Epoch [90/100], Loss: 2.2777\n",
      "Epoch [90/100], Loss: 2.2717\n",
      "Epoch [90/100], Loss: 2.2864\n",
      "Epoch [90/100], Loss: 2.2644\n",
      "Epoch [90/100], Loss: 2.2940\n",
      "Epoch [90/100], Loss: 2.3013\n",
      "Epoch [90/100], Loss: 2.3054\n",
      "Epoch [90/100], Loss: 2.2758\n",
      "Epoch [90/100], Loss: 2.2841\n",
      "Epoch [90/100], Loss: 2.2907\n",
      "Epoch [90/100], Loss: 2.2802\n",
      "Epoch [90/100], Loss: 2.2947\n",
      "Epoch [90/100], Loss: 2.3926\n",
      "Epoch [100/100], Loss: 2.2753\n",
      "Epoch [100/100], Loss: 2.2705\n",
      "Epoch [100/100], Loss: 2.2856\n",
      "Epoch [100/100], Loss: 2.2833\n",
      "Epoch [100/100], Loss: 2.3111\n",
      "Epoch [100/100], Loss: 2.2870\n",
      "Epoch [100/100], Loss: 2.2870\n",
      "Epoch [100/100], Loss: 2.2682\n",
      "Epoch [100/100], Loss: 2.3076\n",
      "Epoch [100/100], Loss: 2.2817\n",
      "Epoch [100/100], Loss: 2.2969\n",
      "Epoch [100/100], Loss: 2.2761\n",
      "Epoch [100/100], Loss: 2.3493\n",
      "Epoch [10/100], Loss: 2.2723\n",
      "Epoch [10/100], Loss: 2.2719\n",
      "Epoch [10/100], Loss: 2.2928\n",
      "Epoch [10/100], Loss: 2.2905\n",
      "Epoch [10/100], Loss: 2.2906\n",
      "Epoch [10/100], Loss: 2.2900\n",
      "Epoch [10/100], Loss: 2.3002\n",
      "Epoch [10/100], Loss: 2.2849\n",
      "Epoch [10/100], Loss: 2.2927\n",
      "Epoch [10/100], Loss: 2.2827\n",
      "Epoch [10/100], Loss: 2.2987\n",
      "Epoch [10/100], Loss: 2.2960\n",
      "Epoch [10/100], Loss: 2.3749\n",
      "Epoch [20/100], Loss: 2.3000\n",
      "Epoch [20/100], Loss: 2.2879\n",
      "Epoch [20/100], Loss: 2.2978\n",
      "Epoch [20/100], Loss: 2.2872\n",
      "Epoch [20/100], Loss: 2.2843\n",
      "Epoch [20/100], Loss: 2.2906\n",
      "Epoch [20/100], Loss: 2.2836\n",
      "Epoch [20/100], Loss: 2.2781\n",
      "Epoch [20/100], Loss: 2.2941\n",
      "Epoch [20/100], Loss: 2.2826\n",
      "Epoch [20/100], Loss: 2.2915\n",
      "Epoch [20/100], Loss: 2.3016\n",
      "Epoch [20/100], Loss: 2.3069\n",
      "Epoch [30/100], Loss: 2.2988\n",
      "Epoch [30/100], Loss: 2.2974\n",
      "Epoch [30/100], Loss: 2.2890\n",
      "Epoch [30/100], Loss: 2.3060\n",
      "Epoch [30/100], Loss: 2.2837\n",
      "Epoch [30/100], Loss: 2.2793\n",
      "Epoch [30/100], Loss: 2.2931\n",
      "Epoch [30/100], Loss: 2.2930\n",
      "Epoch [30/100], Loss: 2.2959\n",
      "Epoch [30/100], Loss: 2.3077\n",
      "Epoch [30/100], Loss: 2.2926\n",
      "Epoch [30/100], Loss: 2.2858\n",
      "Epoch [30/100], Loss: 2.3813\n",
      "Epoch [40/100], Loss: 2.2924\n",
      "Epoch [40/100], Loss: 2.2908\n",
      "Epoch [40/100], Loss: 2.2949\n",
      "Epoch [40/100], Loss: 2.2874\n",
      "Epoch [40/100], Loss: 2.2906\n",
      "Epoch [40/100], Loss: 2.2806\n",
      "Epoch [40/100], Loss: 2.2916\n",
      "Epoch [40/100], Loss: 2.2799\n",
      "Epoch [40/100], Loss: 2.2916\n",
      "Epoch [40/100], Loss: 2.2944\n",
      "Epoch [40/100], Loss: 2.2974\n",
      "Epoch [40/100], Loss: 2.2821\n",
      "Epoch [40/100], Loss: 2.2863\n",
      "Epoch [50/100], Loss: 2.2889\n",
      "Epoch [50/100], Loss: 2.2988\n",
      "Epoch [50/100], Loss: 2.2855\n",
      "Epoch [50/100], Loss: 2.2971\n",
      "Epoch [50/100], Loss: 2.2974\n",
      "Epoch [50/100], Loss: 2.2929\n",
      "Epoch [50/100], Loss: 2.3051\n",
      "Epoch [50/100], Loss: 2.2876\n",
      "Epoch [50/100], Loss: 2.2899\n",
      "Epoch [50/100], Loss: 2.2921\n",
      "Epoch [50/100], Loss: 2.2827\n",
      "Epoch [50/100], Loss: 2.2973\n",
      "Epoch [50/100], Loss: 2.3227\n",
      "Epoch [60/100], Loss: 2.2626\n",
      "Epoch [60/100], Loss: 2.2898\n",
      "Epoch [60/100], Loss: 2.2901\n",
      "Epoch [60/100], Loss: 2.2811\n",
      "Epoch [60/100], Loss: 2.3024\n",
      "Epoch [60/100], Loss: 2.2950\n",
      "Epoch [60/100], Loss: 2.3080\n",
      "Epoch [60/100], Loss: 2.2909\n",
      "Epoch [60/100], Loss: 2.3010\n",
      "Epoch [60/100], Loss: 2.2863\n",
      "Epoch [60/100], Loss: 2.2962\n",
      "Epoch [60/100], Loss: 2.2838\n",
      "Epoch [60/100], Loss: 2.1474\n",
      "Epoch [70/100], Loss: 2.2784\n",
      "Epoch [70/100], Loss: 2.3029\n",
      "Epoch [70/100], Loss: 2.3102\n",
      "Epoch [70/100], Loss: 2.2672\n",
      "Epoch [70/100], Loss: 2.2943\n",
      "Epoch [70/100], Loss: 2.2821\n",
      "Epoch [70/100], Loss: 2.3001\n",
      "Epoch [70/100], Loss: 2.2834\n",
      "Epoch [70/100], Loss: 2.2876\n",
      "Epoch [70/100], Loss: 2.2867\n",
      "Epoch [70/100], Loss: 2.2919\n",
      "Epoch [70/100], Loss: 2.2962\n",
      "Epoch [70/100], Loss: 2.3434\n",
      "Epoch [80/100], Loss: 2.2943\n",
      "Epoch [80/100], Loss: 2.2899\n",
      "Epoch [80/100], Loss: 2.2792\n",
      "Epoch [80/100], Loss: 2.3026\n",
      "Epoch [80/100], Loss: 2.2823\n",
      "Epoch [80/100], Loss: 2.2882\n",
      "Epoch [80/100], Loss: 2.2843\n",
      "Epoch [80/100], Loss: 2.2983\n",
      "Epoch [80/100], Loss: 2.2878\n",
      "Epoch [80/100], Loss: 2.2972\n",
      "Epoch [80/100], Loss: 2.2882\n",
      "Epoch [80/100], Loss: 2.2979\n",
      "Epoch [80/100], Loss: 2.1669\n",
      "Epoch [90/100], Loss: 2.2942\n",
      "Epoch [90/100], Loss: 2.2826\n",
      "Epoch [90/100], Loss: 2.2920\n",
      "Epoch [90/100], Loss: 2.3008\n",
      "Epoch [90/100], Loss: 2.2970\n",
      "Epoch [90/100], Loss: 2.2918\n",
      "Epoch [90/100], Loss: 2.3111\n",
      "Epoch [90/100], Loss: 2.2839\n",
      "Epoch [90/100], Loss: 2.3052\n",
      "Epoch [90/100], Loss: 2.2845\n",
      "Epoch [90/100], Loss: 2.3026\n",
      "Epoch [90/100], Loss: 2.2768\n",
      "Epoch [90/100], Loss: 2.2322\n",
      "Epoch [100/100], Loss: 2.2876\n",
      "Epoch [100/100], Loss: 2.2819\n",
      "Epoch [100/100], Loss: 2.2836\n",
      "Epoch [100/100], Loss: 2.2843\n",
      "Epoch [100/100], Loss: 2.3053\n",
      "Epoch [100/100], Loss: 2.2880\n",
      "Epoch [100/100], Loss: 2.3120\n",
      "Epoch [100/100], Loss: 2.2691\n",
      "Epoch [100/100], Loss: 2.3001\n",
      "Epoch [100/100], Loss: 2.2842\n",
      "Epoch [100/100], Loss: 2.3007\n",
      "Epoch [100/100], Loss: 2.2851\n",
      "Epoch [100/100], Loss: 2.3212\n",
      "Epoch [10/100], Loss: 2.2903\n",
      "Epoch [10/100], Loss: 2.2677\n",
      "Epoch [10/100], Loss: 2.2937\n",
      "Epoch [10/100], Loss: 2.2896\n",
      "Epoch [10/100], Loss: 2.3079\n",
      "Epoch [10/100], Loss: 2.2930\n",
      "Epoch [10/100], Loss: 2.2752\n",
      "Epoch [10/100], Loss: 2.2901\n",
      "Epoch [10/100], Loss: 2.2729\n",
      "Epoch [10/100], Loss: 2.2908\n",
      "Epoch [10/100], Loss: 2.2629\n",
      "Epoch [10/100], Loss: 2.2793\n",
      "Epoch [10/100], Loss: 2.3766\n",
      "Epoch [20/100], Loss: 2.2770\n",
      "Epoch [20/100], Loss: 2.2674\n",
      "Epoch [20/100], Loss: 2.2886\n",
      "Epoch [20/100], Loss: 2.3019\n",
      "Epoch [20/100], Loss: 2.2814\n",
      "Epoch [20/100], Loss: 2.2990\n",
      "Epoch [20/100], Loss: 2.2906\n",
      "Epoch [20/100], Loss: 2.2772\n",
      "Epoch [20/100], Loss: 2.3006\n",
      "Epoch [20/100], Loss: 2.2924\n",
      "Epoch [20/100], Loss: 2.2912\n",
      "Epoch [20/100], Loss: 2.2962\n",
      "Epoch [20/100], Loss: 2.3377\n",
      "Epoch [30/100], Loss: 2.2893\n",
      "Epoch [30/100], Loss: 2.3018\n",
      "Epoch [30/100], Loss: 2.2889\n",
      "Epoch [30/100], Loss: 2.2857\n",
      "Epoch [30/100], Loss: 2.3028\n",
      "Epoch [30/100], Loss: 2.2843\n",
      "Epoch [30/100], Loss: 2.2815\n",
      "Epoch [30/100], Loss: 2.2887\n",
      "Epoch [30/100], Loss: 2.2809\n",
      "Epoch [30/100], Loss: 2.2875\n",
      "Epoch [30/100], Loss: 2.3007\n",
      "Epoch [30/100], Loss: 2.2844\n",
      "Epoch [30/100], Loss: 2.1017\n",
      "Epoch [40/100], Loss: 2.2851\n",
      "Epoch [40/100], Loss: 2.2945\n",
      "Epoch [40/100], Loss: 2.2791\n",
      "Epoch [40/100], Loss: 2.2926\n",
      "Epoch [40/100], Loss: 2.2807\n",
      "Epoch [40/100], Loss: 2.3034\n",
      "Epoch [40/100], Loss: 2.2986\n",
      "Epoch [40/100], Loss: 2.2753\n",
      "Epoch [40/100], Loss: 2.2876\n",
      "Epoch [40/100], Loss: 2.2942\n",
      "Epoch [40/100], Loss: 2.2956\n",
      "Epoch [40/100], Loss: 2.2913\n",
      "Epoch [40/100], Loss: 2.3618\n",
      "Epoch [50/100], Loss: 2.2927\n",
      "Epoch [50/100], Loss: 2.2816\n",
      "Epoch [50/100], Loss: 2.2786\n",
      "Epoch [50/100], Loss: 2.2982\n",
      "Epoch [50/100], Loss: 2.2835\n",
      "Epoch [50/100], Loss: 2.3135\n",
      "Epoch [50/100], Loss: 2.2857\n",
      "Epoch [50/100], Loss: 2.3183\n",
      "Epoch [50/100], Loss: 2.2733\n",
      "Epoch [50/100], Loss: 2.2810\n",
      "Epoch [50/100], Loss: 2.2818\n",
      "Epoch [50/100], Loss: 2.2575\n",
      "Epoch [50/100], Loss: 2.4008\n",
      "Epoch [60/100], Loss: 2.2935\n",
      "Epoch [60/100], Loss: 2.3000\n",
      "Epoch [60/100], Loss: 2.3075\n",
      "Epoch [60/100], Loss: 2.2626\n",
      "Epoch [60/100], Loss: 2.2930\n",
      "Epoch [60/100], Loss: 2.2930\n",
      "Epoch [60/100], Loss: 2.3023\n",
      "Epoch [60/100], Loss: 2.2572\n",
      "Epoch [60/100], Loss: 2.2945\n",
      "Epoch [60/100], Loss: 2.2993\n",
      "Epoch [60/100], Loss: 2.2780\n",
      "Epoch [60/100], Loss: 2.2714\n",
      "Epoch [60/100], Loss: 2.3851\n",
      "Epoch [70/100], Loss: 2.2830\n",
      "Epoch [70/100], Loss: 2.2878\n",
      "Epoch [70/100], Loss: 2.2835\n",
      "Epoch [70/100], Loss: 2.2916\n",
      "Epoch [70/100], Loss: 2.2952\n",
      "Epoch [70/100], Loss: 2.2802\n",
      "Epoch [70/100], Loss: 2.2740\n",
      "Epoch [70/100], Loss: 2.2972\n",
      "Epoch [70/100], Loss: 2.2816\n",
      "Epoch [70/100], Loss: 2.2899\n",
      "Epoch [70/100], Loss: 2.2852\n",
      "Epoch [70/100], Loss: 2.2961\n",
      "Epoch [70/100], Loss: 2.1395\n",
      "Epoch [80/100], Loss: 2.2668\n",
      "Epoch [80/100], Loss: 2.3143\n",
      "Epoch [80/100], Loss: 2.2821\n",
      "Epoch [80/100], Loss: 2.2674\n",
      "Epoch [80/100], Loss: 2.2744\n",
      "Epoch [80/100], Loss: 2.3034\n",
      "Epoch [80/100], Loss: 2.2746\n",
      "Epoch [80/100], Loss: 2.2729\n",
      "Epoch [80/100], Loss: 2.2792\n",
      "Epoch [80/100], Loss: 2.2988\n",
      "Epoch [80/100], Loss: 2.3011\n",
      "Epoch [80/100], Loss: 2.2905\n",
      "Epoch [80/100], Loss: 2.3990\n",
      "Epoch [90/100], Loss: 2.3072\n",
      "Epoch [90/100], Loss: 2.2911\n",
      "Epoch [90/100], Loss: 2.2940\n",
      "Epoch [90/100], Loss: 2.2996\n",
      "Epoch [90/100], Loss: 2.2979\n",
      "Epoch [90/100], Loss: 2.2652\n",
      "Epoch [90/100], Loss: 2.2759\n",
      "Epoch [90/100], Loss: 2.2931\n",
      "Epoch [90/100], Loss: 2.2732\n",
      "Epoch [90/100], Loss: 2.2904\n",
      "Epoch [90/100], Loss: 2.2872\n",
      "Epoch [90/100], Loss: 2.2578\n",
      "Epoch [90/100], Loss: 2.2674\n",
      "Epoch [100/100], Loss: 2.2704\n",
      "Epoch [100/100], Loss: 2.3028\n",
      "Epoch [100/100], Loss: 2.2912\n",
      "Epoch [100/100], Loss: 2.2877\n",
      "Epoch [100/100], Loss: 2.2821\n",
      "Epoch [100/100], Loss: 2.2837\n",
      "Epoch [100/100], Loss: 2.2785\n",
      "Epoch [100/100], Loss: 2.2924\n",
      "Epoch [100/100], Loss: 2.2993\n",
      "Epoch [100/100], Loss: 2.2705\n",
      "Epoch [100/100], Loss: 2.2944\n",
      "Epoch [100/100], Loss: 2.2812\n",
      "Epoch [100/100], Loss: 2.3632\n",
      "Epoch [10/100], Loss: 2.2880\n",
      "Epoch [10/100], Loss: 2.2711\n",
      "Epoch [10/100], Loss: 2.3025\n",
      "Epoch [10/100], Loss: 2.2796\n",
      "Epoch [10/100], Loss: 2.2960\n",
      "Epoch [10/100], Loss: 2.2631\n",
      "Epoch [10/100], Loss: 2.3052\n",
      "Epoch [10/100], Loss: 2.2774\n",
      "Epoch [10/100], Loss: 2.2819\n",
      "Epoch [10/100], Loss: 2.3011\n",
      "Epoch [10/100], Loss: 2.2757\n",
      "Epoch [10/100], Loss: 2.2924\n",
      "Epoch [10/100], Loss: 2.4128\n",
      "Epoch [20/100], Loss: 2.3008\n",
      "Epoch [20/100], Loss: 2.2879\n",
      "Epoch [20/100], Loss: 2.2848\n",
      "Epoch [20/100], Loss: 2.2765\n",
      "Epoch [20/100], Loss: 2.2752\n",
      "Epoch [20/100], Loss: 2.2985\n",
      "Epoch [20/100], Loss: 2.2896\n",
      "Epoch [20/100], Loss: 2.3056\n",
      "Epoch [20/100], Loss: 2.2733\n",
      "Epoch [20/100], Loss: 2.2826\n",
      "Epoch [20/100], Loss: 2.2863\n",
      "Epoch [20/100], Loss: 2.2831\n",
      "Epoch [20/100], Loss: 2.2395\n",
      "Epoch [30/100], Loss: 2.2927\n",
      "Epoch [30/100], Loss: 2.3005\n",
      "Epoch [30/100], Loss: 2.2728\n",
      "Epoch [30/100], Loss: 2.2930\n",
      "Epoch [30/100], Loss: 2.2806\n",
      "Epoch [30/100], Loss: 2.3003\n",
      "Epoch [30/100], Loss: 2.3100\n",
      "Epoch [30/100], Loss: 2.2829\n",
      "Epoch [30/100], Loss: 2.2931\n",
      "Epoch [30/100], Loss: 2.2707\n",
      "Epoch [30/100], Loss: 2.3074\n",
      "Epoch [30/100], Loss: 2.3247\n",
      "Epoch [30/100], Loss: 2.3110\n",
      "Epoch [40/100], Loss: 2.2915\n",
      "Epoch [40/100], Loss: 2.2754\n",
      "Epoch [40/100], Loss: 2.2720\n",
      "Epoch [40/100], Loss: 2.2880\n",
      "Epoch [40/100], Loss: 2.2715\n",
      "Epoch [40/100], Loss: 2.2914\n",
      "Epoch [40/100], Loss: 2.3068\n",
      "Epoch [40/100], Loss: 2.3141\n",
      "Epoch [40/100], Loss: 2.2832\n",
      "Epoch [40/100], Loss: 2.2749\n",
      "Epoch [40/100], Loss: 2.2890\n",
      "Epoch [40/100], Loss: 2.2965\n",
      "Epoch [40/100], Loss: 2.3656\n",
      "Epoch [50/100], Loss: 2.2637\n",
      "Epoch [50/100], Loss: 2.2977\n",
      "Epoch [50/100], Loss: 2.2831\n",
      "Epoch [50/100], Loss: 2.2832\n",
      "Epoch [50/100], Loss: 2.3014\n",
      "Epoch [50/100], Loss: 2.2860\n",
      "Epoch [50/100], Loss: 2.2901\n",
      "Epoch [50/100], Loss: 2.2802\n",
      "Epoch [50/100], Loss: 2.3015\n",
      "Epoch [50/100], Loss: 2.2655\n",
      "Epoch [50/100], Loss: 2.2816\n",
      "Epoch [50/100], Loss: 2.2978\n",
      "Epoch [50/100], Loss: 2.1838\n",
      "Epoch [60/100], Loss: 2.2903\n",
      "Epoch [60/100], Loss: 2.2973\n",
      "Epoch [60/100], Loss: 2.2778\n",
      "Epoch [60/100], Loss: 2.2986\n",
      "Epoch [60/100], Loss: 2.2810\n",
      "Epoch [60/100], Loss: 2.3151\n",
      "Epoch [60/100], Loss: 2.2938\n",
      "Epoch [60/100], Loss: 2.2930\n",
      "Epoch [60/100], Loss: 2.2810\n",
      "Epoch [60/100], Loss: 2.2635\n",
      "Epoch [60/100], Loss: 2.2839\n",
      "Epoch [60/100], Loss: 2.2944\n",
      "Epoch [60/100], Loss: 2.0100\n",
      "Epoch [70/100], Loss: 2.3047\n",
      "Epoch [70/100], Loss: 2.2834\n",
      "Epoch [70/100], Loss: 2.2852\n",
      "Epoch [70/100], Loss: 2.2884\n",
      "Epoch [70/100], Loss: 2.2650\n",
      "Epoch [70/100], Loss: 2.2874\n",
      "Epoch [70/100], Loss: 2.2953\n",
      "Epoch [70/100], Loss: 2.2881\n",
      "Epoch [70/100], Loss: 2.2673\n",
      "Epoch [70/100], Loss: 2.3187\n",
      "Epoch [70/100], Loss: 2.2666\n",
      "Epoch [70/100], Loss: 2.2844\n",
      "Epoch [70/100], Loss: 2.2458\n",
      "Epoch [80/100], Loss: 2.2591\n",
      "Epoch [80/100], Loss: 2.2750\n",
      "Epoch [80/100], Loss: 2.3052\n",
      "Epoch [80/100], Loss: 2.2939\n",
      "Epoch [80/100], Loss: 2.2987\n",
      "Epoch [80/100], Loss: 2.2871\n",
      "Epoch [80/100], Loss: 2.2902\n",
      "Epoch [80/100], Loss: 2.3028\n",
      "Epoch [80/100], Loss: 2.2965\n",
      "Epoch [80/100], Loss: 2.2752\n",
      "Epoch [80/100], Loss: 2.2722\n",
      "Epoch [80/100], Loss: 2.3056\n",
      "Epoch [80/100], Loss: 2.3920\n",
      "Epoch [90/100], Loss: 2.3094\n",
      "Epoch [90/100], Loss: 2.2969\n",
      "Epoch [90/100], Loss: 2.2880\n",
      "Epoch [90/100], Loss: 2.2962\n",
      "Epoch [90/100], Loss: 2.2880\n",
      "Epoch [90/100], Loss: 2.2584\n",
      "Epoch [90/100], Loss: 2.2847\n",
      "Epoch [90/100], Loss: 2.3003\n",
      "Epoch [90/100], Loss: 2.2628\n",
      "Epoch [90/100], Loss: 2.2937\n",
      "Epoch [90/100], Loss: 2.2702\n",
      "Epoch [90/100], Loss: 2.2986\n",
      "Epoch [90/100], Loss: 2.3896\n",
      "Epoch [100/100], Loss: 2.2851\n",
      "Epoch [100/100], Loss: 2.2907\n",
      "Epoch [100/100], Loss: 2.3020\n",
      "Epoch [100/100], Loss: 2.2855\n",
      "Epoch [100/100], Loss: 2.2847\n",
      "Epoch [100/100], Loss: 2.2943\n",
      "Epoch [100/100], Loss: 2.2803\n",
      "Epoch [100/100], Loss: 2.2954\n",
      "Epoch [100/100], Loss: 2.2887\n",
      "Epoch [100/100], Loss: 2.2817\n",
      "Epoch [100/100], Loss: 2.2891\n",
      "Epoch [100/100], Loss: 2.2837\n",
      "Epoch [100/100], Loss: 2.3958\n",
      "Epoch [10/100], Loss: 2.2831\n",
      "Epoch [10/100], Loss: 2.2696\n",
      "Epoch [10/100], Loss: 2.2868\n",
      "Epoch [10/100], Loss: 2.2939\n",
      "Epoch [10/100], Loss: 2.2935\n",
      "Epoch [10/100], Loss: 2.2984\n",
      "Epoch [10/100], Loss: 2.3017\n",
      "Epoch [10/100], Loss: 2.2879\n",
      "Epoch [10/100], Loss: 2.2823\n",
      "Epoch [10/100], Loss: 2.2867\n",
      "Epoch [10/100], Loss: 2.3135\n",
      "Epoch [10/100], Loss: 2.2955\n",
      "Epoch [10/100], Loss: 2.3589\n",
      "Epoch [20/100], Loss: 2.2743\n",
      "Epoch [20/100], Loss: 2.2934\n",
      "Epoch [20/100], Loss: 2.3027\n",
      "Epoch [20/100], Loss: 2.2889\n",
      "Epoch [20/100], Loss: 2.2956\n",
      "Epoch [20/100], Loss: 2.2894\n",
      "Epoch [20/100], Loss: 2.2909\n",
      "Epoch [20/100], Loss: 2.2839\n",
      "Epoch [20/100], Loss: 2.2824\n",
      "Epoch [20/100], Loss: 2.2973\n",
      "Epoch [20/100], Loss: 2.2971\n",
      "Epoch [20/100], Loss: 2.2961\n",
      "Epoch [20/100], Loss: 2.2268\n",
      "Epoch [30/100], Loss: 2.2882\n",
      "Epoch [30/100], Loss: 2.2821\n",
      "Epoch [30/100], Loss: 2.2926\n",
      "Epoch [30/100], Loss: 2.2981\n",
      "Epoch [30/100], Loss: 2.2893\n",
      "Epoch [30/100], Loss: 2.3035\n",
      "Epoch [30/100], Loss: 2.2955\n",
      "Epoch [30/100], Loss: 2.3071\n",
      "Epoch [30/100], Loss: 2.3076\n",
      "Epoch [30/100], Loss: 2.2843\n",
      "Epoch [30/100], Loss: 2.2769\n",
      "Epoch [30/100], Loss: 2.2873\n",
      "Epoch [30/100], Loss: 2.3558\n",
      "Epoch [40/100], Loss: 2.2888\n",
      "Epoch [40/100], Loss: 2.2844\n",
      "Epoch [40/100], Loss: 2.2914\n",
      "Epoch [40/100], Loss: 2.2917\n",
      "Epoch [40/100], Loss: 2.2963\n",
      "Epoch [40/100], Loss: 2.2916\n",
      "Epoch [40/100], Loss: 2.2968\n",
      "Epoch [40/100], Loss: 2.2942\n",
      "Epoch [40/100], Loss: 2.2885\n",
      "Epoch [40/100], Loss: 2.2856\n",
      "Epoch [40/100], Loss: 2.3081\n",
      "Epoch [40/100], Loss: 2.2864\n",
      "Epoch [40/100], Loss: 2.3106\n",
      "Epoch [50/100], Loss: 2.2934\n",
      "Epoch [50/100], Loss: 2.3077\n",
      "Epoch [50/100], Loss: 2.2705\n",
      "Epoch [50/100], Loss: 2.3147\n",
      "Epoch [50/100], Loss: 2.2780\n",
      "Epoch [50/100], Loss: 2.3104\n",
      "Epoch [50/100], Loss: 2.2717\n",
      "Epoch [50/100], Loss: 2.3255\n",
      "Epoch [50/100], Loss: 2.3016\n",
      "Epoch [50/100], Loss: 2.2984\n",
      "Epoch [50/100], Loss: 2.2905\n",
      "Epoch [50/100], Loss: 2.2920\n",
      "Epoch [50/100], Loss: 2.1225\n",
      "Epoch [60/100], Loss: 2.2888\n",
      "Epoch [60/100], Loss: 2.2897\n",
      "Epoch [60/100], Loss: 2.2833\n",
      "Epoch [60/100], Loss: 2.2840\n",
      "Epoch [60/100], Loss: 2.3083\n",
      "Epoch [60/100], Loss: 2.3065\n",
      "Epoch [60/100], Loss: 2.2823\n",
      "Epoch [60/100], Loss: 2.2916\n",
      "Epoch [60/100], Loss: 2.2802\n",
      "Epoch [60/100], Loss: 2.2980\n",
      "Epoch [60/100], Loss: 2.2970\n",
      "Epoch [60/100], Loss: 2.2855\n",
      "Epoch [60/100], Loss: 2.1495\n",
      "Epoch [70/100], Loss: 2.2999\n",
      "Epoch [70/100], Loss: 2.2883\n",
      "Epoch [70/100], Loss: 2.2838\n",
      "Epoch [70/100], Loss: 2.3006\n",
      "Epoch [70/100], Loss: 2.2912\n",
      "Epoch [70/100], Loss: 2.2873\n",
      "Epoch [70/100], Loss: 2.2925\n",
      "Epoch [70/100], Loss: 2.2883\n",
      "Epoch [70/100], Loss: 2.2746\n",
      "Epoch [70/100], Loss: 2.3150\n",
      "Epoch [70/100], Loss: 2.2798\n",
      "Epoch [70/100], Loss: 2.3151\n",
      "Epoch [70/100], Loss: 2.3188\n",
      "Epoch [80/100], Loss: 2.2980\n",
      "Epoch [80/100], Loss: 2.2915\n",
      "Epoch [80/100], Loss: 2.2923\n",
      "Epoch [80/100], Loss: 2.2924\n",
      "Epoch [80/100], Loss: 2.2969\n",
      "Epoch [80/100], Loss: 2.2956\n",
      "Epoch [80/100], Loss: 2.3204\n",
      "Epoch [80/100], Loss: 2.2726\n",
      "Epoch [80/100], Loss: 2.2889\n",
      "Epoch [80/100], Loss: 2.2727\n",
      "Epoch [80/100], Loss: 2.2959\n",
      "Epoch [80/100], Loss: 2.2837\n",
      "Epoch [80/100], Loss: 2.3767\n",
      "Epoch [90/100], Loss: 2.2876\n",
      "Epoch [90/100], Loss: 2.2883\n",
      "Epoch [90/100], Loss: 2.2921\n",
      "Epoch [90/100], Loss: 2.2909\n",
      "Epoch [90/100], Loss: 2.2796\n",
      "Epoch [90/100], Loss: 2.3003\n",
      "Epoch [90/100], Loss: 2.2840\n",
      "Epoch [90/100], Loss: 2.2906\n",
      "Epoch [90/100], Loss: 2.3027\n",
      "Epoch [90/100], Loss: 2.2814\n",
      "Epoch [90/100], Loss: 2.2895\n",
      "Epoch [90/100], Loss: 2.2905\n",
      "Epoch [90/100], Loss: 2.3820\n",
      "Epoch [100/100], Loss: 2.2888\n",
      "Epoch [100/100], Loss: 2.2842\n",
      "Epoch [100/100], Loss: 2.2824\n",
      "Epoch [100/100], Loss: 2.2874\n",
      "Epoch [100/100], Loss: 2.2858\n",
      "Epoch [100/100], Loss: 2.2926\n",
      "Epoch [100/100], Loss: 2.2946\n",
      "Epoch [100/100], Loss: 2.2905\n",
      "Epoch [100/100], Loss: 2.2992\n",
      "Epoch [100/100], Loss: 2.2903\n",
      "Epoch [100/100], Loss: 2.2916\n",
      "Epoch [100/100], Loss: 2.3166\n",
      "Epoch [100/100], Loss: 2.2882\n",
      "Epoch [10/100], Loss: 2.2956\n",
      "Epoch [10/100], Loss: 2.2871\n",
      "Epoch [10/100], Loss: 2.2493\n",
      "Epoch [10/100], Loss: 2.2807\n",
      "Epoch [10/100], Loss: 2.2922\n",
      "Epoch [10/100], Loss: 2.2888\n",
      "Epoch [10/100], Loss: 2.2906\n",
      "Epoch [10/100], Loss: 2.2738\n",
      "Epoch [10/100], Loss: 2.2827\n",
      "Epoch [10/100], Loss: 2.2818\n",
      "Epoch [10/100], Loss: 2.2701\n",
      "Epoch [10/100], Loss: 2.2814\n",
      "Epoch [10/100], Loss: 2.3434\n",
      "Epoch [20/100], Loss: 2.2709\n",
      "Epoch [20/100], Loss: 2.2614\n",
      "Epoch [20/100], Loss: 2.2453\n",
      "Epoch [20/100], Loss: 2.3022\n",
      "Epoch [20/100], Loss: 2.3065\n",
      "Epoch [20/100], Loss: 2.2844\n",
      "Epoch [20/100], Loss: 2.2741\n",
      "Epoch [20/100], Loss: 2.3269\n",
      "Epoch [20/100], Loss: 2.3048\n",
      "Epoch [20/100], Loss: 2.3024\n",
      "Epoch [20/100], Loss: 2.2836\n",
      "Epoch [20/100], Loss: 2.2786\n",
      "Epoch [20/100], Loss: 2.2035\n",
      "Epoch [30/100], Loss: 2.2963\n",
      "Epoch [30/100], Loss: 2.2921\n",
      "Epoch [30/100], Loss: 2.2661\n",
      "Epoch [30/100], Loss: 2.2429\n",
      "Epoch [30/100], Loss: 2.2942\n",
      "Epoch [30/100], Loss: 2.3091\n",
      "Epoch [30/100], Loss: 2.2858\n",
      "Epoch [30/100], Loss: 2.3373\n",
      "Epoch [30/100], Loss: 2.2930\n",
      "Epoch [30/100], Loss: 2.2779\n",
      "Epoch [30/100], Loss: 2.2827\n",
      "Epoch [30/100], Loss: 2.2976\n",
      "Epoch [30/100], Loss: 2.4398\n",
      "Epoch [40/100], Loss: 2.2931\n",
      "Epoch [40/100], Loss: 2.2815\n",
      "Epoch [40/100], Loss: 2.2609\n",
      "Epoch [40/100], Loss: 2.2692\n",
      "Epoch [40/100], Loss: 2.2954\n",
      "Epoch [40/100], Loss: 2.2998\n",
      "Epoch [40/100], Loss: 2.2725\n",
      "Epoch [40/100], Loss: 2.2808\n",
      "Epoch [40/100], Loss: 2.2882\n",
      "Epoch [40/100], Loss: 2.2881\n",
      "Epoch [40/100], Loss: 2.2709\n",
      "Epoch [40/100], Loss: 2.2771\n",
      "Epoch [40/100], Loss: 2.2488\n",
      "Epoch [50/100], Loss: 2.2745\n",
      "Epoch [50/100], Loss: 2.2770\n",
      "Epoch [50/100], Loss: 2.2761\n",
      "Epoch [50/100], Loss: 2.2621\n",
      "Epoch [50/100], Loss: 2.3175\n",
      "Epoch [50/100], Loss: 2.2980\n",
      "Epoch [50/100], Loss: 2.2652\n",
      "Epoch [50/100], Loss: 2.2821\n",
      "Epoch [50/100], Loss: 2.2769\n",
      "Epoch [50/100], Loss: 2.2762\n",
      "Epoch [50/100], Loss: 2.2944\n",
      "Epoch [50/100], Loss: 2.2813\n",
      "Epoch [50/100], Loss: 2.2873\n",
      "Epoch [60/100], Loss: 2.2623\n",
      "Epoch [60/100], Loss: 2.2827\n",
      "Epoch [60/100], Loss: 2.3001\n",
      "Epoch [60/100], Loss: 2.3145\n",
      "Epoch [60/100], Loss: 2.2940\n",
      "Epoch [60/100], Loss: 2.2624\n",
      "Epoch [60/100], Loss: 2.2923\n",
      "Epoch [60/100], Loss: 2.2736\n",
      "Epoch [60/100], Loss: 2.2813\n",
      "Epoch [60/100], Loss: 2.2439\n",
      "Epoch [60/100], Loss: 2.3060\n",
      "Epoch [60/100], Loss: 2.2703\n",
      "Epoch [60/100], Loss: 2.4077\n",
      "Epoch [70/100], Loss: 2.3081\n",
      "Epoch [70/100], Loss: 2.2622\n",
      "Epoch [70/100], Loss: 2.2848\n",
      "Epoch [70/100], Loss: 2.2652\n",
      "Epoch [70/100], Loss: 2.2945\n",
      "Epoch [70/100], Loss: 2.2852\n",
      "Epoch [70/100], Loss: 2.2486\n",
      "Epoch [70/100], Loss: 2.2732\n",
      "Epoch [70/100], Loss: 2.2890\n",
      "Epoch [70/100], Loss: 2.2922\n",
      "Epoch [70/100], Loss: 2.2903\n",
      "Epoch [70/100], Loss: 2.2644\n",
      "Epoch [70/100], Loss: 2.2245\n",
      "Epoch [80/100], Loss: 2.2846\n",
      "Epoch [80/100], Loss: 2.2819\n",
      "Epoch [80/100], Loss: 2.2589\n",
      "Epoch [80/100], Loss: 2.2847\n",
      "Epoch [80/100], Loss: 2.2870\n",
      "Epoch [80/100], Loss: 2.2823\n",
      "Epoch [80/100], Loss: 2.2815\n",
      "Epoch [80/100], Loss: 2.2596\n",
      "Epoch [80/100], Loss: 2.2950\n",
      "Epoch [80/100], Loss: 2.2893\n",
      "Epoch [80/100], Loss: 2.2816\n",
      "Epoch [80/100], Loss: 2.2684\n",
      "Epoch [80/100], Loss: 2.2158\n",
      "Epoch [90/100], Loss: 2.2699\n",
      "Epoch [90/100], Loss: 2.2621\n",
      "Epoch [90/100], Loss: 2.2902\n",
      "Epoch [90/100], Loss: 2.2854\n",
      "Epoch [90/100], Loss: 2.2632\n",
      "Epoch [90/100], Loss: 2.3003\n",
      "Epoch [90/100], Loss: 2.2816\n",
      "Epoch [90/100], Loss: 2.2770\n",
      "Epoch [90/100], Loss: 2.2789\n",
      "Epoch [90/100], Loss: 2.2630\n",
      "Epoch [90/100], Loss: 2.3107\n",
      "Epoch [90/100], Loss: 2.2717\n",
      "Epoch [90/100], Loss: 2.2807\n",
      "Epoch [100/100], Loss: 2.2894\n",
      "Epoch [100/100], Loss: 2.2349\n",
      "Epoch [100/100], Loss: 2.3063\n",
      "Epoch [100/100], Loss: 2.2677\n",
      "Epoch [100/100], Loss: 2.2698\n",
      "Epoch [100/100], Loss: 2.3048\n",
      "Epoch [100/100], Loss: 2.2754\n",
      "Epoch [100/100], Loss: 2.2744\n",
      "Epoch [100/100], Loss: 2.2730\n",
      "Epoch [100/100], Loss: 2.2968\n",
      "Epoch [100/100], Loss: 2.3002\n",
      "Epoch [100/100], Loss: 2.2650\n",
      "Epoch [100/100], Loss: 2.3458\n",
      "Epoch [10/100], Loss: 2.2896\n",
      "Epoch [10/100], Loss: 2.3003\n",
      "Epoch [10/100], Loss: 2.3022\n",
      "Epoch [10/100], Loss: 2.2865\n",
      "Epoch [10/100], Loss: 2.2781\n",
      "Epoch [10/100], Loss: 2.2932\n",
      "Epoch [10/100], Loss: 2.2778\n",
      "Epoch [10/100], Loss: 2.2984\n",
      "Epoch [10/100], Loss: 2.2681\n",
      "Epoch [10/100], Loss: 2.2653\n",
      "Epoch [10/100], Loss: 2.2798\n",
      "Epoch [10/100], Loss: 2.2975\n",
      "Epoch [10/100], Loss: 2.1835\n",
      "Epoch [20/100], Loss: 2.2827\n",
      "Epoch [20/100], Loss: 2.2886\n",
      "Epoch [20/100], Loss: 2.2960\n",
      "Epoch [20/100], Loss: 2.2825\n",
      "Epoch [20/100], Loss: 2.2915\n",
      "Epoch [20/100], Loss: 2.2794\n",
      "Epoch [20/100], Loss: 2.2898\n",
      "Epoch [20/100], Loss: 2.2933\n",
      "Epoch [20/100], Loss: 2.2839\n",
      "Epoch [20/100], Loss: 2.2856\n",
      "Epoch [20/100], Loss: 2.2805\n",
      "Epoch [20/100], Loss: 2.2908\n",
      "Epoch [20/100], Loss: 2.2116\n",
      "Epoch [30/100], Loss: 2.2717\n",
      "Epoch [30/100], Loss: 2.2968\n",
      "Epoch [30/100], Loss: 2.2532\n",
      "Epoch [30/100], Loss: 2.2845\n",
      "Epoch [30/100], Loss: 2.2735\n",
      "Epoch [30/100], Loss: 2.2852\n",
      "Epoch [30/100], Loss: 2.2885\n",
      "Epoch [30/100], Loss: 2.2935\n",
      "Epoch [30/100], Loss: 2.2839\n",
      "Epoch [30/100], Loss: 2.2982\n",
      "Epoch [30/100], Loss: 2.3102\n",
      "Epoch [30/100], Loss: 2.2775\n",
      "Epoch [30/100], Loss: 2.3020\n",
      "Epoch [40/100], Loss: 2.2921\n",
      "Epoch [40/100], Loss: 2.3119\n",
      "Epoch [40/100], Loss: 2.3051\n",
      "Epoch [40/100], Loss: 2.2918\n",
      "Epoch [40/100], Loss: 2.2842\n",
      "Epoch [40/100], Loss: 2.2906\n",
      "Epoch [40/100], Loss: 2.3088\n",
      "Epoch [40/100], Loss: 2.2728\n",
      "Epoch [40/100], Loss: 2.2854\n",
      "Epoch [40/100], Loss: 2.2714\n",
      "Epoch [40/100], Loss: 2.2755\n",
      "Epoch [40/100], Loss: 2.2713\n",
      "Epoch [40/100], Loss: 2.0593\n",
      "Epoch [50/100], Loss: 2.2905\n",
      "Epoch [50/100], Loss: 2.2748\n",
      "Epoch [50/100], Loss: 2.2975\n",
      "Epoch [50/100], Loss: 2.2889\n",
      "Epoch [50/100], Loss: 2.2757\n",
      "Epoch [50/100], Loss: 2.2890\n",
      "Epoch [50/100], Loss: 2.2819\n",
      "Epoch [50/100], Loss: 2.2965\n",
      "Epoch [50/100], Loss: 2.2695\n",
      "Epoch [50/100], Loss: 2.3073\n",
      "Epoch [50/100], Loss: 2.2876\n",
      "Epoch [50/100], Loss: 2.2786\n",
      "Epoch [50/100], Loss: 2.2400\n",
      "Epoch [60/100], Loss: 2.2754\n",
      "Epoch [60/100], Loss: 2.3014\n",
      "Epoch [60/100], Loss: 2.2712\n",
      "Epoch [60/100], Loss: 2.2664\n",
      "Epoch [60/100], Loss: 2.2892\n",
      "Epoch [60/100], Loss: 2.2998\n",
      "Epoch [60/100], Loss: 2.3012\n",
      "Epoch [60/100], Loss: 2.2745\n",
      "Epoch [60/100], Loss: 2.2864\n",
      "Epoch [60/100], Loss: 2.2826\n",
      "Epoch [60/100], Loss: 2.2846\n",
      "Epoch [60/100], Loss: 2.2846\n",
      "Epoch [60/100], Loss: 2.3551\n",
      "Epoch [70/100], Loss: 2.3022\n",
      "Epoch [70/100], Loss: 2.2824\n",
      "Epoch [70/100], Loss: 2.2682\n",
      "Epoch [70/100], Loss: 2.2876\n",
      "Epoch [70/100], Loss: 2.2896\n",
      "Epoch [70/100], Loss: 2.2947\n",
      "Epoch [70/100], Loss: 2.3056\n",
      "Epoch [70/100], Loss: 2.2705\n",
      "Epoch [70/100], Loss: 2.2944\n",
      "Epoch [70/100], Loss: 2.2706\n",
      "Epoch [70/100], Loss: 2.2669\n",
      "Epoch [70/100], Loss: 2.2902\n",
      "Epoch [70/100], Loss: 2.2832\n",
      "Epoch [80/100], Loss: 2.2814\n",
      "Epoch [80/100], Loss: 2.2980\n",
      "Epoch [80/100], Loss: 2.2808\n",
      "Epoch [80/100], Loss: 2.2974\n",
      "Epoch [80/100], Loss: 2.2956\n",
      "Epoch [80/100], Loss: 2.2603\n",
      "Epoch [80/100], Loss: 2.2712\n",
      "Epoch [80/100], Loss: 2.2803\n",
      "Epoch [80/100], Loss: 2.2867\n",
      "Epoch [80/100], Loss: 2.2975\n",
      "Epoch [80/100], Loss: 2.2837\n",
      "Epoch [80/100], Loss: 2.3062\n",
      "Epoch [80/100], Loss: 2.4005\n",
      "Epoch [90/100], Loss: 2.2870\n",
      "Epoch [90/100], Loss: 2.2775\n",
      "Epoch [90/100], Loss: 2.2932\n",
      "Epoch [90/100], Loss: 2.2949\n",
      "Epoch [90/100], Loss: 2.2729\n",
      "Epoch [90/100], Loss: 2.2736\n",
      "Epoch [90/100], Loss: 2.3048\n",
      "Epoch [90/100], Loss: 2.2992\n",
      "Epoch [90/100], Loss: 2.2471\n",
      "Epoch [90/100], Loss: 2.3295\n",
      "Epoch [90/100], Loss: 2.3063\n",
      "Epoch [90/100], Loss: 2.2940\n",
      "Epoch [90/100], Loss: 2.2996\n",
      "Epoch [100/100], Loss: 2.3002\n",
      "Epoch [100/100], Loss: 2.2760\n",
      "Epoch [100/100], Loss: 2.2773\n",
      "Epoch [100/100], Loss: 2.2883\n",
      "Epoch [100/100], Loss: 2.2985\n",
      "Epoch [100/100], Loss: 2.2859\n",
      "Epoch [100/100], Loss: 2.2732\n",
      "Epoch [100/100], Loss: 2.2811\n",
      "Epoch [100/100], Loss: 2.2802\n",
      "Epoch [100/100], Loss: 2.2837\n",
      "Epoch [100/100], Loss: 2.2843\n",
      "Epoch [100/100], Loss: 2.2999\n",
      "Epoch [100/100], Loss: 2.3632\n",
      "Epoch [10/100], Loss: 2.2955\n",
      "Epoch [10/100], Loss: 2.2796\n",
      "Epoch [10/100], Loss: 2.3136\n",
      "Epoch [10/100], Loss: 2.2941\n",
      "Epoch [10/100], Loss: 2.2829\n",
      "Epoch [10/100], Loss: 2.2927\n",
      "Epoch [10/100], Loss: 2.2930\n",
      "Epoch [10/100], Loss: 2.2935\n",
      "Epoch [10/100], Loss: 2.2931\n",
      "Epoch [10/100], Loss: 2.2864\n",
      "Epoch [10/100], Loss: 2.2916\n",
      "Epoch [10/100], Loss: 2.2838\n",
      "Epoch [10/100], Loss: 2.2218\n",
      "Epoch [20/100], Loss: 2.2982\n",
      "Epoch [20/100], Loss: 2.3000\n",
      "Epoch [20/100], Loss: 2.2944\n",
      "Epoch [20/100], Loss: 2.3027\n",
      "Epoch [20/100], Loss: 2.2902\n",
      "Epoch [20/100], Loss: 2.3008\n",
      "Epoch [20/100], Loss: 2.2812\n",
      "Epoch [20/100], Loss: 2.3044\n",
      "Epoch [20/100], Loss: 2.2772\n",
      "Epoch [20/100], Loss: 2.3008\n",
      "Epoch [20/100], Loss: 2.2751\n",
      "Epoch [20/100], Loss: 2.2746\n",
      "Epoch [20/100], Loss: 2.2664\n",
      "Epoch [30/100], Loss: 2.2851\n",
      "Epoch [30/100], Loss: 2.2931\n",
      "Epoch [30/100], Loss: 2.2962\n",
      "Epoch [30/100], Loss: 2.2934\n",
      "Epoch [30/100], Loss: 2.2918\n",
      "Epoch [30/100], Loss: 2.2898\n",
      "Epoch [30/100], Loss: 2.2856\n",
      "Epoch [30/100], Loss: 2.2803\n",
      "Epoch [30/100], Loss: 2.2856\n",
      "Epoch [30/100], Loss: 2.2960\n",
      "Epoch [30/100], Loss: 2.2916\n",
      "Epoch [30/100], Loss: 2.2971\n",
      "Epoch [30/100], Loss: 2.3771\n",
      "Epoch [40/100], Loss: 2.2887\n",
      "Epoch [40/100], Loss: 2.2892\n",
      "Epoch [40/100], Loss: 2.2923\n",
      "Epoch [40/100], Loss: 2.2765\n",
      "Epoch [40/100], Loss: 2.2893\n",
      "Epoch [40/100], Loss: 2.2909\n",
      "Epoch [40/100], Loss: 2.3081\n",
      "Epoch [40/100], Loss: 2.2943\n",
      "Epoch [40/100], Loss: 2.2928\n",
      "Epoch [40/100], Loss: 2.2933\n",
      "Epoch [40/100], Loss: 2.2964\n",
      "Epoch [40/100], Loss: 2.3040\n",
      "Epoch [40/100], Loss: 2.3472\n",
      "Epoch [50/100], Loss: 2.3037\n",
      "Epoch [50/100], Loss: 2.2992\n",
      "Epoch [50/100], Loss: 2.2918\n",
      "Epoch [50/100], Loss: 2.2895\n",
      "Epoch [50/100], Loss: 2.3001\n",
      "Epoch [50/100], Loss: 2.3036\n",
      "Epoch [50/100], Loss: 2.2984\n",
      "Epoch [50/100], Loss: 2.2861\n",
      "Epoch [50/100], Loss: 2.2995\n",
      "Epoch [50/100], Loss: 2.2828\n",
      "Epoch [50/100], Loss: 2.2931\n",
      "Epoch [50/100], Loss: 2.2846\n",
      "Epoch [50/100], Loss: 2.2734\n",
      "Epoch [60/100], Loss: 2.2900\n",
      "Epoch [60/100], Loss: 2.2888\n",
      "Epoch [60/100], Loss: 2.2930\n",
      "Epoch [60/100], Loss: 2.2958\n",
      "Epoch [60/100], Loss: 2.2807\n",
      "Epoch [60/100], Loss: 2.2816\n",
      "Epoch [60/100], Loss: 2.3002\n",
      "Epoch [60/100], Loss: 2.2852\n",
      "Epoch [60/100], Loss: 2.2905\n",
      "Epoch [60/100], Loss: 2.3143\n",
      "Epoch [60/100], Loss: 2.2971\n",
      "Epoch [60/100], Loss: 2.2837\n",
      "Epoch [60/100], Loss: 2.3425\n",
      "Epoch [70/100], Loss: 2.3022\n",
      "Epoch [70/100], Loss: 2.3055\n",
      "Epoch [70/100], Loss: 2.2896\n",
      "Epoch [70/100], Loss: 2.2756\n",
      "Epoch [70/100], Loss: 2.2880\n",
      "Epoch [70/100], Loss: 2.2815\n",
      "Epoch [70/100], Loss: 2.2992\n",
      "Epoch [70/100], Loss: 2.2812\n",
      "Epoch [70/100], Loss: 2.3016\n",
      "Epoch [70/100], Loss: 2.2900\n",
      "Epoch [70/100], Loss: 2.2938\n",
      "Epoch [70/100], Loss: 2.2831\n",
      "Epoch [70/100], Loss: 2.3648\n",
      "Epoch [80/100], Loss: 2.2875\n",
      "Epoch [80/100], Loss: 2.2837\n",
      "Epoch [80/100], Loss: 2.3052\n",
      "Epoch [80/100], Loss: 2.2888\n",
      "Epoch [80/100], Loss: 2.2962\n",
      "Epoch [80/100], Loss: 2.2893\n",
      "Epoch [80/100], Loss: 2.2823\n",
      "Epoch [80/100], Loss: 2.3113\n",
      "Epoch [80/100], Loss: 2.2938\n",
      "Epoch [80/100], Loss: 2.2871\n",
      "Epoch [80/100], Loss: 2.3025\n",
      "Epoch [80/100], Loss: 2.2923\n",
      "Epoch [80/100], Loss: 2.2807\n",
      "Epoch [90/100], Loss: 2.2917\n",
      "Epoch [90/100], Loss: 2.2983\n",
      "Epoch [90/100], Loss: 2.2828\n",
      "Epoch [90/100], Loss: 2.2914\n",
      "Epoch [90/100], Loss: 2.2944\n",
      "Epoch [90/100], Loss: 2.3064\n",
      "Epoch [90/100], Loss: 2.2858\n",
      "Epoch [90/100], Loss: 2.2917\n",
      "Epoch [90/100], Loss: 2.2888\n",
      "Epoch [90/100], Loss: 2.2854\n",
      "Epoch [90/100], Loss: 2.2874\n",
      "Epoch [90/100], Loss: 2.2928\n",
      "Epoch [90/100], Loss: 2.3884\n",
      "Epoch [100/100], Loss: 2.3034\n",
      "Epoch [100/100], Loss: 2.3018\n",
      "Epoch [100/100], Loss: 2.2878\n",
      "Epoch [100/100], Loss: 2.2986\n",
      "Epoch [100/100], Loss: 2.2793\n",
      "Epoch [100/100], Loss: 2.2862\n",
      "Epoch [100/100], Loss: 2.3024\n",
      "Epoch [100/100], Loss: 2.2835\n",
      "Epoch [100/100], Loss: 2.2971\n",
      "Epoch [100/100], Loss: 2.2876\n",
      "Epoch [100/100], Loss: 2.2922\n",
      "Epoch [100/100], Loss: 2.2954\n",
      "Epoch [100/100], Loss: 2.2533\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Define Dataset\n",
    "# Encode y_train using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "\n",
    "# Convert X_train_selected and X_test_selected to a tensor\n",
    "X_train_np = X_train_norm\n",
    "X_test_np = X_test_norm\n",
    "# X_train_np = X_train.values\n",
    "# X_test_np = X_test.values\n",
    "X_train_torch = torch.tensor(X_train_np).float().requires_grad_(True)\n",
    "X_test_torch = torch.tensor(X_test_np).float().requires_grad_(True)\n",
    "m,n=X_train_torch.shape\n",
    "\n",
    "# Convert y_train_encoded and y_test_encoded to a tensor\n",
    "y_train_torch = torch.tensor(y_train_encoded).long()\n",
    "y_test_torch = torch.tensor(y_test_encoded).long()\n",
    "\n",
    "\n",
    "# Define the DNN model\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, n, n_hidden, n_output, dropout_rate, l2_lambda):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(self.n, self.n_hidden)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        #self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.fc3 = nn.Linear(self.n_hidden, self.n_output)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        l2_reg = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)**2\n",
    "        return l2_reg\n",
    "\n",
    "# Define the bootstrapping function\n",
    "def bootstrap(X, y, n_bootstraps):\n",
    "    X_bootstraps = []\n",
    "    y_bootstraps = []\n",
    "    for i in range(n_bootstraps):\n",
    "        idxs = np.random.choice(range(len(X)), size=len(X), replace=True)\n",
    "        X_bootstraps.append(X[idxs])\n",
    "        y_bootstraps.append(y[idxs])\n",
    "    return X_bootstraps, y_bootstraps\n",
    "\n",
    "# Define hyperparameters\n",
    "n_inputs = n\n",
    "n_hidden = 30\n",
    "n_output = 10\n",
    "dropout_rate = 0.2\n",
    "l2_lambda = 0.001\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Setting the model to train mode\n",
    "model.train()\n",
    "\n",
    "# Define the model, criterion, optimizer, and learning rate\n",
    "model = DNNModel(n_inputs, n_hidden, n_output, dropout_rate, l2_lambda)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "# Train the model using bootstrapping\n",
    "n_bootstraps = 10\n",
    "X_bootstraps, y_bootstraps = bootstrap(X_train_torch, y_train_torch, n_bootstraps)\n",
    "for i in range(n_bootstraps):\n",
    "    X_train_boot = X_bootstraps[i]\n",
    "    y_train_boot = y_bootstraps[i]\n",
    "    dataset = TensorDataset(X_train_boot, y_train_boot)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss += l2_lambda * model.l2_regularization()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            # Print training loss every 10 epochs\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "46c04212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        36\n",
      "           1       0.00      0.00      0.00        36\n",
      "           2       0.00      0.00      0.00        33\n",
      "           3       0.00      0.00      0.00        23\n",
      "           4       0.00      0.00      0.00        38\n",
      "           5       0.00      0.00      0.00        26\n",
      "           6       0.00      0.00      0.00        53\n",
      "           7       0.00      0.00      0.00        33\n",
      "           8       0.00      0.00      0.00        49\n",
      "           9       0.15      1.00      0.26        58\n",
      "\n",
      "    accuracy                           0.15       385\n",
      "   macro avg       0.02      0.10      0.03       385\n",
      "weighted avg       0.02      0.15      0.04       385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbp/miniforge3/envs/pytorchbook/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mbp/miniforge3/envs/pytorchbook/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mbp/miniforge3/envs/pytorchbook/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_torch)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    print(classification_report(y_test_torch.numpy(), predicted.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "92c823986b3fa953f4b86b7349397e3d2be04485601d5422eb9395e8cc70ddba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
