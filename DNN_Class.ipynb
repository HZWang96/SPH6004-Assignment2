{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5be150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %pip install pandas\\n%pip install numpy\\n%pip install scikit-learn\\n%pip install seaborn\\n%pip install matplotlib\\n%pip install skorch #for wrapping pytorch models in sklearn\\n '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" %pip install pandas\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install skorch #for wrapping pytorch models in sklearn\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45464378-7b13-4150-a871-746e983f1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163134e8-fc53-4a08-81b1-740ea714ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_data = pd.read_csv('sph_dynamic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95691140-3053-4489-b915-af3db738a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = pd.read_csv('sph_static.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f38196-d822-41d8-8698-7382ec2e0e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id                   0\n",
       "charttime                 0\n",
       "total_protein          6930\n",
       "calcium                 933\n",
       "creatinine              261\n",
       "glucose                 444\n",
       "sodium                  214\n",
       "chloride                241\n",
       "heart_rate             6833\n",
       "sbp                    6895\n",
       "dbp                    6895\n",
       "mbp                    6887\n",
       "resp_rate              6832\n",
       "temperature            6974\n",
       "hemoglobin             1179\n",
       "wbc                    1207\n",
       "alt                    3964\n",
       "ast                    3936\n",
       "alp                    3976\n",
       "bilirubin_total        3957\n",
       "bilirubin_direct       6808\n",
       "bilirubin_indirect     6812\n",
       "ph                     7004\n",
       "lactate                7012\n",
       "pt                     3068\n",
       "urineoutput            6942\n",
       "sofa_respiration       7005\n",
       "sofa_coagulation       7023\n",
       "sofa_liver             7023\n",
       "sofa_cardiovascular    6872\n",
       "sofa_cns               6979\n",
       "sofa_renal             7024\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values in Dynamic Table\n",
    "dynamic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c0670c-b56b-40c0-bbf3-8092bdf0c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with more than 80%\n",
    "for col in dynamic_data.columns:\n",
    "    if dynamic_data[col].isnull().sum() > len(dynamic_data)*0.8:\n",
    "        del dynamic_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5463f9bf-a76f-4c9c-85b7-b95c63f1b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id               0\n",
       "charttime             0\n",
       "calcium             933\n",
       "creatinine          261\n",
       "glucose             444\n",
       "sodium              214\n",
       "chloride            241\n",
       "hemoglobin         1179\n",
       "wbc                1207\n",
       "alt                3964\n",
       "ast                3936\n",
       "alp                3976\n",
       "bilirubin_total    3957\n",
       "pt                 3068\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90ba5554-d3a5-4578-ae46-449b46ad3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mervin\\AppData\\Local\\Temp\\ipykernel_2680\\240455136.py:15: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  dynamic_data = dynamic_data.groupby('stay_id').apply(liver_categorize)\n"
     ]
    }
   ],
   "source": [
    "# ['alt','ast','alp','bilirubin_total','pt'] are liver function related test results\n",
    "# create a new binary column 'liver_function_test', True/1 means have ever taken liver function test\n",
    "liver_test_result = ['alt','ast','alp','bilirubin_total','pt']\n",
    "def liver_categorize(group):\n",
    "    flag = True\n",
    "    for i in liver_test_result:\n",
    "        if group[i].notnull().any():\n",
    "            flag = False\n",
    "    if flag:\n",
    "        group['liver_function_test'] = False\n",
    "    else:\n",
    "        group['liver_function_test'] = True\n",
    "    return group\n",
    "\n",
    "dynamic_data = dynamic_data.groupby('stay_id').apply(liver_categorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08376e4-8ca0-4718-ae71-71a6c8ebf390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>calcium</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>hemoglobin</th>\n",
       "      <th>wbc</th>\n",
       "      <th>alt</th>\n",
       "      <th>ast</th>\n",
       "      <th>alp</th>\n",
       "      <th>bilirubin_total</th>\n",
       "      <th>pt</th>\n",
       "      <th>liver_function_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35715575</td>\n",
       "      <td>2148-12-27 18:15:00.000</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>137.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34483718</td>\n",
       "      <td>2118-01-04 03:58:00.000</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>129.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31826892</td>\n",
       "      <td>2163-03-10 19:59:00.000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36154799</td>\n",
       "      <td>2131-12-02 19:14:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32732521</td>\n",
       "      <td>2116-08-12 12:45:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>31292653</td>\n",
       "      <td>2192-03-18 03:14:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>102.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7020</th>\n",
       "      <td>32964221</td>\n",
       "      <td>2127-01-30 10:00:00.000</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>112.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7021</th>\n",
       "      <td>33493321</td>\n",
       "      <td>2142-07-28 06:02:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1</td>\n",
       "      <td>130.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>38658392</td>\n",
       "      <td>2189-05-17 00:13:00.000</td>\n",
       "      <td>7.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>37805633</td>\n",
       "      <td>2172-07-28 21:25:00.000</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>155.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7024 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stay_id                charttime  calcium  creatinine  glucose  sodium  \\\n",
       "0     35715575  2148-12-27 18:15:00.000      8.5         0.9    137.0   138.0   \n",
       "1     34483718  2118-01-04 03:58:00.000      8.2         0.8    129.0   141.0   \n",
       "2     31826892  2163-03-10 19:59:00.000      7.7         0.4    112.0   136.0   \n",
       "3     36154799  2131-12-02 19:14:00.000      NaN         NaN      NaN     NaN   \n",
       "4     32732521  2116-08-12 12:45:00.000      NaN         4.0    135.0   139.0   \n",
       "...        ...                      ...      ...         ...      ...     ...   \n",
       "7019  31292653  2192-03-18 03:14:00.000      NaN         1.4    102.0   137.0   \n",
       "7020  32964221  2127-01-30 10:00:00.000      8.6         0.5    112.0   139.0   \n",
       "7021  33493321  2142-07-28 06:02:00.000      NaN         1.1    130.0   142.0   \n",
       "7022  38658392  2189-05-17 00:13:00.000      7.3         1.0    174.0   133.0   \n",
       "7023  37805633  2172-07-28 21:25:00.000      8.3         0.9    155.0   144.0   \n",
       "\n",
       "      chloride  hemoglobin   wbc   alt   ast    alp  bilirubin_total    pt  \\\n",
       "0        104.0         NaN   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "1        101.0         8.7  11.3   NaN   NaN    NaN              NaN  12.1   \n",
       "2         98.0         NaN   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "3          NaN        12.3   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "4        105.0         NaN   NaN   NaN   NaN    NaN              NaN   NaN   \n",
       "...        ...         ...   ...   ...   ...    ...              ...   ...   \n",
       "7019     103.0         8.7   4.9   NaN   NaN    NaN              NaN   NaN   \n",
       "7020     107.0         8.9  14.3  14.0  32.0  148.0              2.6   NaN   \n",
       "7021     105.0         8.4   4.0   NaN   NaN    NaN              NaN   NaN   \n",
       "7022      93.0        13.0  19.5   9.0  18.0   48.0              0.5  13.0   \n",
       "7023     110.0        12.1   5.4   NaN   NaN    NaN              NaN   NaN   \n",
       "\n",
       "      liver_function_test  \n",
       "0                    True  \n",
       "1                    True  \n",
       "2                    True  \n",
       "3                    True  \n",
       "4                    True  \n",
       "...                   ...  \n",
       "7019                 True  \n",
       "7020                 True  \n",
       "7021                 True  \n",
       "7022                 True  \n",
       "7023                False  \n",
       "\n",
       "[7024 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a6d1b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>calcium</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>hemoglobin</th>\n",
       "      <th>wbc</th>\n",
       "      <th>alt</th>\n",
       "      <th>ast</th>\n",
       "      <th>alp</th>\n",
       "      <th>bilirubin_total</th>\n",
       "      <th>pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.024000e+03</td>\n",
       "      <td>6091.000000</td>\n",
       "      <td>6763.000000</td>\n",
       "      <td>6580.000000</td>\n",
       "      <td>6810.000000</td>\n",
       "      <td>6783.000000</td>\n",
       "      <td>5845.000000</td>\n",
       "      <td>5817.000000</td>\n",
       "      <td>3060.000000</td>\n",
       "      <td>3088.000000</td>\n",
       "      <td>3048.000000</td>\n",
       "      <td>3067.000000</td>\n",
       "      <td>3956.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.497892e+07</td>\n",
       "      <td>8.389640</td>\n",
       "      <td>1.644019</td>\n",
       "      <td>142.289666</td>\n",
       "      <td>137.309545</td>\n",
       "      <td>102.067079</td>\n",
       "      <td>10.043353</td>\n",
       "      <td>10.973130</td>\n",
       "      <td>161.859150</td>\n",
       "      <td>243.882772</td>\n",
       "      <td>131.933727</td>\n",
       "      <td>4.248745</td>\n",
       "      <td>18.826567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.832325e+06</td>\n",
       "      <td>0.820066</td>\n",
       "      <td>1.839893</td>\n",
       "      <td>89.875986</td>\n",
       "      <td>5.638384</td>\n",
       "      <td>6.731008</td>\n",
       "      <td>2.177573</td>\n",
       "      <td>8.228807</td>\n",
       "      <td>752.898832</td>\n",
       "      <td>1216.527439</td>\n",
       "      <td>123.088598</td>\n",
       "      <td>7.808056</td>\n",
       "      <td>11.588015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000414e+07</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.255070e+07</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.496990e+07</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.746081e+07</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>20.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.999217e+07</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>2970.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>125.200000</td>\n",
       "      <td>15018.000000</td>\n",
       "      <td>28275.000000</td>\n",
       "      <td>1185.000000</td>\n",
       "      <td>52.600000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stay_id      calcium   creatinine      glucose       sodium  \\\n",
       "count  7.024000e+03  6091.000000  6763.000000  6580.000000  6810.000000   \n",
       "mean   3.497892e+07     8.389640     1.644019   142.289666   137.309545   \n",
       "std    2.832325e+06     0.820066     1.839893    89.875986     5.638384   \n",
       "min    3.000414e+07     4.200000     0.100000    30.000000    83.000000   \n",
       "25%    3.255070e+07     7.900000     0.800000   102.000000   134.000000   \n",
       "50%    3.496990e+07     8.400000     1.100000   125.000000   138.000000   \n",
       "75%    3.746081e+07     8.900000     1.800000   159.000000   141.000000   \n",
       "max    3.999217e+07    12.300000    19.700000  2970.000000   185.000000   \n",
       "\n",
       "          chloride   hemoglobin          wbc           alt           ast  \\\n",
       "count  6783.000000  5845.000000  5817.000000   3060.000000   3088.000000   \n",
       "mean    102.067079    10.043353    10.973130    161.859150    243.882772   \n",
       "std       6.731008     2.177573     8.228807    752.898832   1216.527439   \n",
       "min      62.000000     3.900000     0.100000      1.000000      5.000000   \n",
       "25%      98.000000     8.400000     5.900000     17.000000     22.000000   \n",
       "50%     102.000000     9.800000     9.400000     30.000000     42.000000   \n",
       "75%     106.000000    11.400000    14.200000     62.000000     95.000000   \n",
       "max     153.000000    18.400000   125.200000  15018.000000  28275.000000   \n",
       "\n",
       "               alp  bilirubin_total           pt  \n",
       "count  3048.000000      3067.000000  3956.000000  \n",
       "mean    131.933727         4.248745    18.826567  \n",
       "std     123.088598         7.808056    11.588015  \n",
       "min       7.000000         0.100000     9.200000  \n",
       "25%      65.000000         0.500000    12.800000  \n",
       "50%      92.000000         1.000000    14.900000  \n",
       "75%     149.000000         3.600000    20.400000  \n",
       "max    1185.000000        52.600000   150.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c002b8dc-5de6-4b18-8fb0-97c29c1b92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that patients with no relevant results recorded don't have liver issues\n",
    "# so we impute these patients' missing values of these columns with random number in normal range\n",
    "\n",
    "# but i can not find the unit and normal range for them so i drop them first >_<\n",
    "# dynamic_data.drop(['alt','ast','alp','bilirubin_total','pt'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f57b483-f95d-4d87-bc58-8317a9ddafd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id                   0\n",
       "charttime                 0\n",
       "calcium                 933\n",
       "creatinine              261\n",
       "glucose                 444\n",
       "sodium                  214\n",
       "chloride                241\n",
       "hemoglobin             1179\n",
       "wbc                    1207\n",
       "alt                    3964\n",
       "ast                    3936\n",
       "alp                    3976\n",
       "bilirubin_total        3957\n",
       "pt                     3068\n",
       "liver_function_test       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6278aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: alt, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: ast, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: alp, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: bilirubin_total, dtype: float64\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: pt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alt'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'ast'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alp'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'bilirubin_total'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'pt'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "026725c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KNN to impute the rest\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors = 10)\n",
    "dynamic_data.iloc[:,2:] = imputer.fit_transform(dynamic_data.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f54e7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define normal ranges for each column\n",
    "normal_ranges = {\n",
    "    'alt': (5, 40),\n",
    "    'ast': (10, 35),\n",
    "    'alp': (40, 130),\n",
    "    'bilirubin_total': (0.1, 1.0),\n",
    "    'pt': (9.5, 13.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a9dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = dynamic_data['liver_function_test'] == False\n",
    "n_no_test = mask.sum()\n",
    "def sample_normal(col):\n",
    "    lower = normal_ranges[col][0]\n",
    "    upper = normal_ranges[col][1]\n",
    "    return np.random.normal(loc=(lower+upper)/2, scale=(upper-lower)/6, size=n_no_test)\n",
    "\n",
    "sampled_alt = sample_normal(\"alt\")\n",
    "sampled_ast = sample_normal(\"ast\")\n",
    "sampled_alp = sample_normal(\"alp\")\n",
    "sampled_bilirubin_total = sample_normal(\"bilirubin_total\")\n",
    "sampled_pt = sample_normal(\"pt\")\n",
    "dynamic_data.loc[mask, 'alt'] = sampled_alt\n",
    "dynamic_data.loc[mask, 'ast'] = sampled_ast\n",
    "dynamic_data.loc[mask, 'alp'] = sampled_alp\n",
    "dynamic_data.loc[mask, 'bilirubin_total'] = sampled_bilirubin_total\n",
    "dynamic_data.loc[mask, 'pt'] = sampled_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "073bb8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    349.000000\n",
      "mean      22.322735\n",
      "std        5.704281\n",
      "min        6.452952\n",
      "25%       18.397575\n",
      "50%       21.789975\n",
      "75%       26.564911\n",
      "max       38.753696\n",
      "Name: alt, dtype: float64\n",
      "count    349.000000\n",
      "mean      22.418722\n",
      "std        3.818373\n",
      "min       11.547158\n",
      "25%       20.050159\n",
      "50%       22.413922\n",
      "75%       24.943593\n",
      "max       31.478248\n",
      "Name: ast, dtype: float64\n",
      "count    349.000000\n",
      "mean      84.950863\n",
      "std       15.755805\n",
      "min       44.231031\n",
      "25%       73.380964\n",
      "50%       85.609513\n",
      "75%       96.379514\n",
      "max      122.632505\n",
      "Name: alp, dtype: float64\n",
      "count    349.000000\n",
      "mean       0.545112\n",
      "std        0.161028\n",
      "min        0.088599\n",
      "25%        0.433105\n",
      "50%        0.546973\n",
      "75%        0.646089\n",
      "max        1.005938\n",
      "Name: bilirubin_total, dtype: float64\n",
      "count    349.000000\n",
      "mean      11.503562\n",
      "std        0.618739\n",
      "min        9.453288\n",
      "25%       11.051869\n",
      "50%       11.454811\n",
      "75%       11.937679\n",
      "max       13.241363\n",
      "Name: pt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alt'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'ast'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'alp'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'bilirubin_total'].describe())\n",
    "print(dynamic_data.loc[dynamic_data['liver_function_test']==False, 'pt'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b71c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to address the same patinent have differrent results at the same charttime\n",
    "dynamic_data = dynamic_data.groupby(['stay_id','charttime']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86cd74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the gradient\n",
    "def count_gradient(group):\n",
    "    testresult = list(dynamic_data.columns)[2:-1]\n",
    "    for i in testresult:\n",
    "        if len(group) == 1:\n",
    "            group[i+'_grad'] = 0\n",
    "        else:\n",
    "            time_diff = (group['charttime'].iloc[-1] - group['charttime'].iloc[-2]).total_seconds()\n",
    "            group[i+'_grad'] = (group[i].iloc[-1] - group[i].iloc[-2]) / time_diff \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff28e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_data['charttime'] = pd.to_datetime(dynamic_data['charttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e0f0416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mervin\\AppData\\Local\\Temp\\ipykernel_2680\\3194640149.py:2: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  dynamic_data = dynamic_data.groupby('stay_id').apply(count_gradient)\n"
     ]
    }
   ],
   "source": [
    "dynamic_data.sort_values(by = ['stay_id','charttime'], inplace = True)\n",
    "dynamic_data = dynamic_data.groupby('stay_id').apply(count_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ce7aaf6-91bc-4a25-b833-eabac6e3d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the last values of all timepoints for each patient\n",
    "dynamic_data_last = dynamic_data.drop(['charttime'],axis = 1).groupby('stay_id').tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39bb1bfb-2a71-49d1-a941-333d27d8d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dynamic and static table\n",
    "data = static_data.merge(dynamic_data_last, on = 'stay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebdde6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>icu_intime</th>\n",
       "      <th>vent_start</th>\n",
       "      <th>vent_end</th>\n",
       "      <th>vent_duration</th>\n",
       "      <th>calcium</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>...</th>\n",
       "      <th>glucose_grad</th>\n",
       "      <th>sodium_grad</th>\n",
       "      <th>chloride_grad</th>\n",
       "      <th>hemoglobin_grad</th>\n",
       "      <th>wbc_grad</th>\n",
       "      <th>alt_grad</th>\n",
       "      <th>ast_grad</th>\n",
       "      <th>alp_grad</th>\n",
       "      <th>bilirubin_total_grad</th>\n",
       "      <th>pt_grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30004144</td>\n",
       "      <td>2126-04-04 13:20:25.000</td>\n",
       "      <td>4/5/26 16:00</td>\n",
       "      <td>4/6/26 17:00</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.7</td>\n",
       "      <td>133.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>-0.001782</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30005366</td>\n",
       "      <td>2202-12-27 17:36:59.000</td>\n",
       "      <td>12/28/02 14:00</td>\n",
       "      <td>12/28/02 20:00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6.7</td>\n",
       "      <td>41.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30006983</td>\n",
       "      <td>2159-10-12 03:56:42.000</td>\n",
       "      <td>10/12/59 18:00</td>\n",
       "      <td>10/14/59 19:00</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>7.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>-0.000606</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30023204</td>\n",
       "      <td>2124-07-09 16:43:55.000</td>\n",
       "      <td>7/11/24 16:00</td>\n",
       "      <td>7/12/24 16:10</td>\n",
       "      <td>24.166667</td>\n",
       "      <td>8.50</td>\n",
       "      <td>1.4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30031418</td>\n",
       "      <td>2156-03-05 14:11:00.000</td>\n",
       "      <td>3/7/56 22:06</td>\n",
       "      <td>3/8/56 8:00</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>7.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>133.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>39977971</td>\n",
       "      <td>2115-12-11 17:42:45.000</td>\n",
       "      <td>12/12/15 12:00</td>\n",
       "      <td>12/12/15 16:00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.90</td>\n",
       "      <td>2.2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003474</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.002295</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>39982332</td>\n",
       "      <td>2180-03-01 22:35:04.000</td>\n",
       "      <td>3/2/80 19:00</td>\n",
       "      <td>3/3/80 8:00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.82</td>\n",
       "      <td>1.2</td>\n",
       "      <td>119.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>39985110</td>\n",
       "      <td>2141-03-03 05:57:46.000</td>\n",
       "      <td>3/4/41 20:44</td>\n",
       "      <td>3/6/41 4:00</td>\n",
       "      <td>31.266667</td>\n",
       "      <td>10.40</td>\n",
       "      <td>6.8</td>\n",
       "      <td>149.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>-0.001323</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>39986206</td>\n",
       "      <td>2183-06-19 23:25:31.000</td>\n",
       "      <td>6/20/83 22:00</td>\n",
       "      <td>6/30/83 4:00</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>39992167</td>\n",
       "      <td>2114-06-10 19:00:00.000</td>\n",
       "      <td>6/11/14 17:00</td>\n",
       "      <td>6/15/14 5:00</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>136.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.006194</td>\n",
       "      <td>-0.007773</td>\n",
       "      <td>-0.009651</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stay_id               icu_intime      vent_start        vent_end  \\\n",
       "0     30004144  2126-04-04 13:20:25.000    4/5/26 16:00    4/6/26 17:00   \n",
       "1     30005366  2202-12-27 17:36:59.000  12/28/02 14:00  12/28/02 20:00   \n",
       "2     30006983  2159-10-12 03:56:42.000  10/12/59 18:00  10/14/59 19:00   \n",
       "3     30023204  2124-07-09 16:43:55.000   7/11/24 16:00   7/12/24 16:10   \n",
       "4     30031418  2156-03-05 14:11:00.000    3/7/56 22:06     3/8/56 8:00   \n",
       "...        ...                      ...             ...             ...   \n",
       "1918  39977971  2115-12-11 17:42:45.000  12/12/15 12:00  12/12/15 16:00   \n",
       "1919  39982332  2180-03-01 22:35:04.000    3/2/80 19:00     3/3/80 8:00   \n",
       "1920  39985110  2141-03-03 05:57:46.000    3/4/41 20:44     3/6/41 4:00   \n",
       "1921  39986206  2183-06-19 23:25:31.000   6/20/83 22:00    6/30/83 4:00   \n",
       "1922  39992167  2114-06-10 19:00:00.000   6/11/14 17:00    6/15/14 5:00   \n",
       "\n",
       "      vent_duration  calcium  creatinine  glucose  sodium  chloride  ...  \\\n",
       "0         25.000000     6.80         0.7    133.0   135.0     102.0  ...   \n",
       "1          6.000000     8.80         6.7     41.0   139.0     100.0  ...   \n",
       "2         49.000000     7.10         1.0     89.0   136.0     108.0  ...   \n",
       "3         24.166667     8.50         1.4    107.0   131.0     100.0  ...   \n",
       "4          9.900000     7.40         0.4    133.0   139.0     106.0  ...   \n",
       "...             ...      ...         ...      ...     ...       ...  ...   \n",
       "1918       4.000000     8.90         2.2     98.0   132.0      97.0  ...   \n",
       "1919      13.000000     8.82         1.2    119.0   140.0     103.0  ...   \n",
       "1920      31.266667    10.40         6.8    149.0   139.0      98.0  ...   \n",
       "1921     222.000000     7.50         6.0    101.0   139.0     103.0  ...   \n",
       "1922      84.000000     7.50         0.8    136.0   128.0     100.0  ...   \n",
       "\n",
       "      glucose_grad  sodium_grad  chloride_grad  hemoglobin_grad  wbc_grad  \\\n",
       "0         0.001466     0.000000      -0.000077        -0.000093 -0.000023   \n",
       "1         0.000000     0.000000       0.000000         0.000000  0.000000   \n",
       "2        -0.001230     0.000073       0.000110        -0.000015  0.000022   \n",
       "3         0.000403    -0.000050      -0.000201         0.000076 -0.000222   \n",
       "4         0.000000     0.000000       0.000000         0.000000  0.000000   \n",
       "...            ...          ...            ...              ...       ...   \n",
       "1918     -0.003474     0.000100       0.000033        -0.000037 -0.000017   \n",
       "1919      0.000000     0.000000       0.000000         0.000000  0.000000   \n",
       "1920      0.000425     0.000018       0.000000         0.000007  0.000052   \n",
       "1921     -0.000866     0.000000       0.000000         0.000017 -0.000039   \n",
       "1922      0.001674    -0.000218       0.000000         0.000000 -0.000102   \n",
       "\n",
       "      alt_grad  ast_grad  alp_grad  bilirubin_total_grad   pt_grad  \n",
       "0    -0.000216 -0.001782  0.002238              0.000002  0.000093  \n",
       "1     0.000000  0.000000  0.000000              0.000000  0.000000  \n",
       "2     0.000055  0.000275 -0.000606              0.000004  0.000031  \n",
       "3     0.002110  0.001334  0.000534             -0.000062 -0.000226  \n",
       "4     0.000000  0.000000  0.000000              0.000000  0.000000  \n",
       "...        ...       ...       ...                   ...       ...  \n",
       "1918 -0.002295  0.000852 -0.000658             -0.000005  0.000010  \n",
       "1919  0.000000  0.000000  0.000000              0.000000  0.000000  \n",
       "1920  0.000575  0.000778 -0.001323              0.000013  0.000032  \n",
       "1921  0.002264  0.002580  0.003697              0.000011  0.000013  \n",
       "1922 -0.006194 -0.007773 -0.009651              0.000131  0.000022  \n",
       "\n",
       "[1923 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b40ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data type of icu_intime, vent_start and vent_end to date and time format\n",
    "data['icu_intime'] = pd.to_datetime(data['icu_intime'])\n",
    "data['vent_start'] = pd.to_datetime(data['vent_start'], format='%m/%d/%y %H:%M')\n",
    "data['vent_end'] = pd.to_datetime(data['vent_end'], format='%m/%d/%y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c33d740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id                          int64\n",
       "icu_intime              datetime64[ns]\n",
       "vent_start              datetime64[ns]\n",
       "vent_end                datetime64[ns]\n",
       "vent_duration                  float64\n",
       "calcium                        float64\n",
       "creatinine                     float64\n",
       "glucose                        float64\n",
       "sodium                         float64\n",
       "chloride                       float64\n",
       "hemoglobin                     float64\n",
       "wbc                            float64\n",
       "alt                            float64\n",
       "ast                            float64\n",
       "alp                            float64\n",
       "bilirubin_total                float64\n",
       "pt                             float64\n",
       "liver_function_test            float64\n",
       "calcium_grad                   float64\n",
       "creatinine_grad                float64\n",
       "glucose_grad                   float64\n",
       "sodium_grad                    float64\n",
       "chloride_grad                  float64\n",
       "hemoglobin_grad                float64\n",
       "wbc_grad                       float64\n",
       "alt_grad                       float64\n",
       "ast_grad                       float64\n",
       "alp_grad                       float64\n",
       "bilirubin_total_grad           float64\n",
       "pt_grad                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e3da8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-12 hours       352\n",
       "over 72 hours    294\n",
       "12-18 hours      289\n",
       "0-6 hours        272\n",
       "24-42 hours      256\n",
       "42-72 hours      241\n",
       "18-24 hours      219\n",
       "Name: vent_duration_category, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorize vent_duration \n",
    "data['vent_duration_category'] = pd.cut(data['vent_duration'], bins=[0,6,12,18,24,42,72,np.inf],\n",
    "                                         labels=['0-6 hours', '6-12 hours', '12-18 hours', '18-24 hours',\n",
    "                                                 '24-42 hours', '42-72 hours','over 72 hours'])\n",
    "data['vent_duration_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a5fc1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#Dropped icu_intime, vent_start and vent_end as they are usable for correlation\n",
    "#Creating correlation matrix to observe the relationship between variables\n",
    "df_corr = data.loc[:,'vent_duration':'pt_grad'].corr()\n",
    "\n",
    "#Setting up plots\n",
    "#f, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "#Setting up lower triangle correlation matrix\n",
    "#mask = np.triu(np.ones(df_corr.shape), k=0).astype(bool)\n",
    "#cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "#sns.heatmap(df_corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "651cd34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting pairplot to assess the relationship and distribution of each variable. \n",
    "#sns.pairplot(data.loc[:,'vent_duration':'pt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b9c3036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: (1923, 31)\n",
      "Trimmed Shape: (1296, 31)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Scale the numeric columns using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "# Create a new dataframe with the scaled numeric columns\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=numeric_cols, index=data.index)\n",
    "\n",
    "# Identify outliers with z-scores greater than 2.5 or less than -2.5 (95%)\n",
    "z_scores = (scaled_df - scaled_df.mean()) / scaled_df.std()\n",
    "outliers = data[(z_scores > 2.5).any(axis=1) | (z_scores < -2.5).any(axis=1)]\n",
    "\n",
    "# Remove outliers from the original dataframe\n",
    "trimmed_df = data.drop(outliers.index)\n",
    "\n",
    "print(\"Original Shape:\", data.shape)\n",
    "print(\"Trimmed Shape:\", trimmed_df.shape)\n",
    "#print(sns.pairplot(trimmed_df.loc[:,'vent_duration':'pt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1fd52d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating correlation matrix to observe the relationship between variables\n",
    "trimmed_df_corr = trimmed_df.loc[:,'vent_duration':'pt_grad'].corr()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87069e56",
   "metadata": {},
   "source": [
    "## Splitting Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6de74863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting Datasets into Train and Test\n",
    "X= data.drop(['stay_id', 'icu_intime', 'vent_start', 'vent_end','vent_duration', 'vent_duration_category'], axis=1)\n",
    "y= data['vent_duration_category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split to train & test set (8:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e2b8b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1538, 25)\n",
      "y_train: (1538,)\n",
      "X_test: (385, 25)\n",
      "y_test: (385,)\n"
     ]
    }
   ],
   "source": [
    "# The dimension of the training set\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "\n",
    "# The dimension of the test set\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "095eb7ae",
   "metadata": {},
   "source": [
    "## Normalising Dataset after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "91f10aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean:  0.3995145255507676\n",
      "Training set std:  0.36955481888088537\n",
      "Test set mean:  0.40121762017517903\n",
      "Test set std:  0.3699436336238242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Calculate the normalization parameters on the training set\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Normalize the training set using the calculated parameters\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_train_norm_df = pd.DataFrame(X_train_norm, columns=X_train.columns)\n",
    "\n",
    "# Normalize the test set using the same normalization parameters as the training set\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "X_test_norm_df = pd.DataFrame(X_test_norm, columns=X_test.columns)\n",
    "\n",
    "# Verify that the mean and standard deviation of the training and test sets are similar\n",
    "print(\"Training set mean: \", np.mean(X_train_norm))\n",
    "print(\"Training set std: \", np.std(X_train_norm))\n",
    "print(\"Test set mean: \", np.mean(X_test_norm))\n",
    "print(\"Test set std: \", np.std(X_test_norm))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d07b19",
   "metadata": {},
   "source": [
    "## Feature Selection Using Recursive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8ff292ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       calcium  creatinine   glucose    sodium  chloride  hemoglobin  \\\n",
      "0     0.515625    0.087629  0.229541  0.422535  0.414634    0.379577   \n",
      "1     0.421875    0.061856  0.173653  0.535211  0.548780    0.373239   \n",
      "2     0.531250    0.041237  0.165669  0.450704  0.426829    0.612676   \n",
      "3     0.359375    0.025773  0.207585  0.422535  0.439024    0.549296   \n",
      "4     0.468750    0.036082  0.187625  0.464789  0.451220    0.429577   \n",
      "...        ...         ...       ...       ...       ...         ...   \n",
      "1533  0.359375    0.067010  0.219561  0.478873  0.426829    0.619718   \n",
      "1534  0.546875    0.041237  0.137725  0.464789  0.487805    0.640845   \n",
      "1535  0.453125    0.056701  0.107784  0.408451  0.365854    0.542254   \n",
      "1536  0.531250    0.072165  0.409182  0.450704  0.426829    0.352113   \n",
      "1537  0.421875    0.587629  0.211577  0.450704  0.414634    0.274648   \n",
      "\n",
      "           wbc       alt       ast       alp  ...  glucose_grad  sodium_grad  \\\n",
      "0     0.084607  0.062711  0.116021  0.292140  ...      0.975656     0.299534   \n",
      "1     0.747644  0.000716  0.001852  0.266098  ...      0.972396     0.290959   \n",
      "2     0.117277  0.006817  0.003976  0.206723  ...      0.972396     0.290959   \n",
      "3     0.307853  0.002704  0.002575  0.103220  ...      0.973320     0.276758   \n",
      "4     0.064921  0.001487  0.000280  0.057110  ...      0.971355     0.310476   \n",
      "...        ...       ...       ...       ...  ...           ...          ...   \n",
      "1533  0.158115  0.007469  0.005223  0.198295  ...      0.967559     0.278235   \n",
      "1534  0.115183  0.030305  0.014051  0.075758  ...      0.955207     0.334092   \n",
      "1535  0.167539  0.006952  0.007834  0.185417  ...      0.971819     0.312579   \n",
      "1536  0.081675  0.007612  0.003885  0.205398  ...      0.972396     0.290959   \n",
      "1537  0.169634  0.007588  0.004644  0.177557  ...      0.974641     0.278492   \n",
      "\n",
      "      chloride_grad  hemoglobin_grad  wbc_grad  alt_grad  ast_grad  alp_grad  \\\n",
      "0          0.082224         0.932514  0.113563  0.944679  0.737704  0.982922   \n",
      "1          0.081818         0.936061  0.119501  0.933157  0.691004  0.982700   \n",
      "2          0.081818         0.936061  0.119501  0.933157  0.691004  0.982700   \n",
      "3          0.081146         0.938677  0.125079  0.933196  0.691152  0.982703   \n",
      "4          0.082280         0.933543  0.113076  0.932982  0.690689  0.982646   \n",
      "...             ...              ...       ...       ...       ...       ...   \n",
      "1533       0.080614         0.955858  0.129623  0.919174  0.691150  0.982967   \n",
      "1534       0.085389         0.939152  0.119501  0.933831  0.690290  0.982716   \n",
      "1535       0.082330         0.930306  0.119610  0.934259  0.694067  0.983100   \n",
      "1536       0.081818         0.936061  0.119501  0.933157  0.691004  0.982700   \n",
      "1537       0.080639         0.934529  0.119002  0.935161  0.693419  0.983054   \n",
      "\n",
      "      bilirubin_total_grad   pt_grad  \n",
      "0                 0.962379  0.659741  \n",
      "1                 0.961294  0.629214  \n",
      "2                 0.961294  0.629214  \n",
      "3                 0.961562  0.629719  \n",
      "4                 0.961453  0.633775  \n",
      "...                    ...       ...  \n",
      "1533              0.960356  0.582480  \n",
      "1534              0.961468  0.580049  \n",
      "1535              0.961405  0.624758  \n",
      "1536              0.961294  0.629214  \n",
      "1537              0.961670  0.665999  \n",
      "\n",
      "[1538 rows x 25 columns]\n",
      "426       12-18 hours\n",
      "141         0-6 hours\n",
      "1323        0-6 hours\n",
      "692        6-12 hours\n",
      "1797    over 72 hours\n",
      "            ...      \n",
      "1130    over 72 hours\n",
      "1294      12-18 hours\n",
      "860       18-24 hours\n",
      "1459      12-18 hours\n",
      "1126      42-72 hours\n",
      "Name: vent_duration_category, Length: 1538, dtype: category\n",
      "Categories (7, object): ['0-6 hours' < '6-12 hours' < '12-18 hours' < '18-24 hours' < '24-42 hours' < '42-72 hours' < 'over 72 hours']\n"
     ]
    }
   ],
   "source": [
    "# listing columns\n",
    "print(X_train_norm_df)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "90bdecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform feature selection using SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_train_selected = selector.fit_transform(X_train_norm_df, y_train_encoded)\n",
    "X_test_selected = selector.transform(X_test_norm_df)\n",
    "\n",
    "X_train.columns[selector.get_support()]\n",
    "m, n = X_train_selected.shape\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7c19b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['calcium', 'creatinine', 'alt', 'alp', 'pt'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# showing the selected features\n",
    "print(X_train.columns[selector.get_support()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e79d673",
   "metadata": {},
   "source": [
    "## DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "71b01dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 30]             180\n",
      "              ReLU-2                   [-1, 30]               0\n",
      "           Dropout-3                   [-1, 30]               0\n",
      "            Linear-4                   [-1, 30]             930\n",
      "              ReLU-5                   [-1, 30]               0\n",
      "           Dropout-6                   [-1, 30]               0\n",
      "            Linear-7                    [-1, 7]             217\n",
      "           Softmax-8                    [-1, 7]               0\n",
      "================================================================\n",
      "Total params: 1,327\n",
      "Trainable params: 1,327\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n",
      "Epoch [10/300], Loss: 1.9684\n",
      "Epoch [10/300], Loss: 1.9677\n",
      "Epoch [10/300], Loss: 1.9661\n",
      "Epoch [10/300], Loss: 1.9686\n",
      "Epoch [10/300], Loss: 1.9691\n",
      "Epoch [10/300], Loss: 1.9697\n",
      "Epoch [10/300], Loss: 1.9665\n",
      "Epoch [10/300], Loss: 1.9710\n",
      "Epoch [10/300], Loss: 1.9634\n",
      "Epoch [10/300], Loss: 1.9681\n",
      "Epoch [10/300], Loss: 1.9706\n",
      "Epoch [10/300], Loss: 1.9663\n",
      "Epoch [10/300], Loss: 1.9527\n",
      "Epoch [20/300], Loss: 1.9654\n",
      "Epoch [20/300], Loss: 1.9689\n",
      "Epoch [20/300], Loss: 1.9639\n",
      "Epoch [20/300], Loss: 1.9662\n",
      "Epoch [20/300], Loss: 1.9698\n",
      "Epoch [20/300], Loss: 1.9644\n",
      "Epoch [20/300], Loss: 1.9628\n",
      "Epoch [20/300], Loss: 1.9662\n",
      "Epoch [20/300], Loss: 1.9621\n",
      "Epoch [20/300], Loss: 1.9652\n",
      "Epoch [20/300], Loss: 1.9651\n",
      "Epoch [20/300], Loss: 1.9650\n",
      "Epoch [20/300], Loss: 1.9468\n",
      "Epoch [30/300], Loss: 1.9617\n",
      "Epoch [30/300], Loss: 1.9604\n",
      "Epoch [30/300], Loss: 1.9628\n",
      "Epoch [30/300], Loss: 1.9651\n",
      "Epoch [30/300], Loss: 1.9655\n",
      "Epoch [30/300], Loss: 1.9639\n",
      "Epoch [30/300], Loss: 1.9597\n",
      "Epoch [30/300], Loss: 1.9634\n",
      "Epoch [30/300], Loss: 1.9618\n",
      "Epoch [30/300], Loss: 1.9651\n",
      "Epoch [30/300], Loss: 1.9638\n",
      "Epoch [30/300], Loss: 1.9611\n",
      "Epoch [30/300], Loss: 1.9615\n",
      "Epoch [40/300], Loss: 1.9598\n",
      "Epoch [40/300], Loss: 1.9605\n",
      "Epoch [40/300], Loss: 1.9627\n",
      "Epoch [40/300], Loss: 1.9616\n",
      "Epoch [40/300], Loss: 1.9609\n",
      "Epoch [40/300], Loss: 1.9624\n",
      "Epoch [40/300], Loss: 1.9627\n",
      "Epoch [40/300], Loss: 1.9595\n",
      "Epoch [40/300], Loss: 1.9621\n",
      "Epoch [40/300], Loss: 1.9623\n",
      "Epoch [40/300], Loss: 1.9589\n",
      "Epoch [40/300], Loss: 1.9588\n",
      "Epoch [40/300], Loss: 1.9438\n",
      "Epoch [50/300], Loss: 1.9608\n",
      "Epoch [50/300], Loss: 1.9591\n",
      "Epoch [50/300], Loss: 1.9582\n",
      "Epoch [50/300], Loss: 1.9578\n",
      "Epoch [50/300], Loss: 1.9586\n",
      "Epoch [50/300], Loss: 1.9615\n",
      "Epoch [50/300], Loss: 1.9577\n",
      "Epoch [50/300], Loss: 1.9602\n",
      "Epoch [50/300], Loss: 1.9564\n",
      "Epoch [50/300], Loss: 1.9640\n",
      "Epoch [50/300], Loss: 1.9584\n",
      "Epoch [50/300], Loss: 1.9591\n",
      "Epoch [50/300], Loss: 1.9622\n",
      "Epoch [60/300], Loss: 1.9603\n",
      "Epoch [60/300], Loss: 1.9555\n",
      "Epoch [60/300], Loss: 1.9588\n",
      "Epoch [60/300], Loss: 1.9550\n",
      "Epoch [60/300], Loss: 1.9585\n",
      "Epoch [60/300], Loss: 1.9570\n",
      "Epoch [60/300], Loss: 1.9600\n",
      "Epoch [60/300], Loss: 1.9588\n",
      "Epoch [60/300], Loss: 1.9565\n",
      "Epoch [60/300], Loss: 1.9577\n",
      "Epoch [60/300], Loss: 1.9572\n",
      "Epoch [60/300], Loss: 1.9583\n",
      "Epoch [60/300], Loss: 1.9368\n",
      "Epoch [70/300], Loss: 1.9588\n",
      "Epoch [70/300], Loss: 1.9556\n",
      "Epoch [70/300], Loss: 1.9582\n",
      "Epoch [70/300], Loss: 1.9540\n",
      "Epoch [70/300], Loss: 1.9577\n",
      "Epoch [70/300], Loss: 1.9544\n",
      "Epoch [70/300], Loss: 1.9594\n",
      "Epoch [70/300], Loss: 1.9574\n",
      "Epoch [70/300], Loss: 1.9553\n",
      "Epoch [70/300], Loss: 1.9561\n",
      "Epoch [70/300], Loss: 1.9554\n",
      "Epoch [70/300], Loss: 1.9557\n",
      "Epoch [70/300], Loss: 1.9623\n",
      "Epoch [80/300], Loss: 1.9550\n",
      "Epoch [80/300], Loss: 1.9556\n",
      "Epoch [80/300], Loss: 1.9564\n",
      "Epoch [80/300], Loss: 1.9587\n",
      "Epoch [80/300], Loss: 1.9537\n",
      "Epoch [80/300], Loss: 1.9568\n",
      "Epoch [80/300], Loss: 1.9546\n",
      "Epoch [80/300], Loss: 1.9572\n",
      "Epoch [80/300], Loss: 1.9545\n",
      "Epoch [80/300], Loss: 1.9540\n",
      "Epoch [80/300], Loss: 1.9567\n",
      "Epoch [80/300], Loss: 1.9522\n",
      "Epoch [80/300], Loss: 1.9498\n",
      "Epoch [90/300], Loss: 1.9566\n",
      "Epoch [90/300], Loss: 1.9534\n",
      "Epoch [90/300], Loss: 1.9521\n",
      "Epoch [90/300], Loss: 1.9524\n",
      "Epoch [90/300], Loss: 1.9551\n",
      "Epoch [90/300], Loss: 1.9558\n",
      "Epoch [90/300], Loss: 1.9547\n",
      "Epoch [90/300], Loss: 1.9534\n",
      "Epoch [90/300], Loss: 1.9526\n",
      "Epoch [90/300], Loss: 1.9537\n",
      "Epoch [90/300], Loss: 1.9539\n",
      "Epoch [90/300], Loss: 1.9556\n",
      "Epoch [90/300], Loss: 1.9416\n",
      "Epoch [100/300], Loss: 1.9532\n",
      "Epoch [100/300], Loss: 1.9515\n",
      "Epoch [100/300], Loss: 1.9549\n",
      "Epoch [100/300], Loss: 1.9542\n",
      "Epoch [100/300], Loss: 1.9520\n",
      "Epoch [100/300], Loss: 1.9528\n",
      "Epoch [100/300], Loss: 1.9519\n",
      "Epoch [100/300], Loss: 1.9554\n",
      "Epoch [100/300], Loss: 1.9527\n",
      "Epoch [100/300], Loss: 1.9532\n",
      "Epoch [100/300], Loss: 1.9552\n",
      "Epoch [100/300], Loss: 1.9504\n",
      "Epoch [100/300], Loss: 1.9356\n",
      "Epoch [110/300], Loss: 1.9522\n",
      "Epoch [110/300], Loss: 1.9500\n",
      "Epoch [110/300], Loss: 1.9531\n",
      "Epoch [110/300], Loss: 1.9506\n",
      "Epoch [110/300], Loss: 1.9533\n",
      "Epoch [110/300], Loss: 1.9513\n",
      "Epoch [110/300], Loss: 1.9525\n",
      "Epoch [110/300], Loss: 1.9542\n",
      "Epoch [110/300], Loss: 1.9509\n",
      "Epoch [110/300], Loss: 1.9534\n",
      "Epoch [110/300], Loss: 1.9550\n",
      "Epoch [110/300], Loss: 1.9518\n",
      "Epoch [110/300], Loss: 1.9672\n",
      "Epoch [120/300], Loss: 1.9520\n",
      "Epoch [120/300], Loss: 1.9529\n",
      "Epoch [120/300], Loss: 1.9522\n",
      "Epoch [120/300], Loss: 1.9508\n",
      "Epoch [120/300], Loss: 1.9517\n",
      "Epoch [120/300], Loss: 1.9501\n",
      "Epoch [120/300], Loss: 1.9516\n",
      "Epoch [120/300], Loss: 1.9521\n",
      "Epoch [120/300], Loss: 1.9496\n",
      "Epoch [120/300], Loss: 1.9490\n",
      "Epoch [120/300], Loss: 1.9518\n",
      "Epoch [120/300], Loss: 1.9510\n",
      "Epoch [120/300], Loss: 1.9433\n",
      "Epoch [130/300], Loss: 1.9481\n",
      "Epoch [130/300], Loss: 1.9465\n",
      "Epoch [130/300], Loss: 1.9522\n",
      "Epoch [130/300], Loss: 1.9533\n",
      "Epoch [130/300], Loss: 1.9533\n",
      "Epoch [130/300], Loss: 1.9485\n",
      "Epoch [130/300], Loss: 1.9516\n",
      "Epoch [130/300], Loss: 1.9509\n",
      "Epoch [130/300], Loss: 1.9496\n",
      "Epoch [130/300], Loss: 1.9511\n",
      "Epoch [130/300], Loss: 1.9506\n",
      "Epoch [130/300], Loss: 1.9509\n",
      "Epoch [130/300], Loss: 1.9401\n",
      "Epoch [140/300], Loss: 1.9514\n",
      "Epoch [140/300], Loss: 1.9526\n",
      "Epoch [140/300], Loss: 1.9503\n",
      "Epoch [140/300], Loss: 1.9512\n",
      "Epoch [140/300], Loss: 1.9457\n",
      "Epoch [140/300], Loss: 1.9482\n",
      "Epoch [140/300], Loss: 1.9537\n",
      "Epoch [140/300], Loss: 1.9521\n",
      "Epoch [140/300], Loss: 1.9486\n",
      "Epoch [140/300], Loss: 1.9477\n",
      "Epoch [140/300], Loss: 1.9494\n",
      "Epoch [140/300], Loss: 1.9486\n",
      "Epoch [140/300], Loss: 1.9529\n",
      "Epoch [150/300], Loss: 1.9484\n",
      "Epoch [150/300], Loss: 1.9485\n",
      "Epoch [150/300], Loss: 1.9534\n",
      "Epoch [150/300], Loss: 1.9500\n",
      "Epoch [150/300], Loss: 1.9502\n",
      "Epoch [150/300], Loss: 1.9521\n",
      "Epoch [150/300], Loss: 1.9457\n",
      "Epoch [150/300], Loss: 1.9466\n",
      "Epoch [150/300], Loss: 1.9463\n",
      "Epoch [150/300], Loss: 1.9486\n",
      "Epoch [150/300], Loss: 1.9490\n",
      "Epoch [150/300], Loss: 1.9525\n",
      "Epoch [150/300], Loss: 1.9725\n",
      "Epoch [160/300], Loss: 1.9480\n",
      "Epoch [160/300], Loss: 1.9503\n",
      "Epoch [160/300], Loss: 1.9468\n",
      "Epoch [160/300], Loss: 1.9516\n",
      "Epoch [160/300], Loss: 1.9495\n",
      "Epoch [160/300], Loss: 1.9484\n",
      "Epoch [160/300], Loss: 1.9455\n",
      "Epoch [160/300], Loss: 1.9512\n",
      "Epoch [160/300], Loss: 1.9426\n",
      "Epoch [160/300], Loss: 1.9525\n",
      "Epoch [160/300], Loss: 1.9468\n",
      "Epoch [160/300], Loss: 1.9490\n",
      "Epoch [160/300], Loss: 1.9686\n",
      "Epoch [170/300], Loss: 1.9509\n",
      "Epoch [170/300], Loss: 1.9469\n",
      "Epoch [170/300], Loss: 1.9476\n",
      "Epoch [170/300], Loss: 1.9461\n",
      "Epoch [170/300], Loss: 1.9468\n",
      "Epoch [170/300], Loss: 1.9473\n",
      "Epoch [170/300], Loss: 1.9472\n",
      "Epoch [170/300], Loss: 1.9493\n",
      "Epoch [170/300], Loss: 1.9497\n",
      "Epoch [170/300], Loss: 1.9491\n",
      "Epoch [170/300], Loss: 1.9505\n",
      "Epoch [170/300], Loss: 1.9496\n",
      "Epoch [170/300], Loss: 1.9354\n",
      "Epoch [180/300], Loss: 1.9488\n",
      "Epoch [180/300], Loss: 1.9494\n",
      "Epoch [180/300], Loss: 1.9479\n",
      "Epoch [180/300], Loss: 1.9444\n",
      "Epoch [180/300], Loss: 1.9491\n",
      "Epoch [180/300], Loss: 1.9469\n",
      "Epoch [180/300], Loss: 1.9459\n",
      "Epoch [180/300], Loss: 1.9502\n",
      "Epoch [180/300], Loss: 1.9488\n",
      "Epoch [180/300], Loss: 1.9516\n",
      "Epoch [180/300], Loss: 1.9460\n",
      "Epoch [180/300], Loss: 1.9470\n",
      "Epoch [180/300], Loss: 1.9429\n",
      "Epoch [190/300], Loss: 1.9477\n",
      "Epoch [190/300], Loss: 1.9504\n",
      "Epoch [190/300], Loss: 1.9480\n",
      "Epoch [190/300], Loss: 1.9509\n",
      "Epoch [190/300], Loss: 1.9443\n",
      "Epoch [190/300], Loss: 1.9522\n",
      "Epoch [190/300], Loss: 1.9450\n",
      "Epoch [190/300], Loss: 1.9478\n",
      "Epoch [190/300], Loss: 1.9476\n",
      "Epoch [190/300], Loss: 1.9443\n",
      "Epoch [190/300], Loss: 1.9442\n",
      "Epoch [190/300], Loss: 1.9461\n",
      "Epoch [190/300], Loss: 1.9514\n",
      "Epoch [200/300], Loss: 1.9475\n",
      "Epoch [200/300], Loss: 1.9461\n",
      "Epoch [200/300], Loss: 1.9438\n",
      "Epoch [200/300], Loss: 1.9450\n",
      "Epoch [200/300], Loss: 1.9477\n",
      "Epoch [200/300], Loss: 1.9471\n",
      "Epoch [200/300], Loss: 1.9470\n",
      "Epoch [200/300], Loss: 1.9466\n",
      "Epoch [200/300], Loss: 1.9507\n",
      "Epoch [200/300], Loss: 1.9470\n",
      "Epoch [200/300], Loss: 1.9493\n",
      "Epoch [200/300], Loss: 1.9467\n",
      "Epoch [200/300], Loss: 1.9358\n",
      "Epoch [210/300], Loss: 1.9465\n",
      "Epoch [210/300], Loss: 1.9458\n",
      "Epoch [210/300], Loss: 1.9489\n",
      "Epoch [210/300], Loss: 1.9446\n",
      "Epoch [210/300], Loss: 1.9504\n",
      "Epoch [210/300], Loss: 1.9449\n",
      "Epoch [210/300], Loss: 1.9460\n",
      "Epoch [210/300], Loss: 1.9483\n",
      "Epoch [210/300], Loss: 1.9434\n",
      "Epoch [210/300], Loss: 1.9458\n",
      "Epoch [210/300], Loss: 1.9452\n",
      "Epoch [210/300], Loss: 1.9490\n",
      "Epoch [210/300], Loss: 1.9625\n",
      "Epoch [220/300], Loss: 1.9478\n",
      "Epoch [220/300], Loss: 1.9449\n",
      "Epoch [220/300], Loss: 1.9419\n",
      "Epoch [220/300], Loss: 1.9494\n",
      "Epoch [220/300], Loss: 1.9466\n",
      "Epoch [220/300], Loss: 1.9450\n",
      "Epoch [220/300], Loss: 1.9458\n",
      "Epoch [220/300], Loss: 1.9409\n",
      "Epoch [220/300], Loss: 1.9475\n",
      "Epoch [220/300], Loss: 1.9480\n",
      "Epoch [220/300], Loss: 1.9464\n",
      "Epoch [220/300], Loss: 1.9496\n",
      "Epoch [220/300], Loss: 1.9779\n",
      "Epoch [230/300], Loss: 1.9450\n",
      "Epoch [230/300], Loss: 1.9452\n",
      "Epoch [230/300], Loss: 1.9465\n",
      "Epoch [230/300], Loss: 1.9404\n",
      "Epoch [230/300], Loss: 1.9472\n",
      "Epoch [230/300], Loss: 1.9486\n",
      "Epoch [230/300], Loss: 1.9444\n",
      "Epoch [230/300], Loss: 1.9482\n",
      "Epoch [230/300], Loss: 1.9417\n",
      "Epoch [230/300], Loss: 1.9457\n",
      "Epoch [230/300], Loss: 1.9428\n",
      "Epoch [230/300], Loss: 1.9529\n",
      "Epoch [230/300], Loss: 1.9741\n",
      "Epoch [240/300], Loss: 1.9441\n",
      "Epoch [240/300], Loss: 1.9439\n",
      "Epoch [240/300], Loss: 1.9499\n",
      "Epoch [240/300], Loss: 1.9440\n",
      "Epoch [240/300], Loss: 1.9450\n",
      "Epoch [240/300], Loss: 1.9471\n",
      "Epoch [240/300], Loss: 1.9485\n",
      "Epoch [240/300], Loss: 1.9398\n",
      "Epoch [240/300], Loss: 1.9405\n",
      "Epoch [240/300], Loss: 1.9440\n",
      "Epoch [240/300], Loss: 1.9514\n",
      "Epoch [240/300], Loss: 1.9491\n",
      "Epoch [240/300], Loss: 1.9369\n",
      "Epoch [250/300], Loss: 1.9441\n",
      "Epoch [250/300], Loss: 1.9455\n",
      "Epoch [250/300], Loss: 1.9457\n",
      "Epoch [250/300], Loss: 1.9434\n",
      "Epoch [250/300], Loss: 1.9475\n",
      "Epoch [250/300], Loss: 1.9478\n",
      "Epoch [250/300], Loss: 1.9428\n",
      "Epoch [250/300], Loss: 1.9490\n",
      "Epoch [250/300], Loss: 1.9467\n",
      "Epoch [250/300], Loss: 1.9409\n",
      "Epoch [250/300], Loss: 1.9454\n",
      "Epoch [250/300], Loss: 1.9428\n",
      "Epoch [250/300], Loss: 1.9591\n",
      "Epoch [260/300], Loss: 1.9445\n",
      "Epoch [260/300], Loss: 1.9446\n",
      "Epoch [260/300], Loss: 1.9420\n",
      "Epoch [260/300], Loss: 1.9465\n",
      "Epoch [260/300], Loss: 1.9442\n",
      "Epoch [260/300], Loss: 1.9491\n",
      "Epoch [260/300], Loss: 1.9415\n",
      "Epoch [260/300], Loss: 1.9382\n",
      "Epoch [260/300], Loss: 1.9479\n",
      "Epoch [260/300], Loss: 1.9427\n",
      "Epoch [260/300], Loss: 1.9477\n",
      "Epoch [260/300], Loss: 1.9496\n",
      "Epoch [260/300], Loss: 1.9351\n",
      "Epoch [270/300], Loss: 1.9388\n",
      "Epoch [270/300], Loss: 1.9470\n",
      "Epoch [270/300], Loss: 1.9486\n",
      "Epoch [270/300], Loss: 1.9442\n",
      "Epoch [270/300], Loss: 1.9373\n",
      "Epoch [270/300], Loss: 1.9496\n",
      "Epoch [270/300], Loss: 1.9449\n",
      "Epoch [270/300], Loss: 1.9380\n",
      "Epoch [270/300], Loss: 1.9484\n",
      "Epoch [270/300], Loss: 1.9471\n",
      "Epoch [270/300], Loss: 1.9479\n",
      "Epoch [270/300], Loss: 1.9447\n",
      "Epoch [270/300], Loss: 1.9497\n",
      "Epoch [280/300], Loss: 1.9480\n",
      "Epoch [280/300], Loss: 1.9396\n",
      "Epoch [280/300], Loss: 1.9496\n",
      "Epoch [280/300], Loss: 1.9414\n",
      "Epoch [280/300], Loss: 1.9427\n",
      "Epoch [280/300], Loss: 1.9410\n",
      "Epoch [280/300], Loss: 1.9468\n",
      "Epoch [280/300], Loss: 1.9437\n",
      "Epoch [280/300], Loss: 1.9415\n",
      "Epoch [280/300], Loss: 1.9480\n",
      "Epoch [280/300], Loss: 1.9412\n",
      "Epoch [280/300], Loss: 1.9450\n",
      "Epoch [280/300], Loss: 1.9027\n",
      "Epoch [290/300], Loss: 1.9467\n",
      "Epoch [290/300], Loss: 1.9395\n",
      "Epoch [290/300], Loss: 1.9427\n",
      "Epoch [290/300], Loss: 1.9363\n",
      "Epoch [290/300], Loss: 1.9361\n",
      "Epoch [290/300], Loss: 1.9475\n",
      "Epoch [290/300], Loss: 1.9417\n",
      "Epoch [290/300], Loss: 1.9572\n",
      "Epoch [290/300], Loss: 1.9483\n",
      "Epoch [290/300], Loss: 1.9446\n",
      "Epoch [290/300], Loss: 1.9417\n",
      "Epoch [290/300], Loss: 1.9477\n",
      "Epoch [290/300], Loss: 1.8994\n",
      "Epoch [300/300], Loss: 1.9442\n",
      "Epoch [300/300], Loss: 1.9522\n",
      "Epoch [300/300], Loss: 1.9392\n",
      "Epoch [300/300], Loss: 1.9419\n",
      "Epoch [300/300], Loss: 1.9428\n",
      "Epoch [300/300], Loss: 1.9450\n",
      "Epoch [300/300], Loss: 1.9472\n",
      "Epoch [300/300], Loss: 1.9355\n",
      "Epoch [300/300], Loss: 1.9454\n",
      "Epoch [300/300], Loss: 1.9494\n",
      "Epoch [300/300], Loss: 1.9391\n",
      "Epoch [300/300], Loss: 1.9434\n",
      "Epoch [300/300], Loss: 1.9865\n",
      "Epoch [10/300], Loss: 1.9532\n",
      "Epoch [10/300], Loss: 1.9338\n",
      "Epoch [10/300], Loss: 1.9482\n",
      "Epoch [10/300], Loss: 1.9415\n",
      "Epoch [10/300], Loss: 1.9401\n",
      "Epoch [10/300], Loss: 1.9357\n",
      "Epoch [10/300], Loss: 1.9420\n",
      "Epoch [10/300], Loss: 1.9423\n",
      "Epoch [10/300], Loss: 1.9414\n",
      "Epoch [10/300], Loss: 1.9450\n",
      "Epoch [10/300], Loss: 1.9546\n",
      "Epoch [10/300], Loss: 1.9388\n",
      "Epoch [10/300], Loss: 1.8871\n",
      "Epoch [20/300], Loss: 1.9319\n",
      "Epoch [20/300], Loss: 1.9429\n",
      "Epoch [20/300], Loss: 1.9471\n",
      "Epoch [20/300], Loss: 1.9471\n",
      "Epoch [20/300], Loss: 1.9440\n",
      "Epoch [20/300], Loss: 1.9402\n",
      "Epoch [20/300], Loss: 1.9395\n",
      "Epoch [20/300], Loss: 1.9493\n",
      "Epoch [20/300], Loss: 1.9413\n",
      "Epoch [20/300], Loss: 1.9393\n",
      "Epoch [20/300], Loss: 1.9390\n",
      "Epoch [20/300], Loss: 1.9484\n",
      "Epoch [20/300], Loss: 1.9186\n",
      "Epoch [30/300], Loss: 1.9444\n",
      "Epoch [30/300], Loss: 1.9446\n",
      "Epoch [30/300], Loss: 1.9457\n",
      "Epoch [30/300], Loss: 1.9424\n",
      "Epoch [30/300], Loss: 1.9341\n",
      "Epoch [30/300], Loss: 1.9427\n",
      "Epoch [30/300], Loss: 1.9386\n",
      "Epoch [30/300], Loss: 1.9373\n",
      "Epoch [30/300], Loss: 1.9499\n",
      "Epoch [30/300], Loss: 1.9489\n",
      "Epoch [30/300], Loss: 1.9387\n",
      "Epoch [30/300], Loss: 1.9406\n",
      "Epoch [30/300], Loss: 2.0006\n",
      "Epoch [40/300], Loss: 1.9346\n",
      "Epoch [40/300], Loss: 1.9355\n",
      "Epoch [40/300], Loss: 1.9495\n",
      "Epoch [40/300], Loss: 1.9375\n",
      "Epoch [40/300], Loss: 1.9512\n",
      "Epoch [40/300], Loss: 1.9452\n",
      "Epoch [40/300], Loss: 1.9373\n",
      "Epoch [40/300], Loss: 1.9441\n",
      "Epoch [40/300], Loss: 1.9440\n",
      "Epoch [40/300], Loss: 1.9433\n",
      "Epoch [40/300], Loss: 1.9525\n",
      "Epoch [40/300], Loss: 1.9396\n",
      "Epoch [40/300], Loss: 1.9463\n",
      "Epoch [50/300], Loss: 1.9457\n",
      "Epoch [50/300], Loss: 1.9388\n",
      "Epoch [50/300], Loss: 1.9358\n",
      "Epoch [50/300], Loss: 1.9465\n",
      "Epoch [50/300], Loss: 1.9434\n",
      "Epoch [50/300], Loss: 1.9366\n",
      "Epoch [50/300], Loss: 1.9459\n",
      "Epoch [50/300], Loss: 1.9487\n",
      "Epoch [50/300], Loss: 1.9321\n",
      "Epoch [50/300], Loss: 1.9539\n",
      "Epoch [50/300], Loss: 1.9387\n",
      "Epoch [50/300], Loss: 1.9371\n",
      "Epoch [50/300], Loss: 1.8784\n",
      "Epoch [60/300], Loss: 1.9377\n",
      "Epoch [60/300], Loss: 1.9423\n",
      "Epoch [60/300], Loss: 1.9430\n",
      "Epoch [60/300], Loss: 1.9345\n",
      "Epoch [60/300], Loss: 1.9470\n",
      "Epoch [60/300], Loss: 1.9416\n",
      "Epoch [60/300], Loss: 1.9447\n",
      "Epoch [60/300], Loss: 1.9341\n",
      "Epoch [60/300], Loss: 1.9419\n",
      "Epoch [60/300], Loss: 1.9539\n",
      "Epoch [60/300], Loss: 1.9427\n",
      "Epoch [60/300], Loss: 1.9403\n",
      "Epoch [60/300], Loss: 1.9784\n",
      "Epoch [70/300], Loss: 1.9512\n",
      "Epoch [70/300], Loss: 1.9347\n",
      "Epoch [70/300], Loss: 1.9447\n",
      "Epoch [70/300], Loss: 1.9412\n",
      "Epoch [70/300], Loss: 1.9389\n",
      "Epoch [70/300], Loss: 1.9387\n",
      "Epoch [70/300], Loss: 1.9454\n",
      "Epoch [70/300], Loss: 1.9352\n",
      "Epoch [70/300], Loss: 1.9432\n",
      "Epoch [70/300], Loss: 1.9454\n",
      "Epoch [70/300], Loss: 1.9389\n",
      "Epoch [70/300], Loss: 1.9443\n",
      "Epoch [70/300], Loss: 1.9658\n",
      "Epoch [80/300], Loss: 1.9366\n",
      "Epoch [80/300], Loss: 1.9334\n",
      "Epoch [80/300], Loss: 1.9449\n",
      "Epoch [80/300], Loss: 1.9505\n",
      "Epoch [80/300], Loss: 1.9357\n",
      "Epoch [80/300], Loss: 1.9514\n",
      "Epoch [80/300], Loss: 1.9400\n",
      "Epoch [80/300], Loss: 1.9328\n",
      "Epoch [80/300], Loss: 1.9441\n",
      "Epoch [80/300], Loss: 1.9366\n",
      "Epoch [80/300], Loss: 1.9422\n",
      "Epoch [80/300], Loss: 1.9494\n",
      "Epoch [80/300], Loss: 1.9628\n",
      "Epoch [90/300], Loss: 1.9358\n",
      "Epoch [90/300], Loss: 1.9434\n",
      "Epoch [90/300], Loss: 1.9390\n",
      "Epoch [90/300], Loss: 1.9481\n",
      "Epoch [90/300], Loss: 1.9439\n",
      "Epoch [90/300], Loss: 1.9426\n",
      "Epoch [90/300], Loss: 1.9535\n",
      "Epoch [90/300], Loss: 1.9425\n",
      "Epoch [90/300], Loss: 1.9423\n",
      "Epoch [90/300], Loss: 1.9319\n",
      "Epoch [90/300], Loss: 1.9297\n",
      "Epoch [90/300], Loss: 1.9444\n",
      "Epoch [90/300], Loss: 1.9612\n",
      "Epoch [100/300], Loss: 1.9527\n",
      "Epoch [100/300], Loss: 1.9446\n",
      "Epoch [100/300], Loss: 1.9516\n",
      "Epoch [100/300], Loss: 1.9302\n",
      "Epoch [100/300], Loss: 1.9441\n",
      "Epoch [100/300], Loss: 1.9349\n",
      "Epoch [100/300], Loss: 1.9467\n",
      "Epoch [100/300], Loss: 1.9370\n",
      "Epoch [100/300], Loss: 1.9401\n",
      "Epoch [100/300], Loss: 1.9413\n",
      "Epoch [100/300], Loss: 1.9402\n",
      "Epoch [100/300], Loss: 1.9374\n",
      "Epoch [100/300], Loss: 1.9712\n",
      "Epoch [110/300], Loss: 1.9517\n",
      "Epoch [110/300], Loss: 1.9378\n",
      "Epoch [110/300], Loss: 1.9398\n",
      "Epoch [110/300], Loss: 1.9339\n",
      "Epoch [110/300], Loss: 1.9369\n",
      "Epoch [110/300], Loss: 1.9499\n",
      "Epoch [110/300], Loss: 1.9431\n",
      "Epoch [110/300], Loss: 1.9359\n",
      "Epoch [110/300], Loss: 1.9345\n",
      "Epoch [110/300], Loss: 1.9481\n",
      "Epoch [110/300], Loss: 1.9370\n",
      "Epoch [110/300], Loss: 1.9469\n",
      "Epoch [110/300], Loss: 1.9927\n",
      "Epoch [120/300], Loss: 1.9511\n",
      "Epoch [120/300], Loss: 1.9522\n",
      "Epoch [120/300], Loss: 1.9359\n",
      "Epoch [120/300], Loss: 1.9524\n",
      "Epoch [120/300], Loss: 1.9407\n",
      "Epoch [120/300], Loss: 1.9234\n",
      "Epoch [120/300], Loss: 1.9434\n",
      "Epoch [120/300], Loss: 1.9519\n",
      "Epoch [120/300], Loss: 1.9397\n",
      "Epoch [120/300], Loss: 1.9380\n",
      "Epoch [120/300], Loss: 1.9222\n",
      "Epoch [120/300], Loss: 1.9506\n",
      "Epoch [120/300], Loss: 1.9761\n",
      "Epoch [130/300], Loss: 1.9352\n",
      "Epoch [130/300], Loss: 1.9501\n",
      "Epoch [130/300], Loss: 1.9302\n",
      "Epoch [130/300], Loss: 1.9334\n",
      "Epoch [130/300], Loss: 1.9435\n",
      "Epoch [130/300], Loss: 1.9421\n",
      "Epoch [130/300], Loss: 1.9456\n",
      "Epoch [130/300], Loss: 1.9436\n",
      "Epoch [130/300], Loss: 1.9404\n",
      "Epoch [130/300], Loss: 1.9442\n",
      "Epoch [130/300], Loss: 1.9351\n",
      "Epoch [130/300], Loss: 1.9508\n",
      "Epoch [130/300], Loss: 1.9673\n",
      "Epoch [140/300], Loss: 1.9524\n",
      "Epoch [140/300], Loss: 1.9371\n",
      "Epoch [140/300], Loss: 1.9337\n",
      "Epoch [140/300], Loss: 1.9400\n",
      "Epoch [140/300], Loss: 1.9440\n",
      "Epoch [140/300], Loss: 1.9432\n",
      "Epoch [140/300], Loss: 1.9471\n",
      "Epoch [140/300], Loss: 1.9342\n",
      "Epoch [140/300], Loss: 1.9374\n",
      "Epoch [140/300], Loss: 1.9382\n",
      "Epoch [140/300], Loss: 1.9394\n",
      "Epoch [140/300], Loss: 1.9382\n",
      "Epoch [140/300], Loss: 1.9748\n",
      "Epoch [150/300], Loss: 1.9448\n",
      "Epoch [150/300], Loss: 1.9423\n",
      "Epoch [150/300], Loss: 1.9360\n",
      "Epoch [150/300], Loss: 1.9466\n",
      "Epoch [150/300], Loss: 1.9367\n",
      "Epoch [150/300], Loss: 1.9458\n",
      "Epoch [150/300], Loss: 1.9359\n",
      "Epoch [150/300], Loss: 1.9398\n",
      "Epoch [150/300], Loss: 1.9425\n",
      "Epoch [150/300], Loss: 1.9458\n",
      "Epoch [150/300], Loss: 1.9398\n",
      "Epoch [150/300], Loss: 1.9264\n",
      "Epoch [150/300], Loss: 2.0124\n",
      "Epoch [160/300], Loss: 1.9511\n",
      "Epoch [160/300], Loss: 1.9385\n",
      "Epoch [160/300], Loss: 1.9508\n",
      "Epoch [160/300], Loss: 1.9361\n",
      "Epoch [160/300], Loss: 1.9511\n",
      "Epoch [160/300], Loss: 1.9368\n",
      "Epoch [160/300], Loss: 1.9364\n",
      "Epoch [160/300], Loss: 1.9272\n",
      "Epoch [160/300], Loss: 1.9388\n",
      "Epoch [160/300], Loss: 1.9488\n",
      "Epoch [160/300], Loss: 1.9282\n",
      "Epoch [160/300], Loss: 1.9447\n",
      "Epoch [160/300], Loss: 2.0114\n",
      "Epoch [170/300], Loss: 1.9433\n",
      "Epoch [170/300], Loss: 1.9594\n",
      "Epoch [170/300], Loss: 1.9367\n",
      "Epoch [170/300], Loss: 1.9549\n",
      "Epoch [170/300], Loss: 1.9441\n",
      "Epoch [170/300], Loss: 1.9378\n",
      "Epoch [170/300], Loss: 1.9391\n",
      "Epoch [170/300], Loss: 1.9357\n",
      "Epoch [170/300], Loss: 1.9303\n",
      "Epoch [170/300], Loss: 1.9398\n",
      "Epoch [170/300], Loss: 1.9328\n",
      "Epoch [170/300], Loss: 1.9465\n",
      "Epoch [170/300], Loss: 2.0158\n",
      "Epoch [180/300], Loss: 1.9457\n",
      "Epoch [180/300], Loss: 1.9375\n",
      "Epoch [180/300], Loss: 1.9369\n",
      "Epoch [180/300], Loss: 1.9455\n",
      "Epoch [180/300], Loss: 1.9457\n",
      "Epoch [180/300], Loss: 1.9494\n",
      "Epoch [180/300], Loss: 1.9410\n",
      "Epoch [180/300], Loss: 1.9390\n",
      "Epoch [180/300], Loss: 1.9472\n",
      "Epoch [180/300], Loss: 1.9362\n",
      "Epoch [180/300], Loss: 1.9330\n",
      "Epoch [180/300], Loss: 1.9383\n",
      "Epoch [180/300], Loss: 1.9219\n",
      "Epoch [190/300], Loss: 1.9413\n",
      "Epoch [190/300], Loss: 1.9444\n",
      "Epoch [190/300], Loss: 1.9385\n",
      "Epoch [190/300], Loss: 1.9419\n",
      "Epoch [190/300], Loss: 1.9402\n",
      "Epoch [190/300], Loss: 1.9327\n",
      "Epoch [190/300], Loss: 1.9541\n",
      "Epoch [190/300], Loss: 1.9207\n",
      "Epoch [190/300], Loss: 1.9478\n",
      "Epoch [190/300], Loss: 1.9431\n",
      "Epoch [190/300], Loss: 1.9376\n",
      "Epoch [190/300], Loss: 1.9531\n",
      "Epoch [190/300], Loss: 2.0119\n",
      "Epoch [200/300], Loss: 1.9400\n",
      "Epoch [200/300], Loss: 1.9343\n",
      "Epoch [200/300], Loss: 1.9504\n",
      "Epoch [200/300], Loss: 1.9318\n",
      "Epoch [200/300], Loss: 1.9449\n",
      "Epoch [200/300], Loss: 1.9448\n",
      "Epoch [200/300], Loss: 1.9230\n",
      "Epoch [200/300], Loss: 1.9411\n",
      "Epoch [200/300], Loss: 1.9388\n",
      "Epoch [200/300], Loss: 1.9521\n",
      "Epoch [200/300], Loss: 1.9370\n",
      "Epoch [200/300], Loss: 1.9426\n",
      "Epoch [200/300], Loss: 2.0106\n",
      "Epoch [210/300], Loss: 1.9184\n",
      "Epoch [210/300], Loss: 1.9504\n",
      "Epoch [210/300], Loss: 1.9368\n",
      "Epoch [210/300], Loss: 1.9438\n",
      "Epoch [210/300], Loss: 1.9465\n",
      "Epoch [210/300], Loss: 1.9370\n",
      "Epoch [210/300], Loss: 1.9383\n",
      "Epoch [210/300], Loss: 1.9479\n",
      "Epoch [210/300], Loss: 1.9468\n",
      "Epoch [210/300], Loss: 1.9421\n",
      "Epoch [210/300], Loss: 1.9487\n",
      "Epoch [210/300], Loss: 1.9374\n",
      "Epoch [210/300], Loss: 1.9638\n",
      "Epoch [220/300], Loss: 1.9309\n",
      "Epoch [220/300], Loss: 1.9433\n",
      "Epoch [220/300], Loss: 1.9546\n",
      "Epoch [220/300], Loss: 1.9451\n",
      "Epoch [220/300], Loss: 1.9264\n",
      "Epoch [220/300], Loss: 1.9369\n",
      "Epoch [220/300], Loss: 1.9414\n",
      "Epoch [220/300], Loss: 1.9338\n",
      "Epoch [220/300], Loss: 1.9468\n",
      "Epoch [220/300], Loss: 1.9551\n",
      "Epoch [220/300], Loss: 1.9447\n",
      "Epoch [220/300], Loss: 1.9347\n",
      "Epoch [220/300], Loss: 1.9626\n",
      "Epoch [230/300], Loss: 1.9404\n",
      "Epoch [230/300], Loss: 1.9408\n",
      "Epoch [230/300], Loss: 1.9466\n",
      "Epoch [230/300], Loss: 1.9437\n",
      "Epoch [230/300], Loss: 1.9318\n",
      "Epoch [230/300], Loss: 1.9476\n",
      "Epoch [230/300], Loss: 1.9489\n",
      "Epoch [230/300], Loss: 1.9398\n",
      "Epoch [230/300], Loss: 1.9318\n",
      "Epoch [230/300], Loss: 1.9383\n",
      "Epoch [230/300], Loss: 1.9355\n",
      "Epoch [230/300], Loss: 1.9398\n",
      "Epoch [230/300], Loss: 1.9772\n",
      "Epoch [240/300], Loss: 1.9564\n",
      "Epoch [240/300], Loss: 1.9337\n",
      "Epoch [240/300], Loss: 1.9394\n",
      "Epoch [240/300], Loss: 1.9313\n",
      "Epoch [240/300], Loss: 1.9378\n",
      "Epoch [240/300], Loss: 1.9394\n",
      "Epoch [240/300], Loss: 1.9463\n",
      "Epoch [240/300], Loss: 1.9408\n",
      "Epoch [240/300], Loss: 1.9504\n",
      "Epoch [240/300], Loss: 1.9287\n",
      "Epoch [240/300], Loss: 1.9412\n",
      "Epoch [240/300], Loss: 1.9439\n",
      "Epoch [240/300], Loss: 1.9179\n",
      "Epoch [250/300], Loss: 1.9324\n",
      "Epoch [250/300], Loss: 1.9379\n",
      "Epoch [250/300], Loss: 1.9388\n",
      "Epoch [250/300], Loss: 1.9351\n",
      "Epoch [250/300], Loss: 1.9426\n",
      "Epoch [250/300], Loss: 1.9407\n",
      "Epoch [250/300], Loss: 1.9422\n",
      "Epoch [250/300], Loss: 1.9461\n",
      "Epoch [250/300], Loss: 1.9402\n",
      "Epoch [250/300], Loss: 1.9426\n",
      "Epoch [250/300], Loss: 1.9331\n",
      "Epoch [250/300], Loss: 1.9512\n",
      "Epoch [250/300], Loss: 1.9509\n",
      "Epoch [260/300], Loss: 1.9344\n",
      "Epoch [260/300], Loss: 1.9431\n",
      "Epoch [260/300], Loss: 1.9390\n",
      "Epoch [260/300], Loss: 1.9301\n",
      "Epoch [260/300], Loss: 1.9500\n",
      "Epoch [260/300], Loss: 1.9400\n",
      "Epoch [260/300], Loss: 1.9417\n",
      "Epoch [260/300], Loss: 1.9554\n",
      "Epoch [260/300], Loss: 1.9383\n",
      "Epoch [260/300], Loss: 1.9425\n",
      "Epoch [260/300], Loss: 1.9378\n",
      "Epoch [260/300], Loss: 1.9400\n",
      "Epoch [260/300], Loss: 1.9713\n",
      "Epoch [270/300], Loss: 1.9285\n",
      "Epoch [270/300], Loss: 1.9431\n",
      "Epoch [270/300], Loss: 1.9332\n",
      "Epoch [270/300], Loss: 1.9341\n",
      "Epoch [270/300], Loss: 1.9378\n",
      "Epoch [270/300], Loss: 1.9508\n",
      "Epoch [270/300], Loss: 1.9546\n",
      "Epoch [270/300], Loss: 1.9521\n",
      "Epoch [270/300], Loss: 1.9286\n",
      "Epoch [270/300], Loss: 1.9340\n",
      "Epoch [270/300], Loss: 1.9439\n",
      "Epoch [270/300], Loss: 1.9450\n",
      "Epoch [270/300], Loss: 1.9036\n",
      "Epoch [280/300], Loss: 1.9337\n",
      "Epoch [280/300], Loss: 1.9452\n",
      "Epoch [280/300], Loss: 1.9386\n",
      "Epoch [280/300], Loss: 1.9399\n",
      "Epoch [280/300], Loss: 1.9399\n",
      "Epoch [280/300], Loss: 1.9386\n",
      "Epoch [280/300], Loss: 1.9453\n",
      "Epoch [280/300], Loss: 1.9465\n",
      "Epoch [280/300], Loss: 1.9504\n",
      "Epoch [280/300], Loss: 1.9509\n",
      "Epoch [280/300], Loss: 1.9334\n",
      "Epoch [280/300], Loss: 1.9275\n",
      "Epoch [280/300], Loss: 1.9901\n",
      "Epoch [290/300], Loss: 1.9362\n",
      "Epoch [290/300], Loss: 1.9370\n",
      "Epoch [290/300], Loss: 1.9565\n",
      "Epoch [290/300], Loss: 1.9323\n",
      "Epoch [290/300], Loss: 1.9368\n",
      "Epoch [290/300], Loss: 1.9482\n",
      "Epoch [290/300], Loss: 1.9410\n",
      "Epoch [290/300], Loss: 1.9358\n",
      "Epoch [290/300], Loss: 1.9457\n",
      "Epoch [290/300], Loss: 1.9409\n",
      "Epoch [290/300], Loss: 1.9374\n",
      "Epoch [290/300], Loss: 1.9369\n",
      "Epoch [290/300], Loss: 1.9756\n",
      "Epoch [300/300], Loss: 1.9384\n",
      "Epoch [300/300], Loss: 1.9417\n",
      "Epoch [300/300], Loss: 1.9371\n",
      "Epoch [300/300], Loss: 1.9435\n",
      "Epoch [300/300], Loss: 1.9470\n",
      "Epoch [300/300], Loss: 1.9406\n",
      "Epoch [300/300], Loss: 1.9316\n",
      "Epoch [300/300], Loss: 1.9503\n",
      "Epoch [300/300], Loss: 1.9276\n",
      "Epoch [300/300], Loss: 1.9455\n",
      "Epoch [300/300], Loss: 1.9399\n",
      "Epoch [300/300], Loss: 1.9482\n",
      "Epoch [300/300], Loss: 2.0108\n",
      "Epoch [10/300], Loss: 1.9463\n",
      "Epoch [10/300], Loss: 1.9261\n",
      "Epoch [10/300], Loss: 1.9467\n",
      "Epoch [10/300], Loss: 1.9373\n",
      "Epoch [10/300], Loss: 1.9377\n",
      "Epoch [10/300], Loss: 1.9222\n",
      "Epoch [10/300], Loss: 1.9504\n",
      "Epoch [10/300], Loss: 1.9399\n",
      "Epoch [10/300], Loss: 1.9432\n",
      "Epoch [10/300], Loss: 1.9534\n",
      "Epoch [10/300], Loss: 1.9339\n",
      "Epoch [10/300], Loss: 1.9466\n",
      "Epoch [10/300], Loss: 2.0100\n",
      "Epoch [20/300], Loss: 1.9485\n",
      "Epoch [20/300], Loss: 1.9352\n",
      "Epoch [20/300], Loss: 1.9484\n",
      "Epoch [20/300], Loss: 1.9430\n",
      "Epoch [20/300], Loss: 1.9347\n",
      "Epoch [20/300], Loss: 1.9196\n",
      "Epoch [20/300], Loss: 1.9438\n",
      "Epoch [20/300], Loss: 1.9403\n",
      "Epoch [20/300], Loss: 1.9469\n",
      "Epoch [20/300], Loss: 1.9448\n",
      "Epoch [20/300], Loss: 1.9410\n",
      "Epoch [20/300], Loss: 1.9430\n",
      "Epoch [20/300], Loss: 1.9631\n",
      "Epoch [30/300], Loss: 1.9450\n",
      "Epoch [30/300], Loss: 1.9449\n",
      "Epoch [30/300], Loss: 1.9418\n",
      "Epoch [30/300], Loss: 1.9415\n",
      "Epoch [30/300], Loss: 1.9333\n",
      "Epoch [30/300], Loss: 1.9314\n",
      "Epoch [30/300], Loss: 1.9469\n",
      "Epoch [30/300], Loss: 1.9429\n",
      "Epoch [30/300], Loss: 1.9517\n",
      "Epoch [30/300], Loss: 1.9387\n",
      "Epoch [30/300], Loss: 1.9280\n",
      "Epoch [30/300], Loss: 1.9364\n",
      "Epoch [30/300], Loss: 1.9168\n",
      "Epoch [40/300], Loss: 1.9405\n",
      "Epoch [40/300], Loss: 1.9503\n",
      "Epoch [40/300], Loss: 1.9343\n",
      "Epoch [40/300], Loss: 1.9467\n",
      "Epoch [40/300], Loss: 1.9417\n",
      "Epoch [40/300], Loss: 1.9458\n",
      "Epoch [40/300], Loss: 1.9483\n",
      "Epoch [40/300], Loss: 1.9397\n",
      "Epoch [40/300], Loss: 1.9428\n",
      "Epoch [40/300], Loss: 1.9218\n",
      "Epoch [40/300], Loss: 1.9435\n",
      "Epoch [40/300], Loss: 1.9373\n",
      "Epoch [40/300], Loss: 1.8299\n",
      "Epoch [50/300], Loss: 1.9465\n",
      "Epoch [50/300], Loss: 1.9464\n",
      "Epoch [50/300], Loss: 1.9376\n",
      "Epoch [50/300], Loss: 1.9346\n",
      "Epoch [50/300], Loss: 1.9294\n",
      "Epoch [50/300], Loss: 1.9424\n",
      "Epoch [50/300], Loss: 1.9383\n",
      "Epoch [50/300], Loss: 1.9396\n",
      "Epoch [50/300], Loss: 1.9398\n",
      "Epoch [50/300], Loss: 1.9337\n",
      "Epoch [50/300], Loss: 1.9469\n",
      "Epoch [50/300], Loss: 1.9467\n",
      "Epoch [50/300], Loss: 1.9600\n",
      "Epoch [60/300], Loss: 1.9628\n",
      "Epoch [60/300], Loss: 1.9400\n",
      "Epoch [60/300], Loss: 1.9326\n",
      "Epoch [60/300], Loss: 1.9304\n",
      "Epoch [60/300], Loss: 1.9453\n",
      "Epoch [60/300], Loss: 1.9314\n",
      "Epoch [60/300], Loss: 1.9386\n",
      "Epoch [60/300], Loss: 1.9472\n",
      "Epoch [60/300], Loss: 1.9324\n",
      "Epoch [60/300], Loss: 1.9386\n",
      "Epoch [60/300], Loss: 1.9468\n",
      "Epoch [60/300], Loss: 1.9352\n",
      "Epoch [60/300], Loss: 1.9652\n",
      "Epoch [70/300], Loss: 1.9363\n",
      "Epoch [70/300], Loss: 1.9490\n",
      "Epoch [70/300], Loss: 1.9393\n",
      "Epoch [70/300], Loss: 1.9563\n",
      "Epoch [70/300], Loss: 1.9370\n",
      "Epoch [70/300], Loss: 1.9420\n",
      "Epoch [70/300], Loss: 1.9419\n",
      "Epoch [70/300], Loss: 1.9314\n",
      "Epoch [70/300], Loss: 1.9438\n",
      "Epoch [70/300], Loss: 1.9295\n",
      "Epoch [70/300], Loss: 1.9409\n",
      "Epoch [70/300], Loss: 1.9396\n",
      "Epoch [70/300], Loss: 1.8814\n",
      "Epoch [80/300], Loss: 1.9432\n",
      "Epoch [80/300], Loss: 1.9428\n",
      "Epoch [80/300], Loss: 1.9434\n",
      "Epoch [80/300], Loss: 1.9330\n",
      "Epoch [80/300], Loss: 1.9426\n",
      "Epoch [80/300], Loss: 1.9466\n",
      "Epoch [80/300], Loss: 1.9394\n",
      "Epoch [80/300], Loss: 1.9456\n",
      "Epoch [80/300], Loss: 1.9385\n",
      "Epoch [80/300], Loss: 1.9444\n",
      "Epoch [80/300], Loss: 1.9312\n",
      "Epoch [80/300], Loss: 1.9367\n",
      "Epoch [80/300], Loss: 1.8884\n",
      "Epoch [90/300], Loss: 1.9421\n",
      "Epoch [90/300], Loss: 1.9317\n",
      "Epoch [90/300], Loss: 1.9411\n",
      "Epoch [90/300], Loss: 1.9460\n",
      "Epoch [90/300], Loss: 1.9305\n",
      "Epoch [90/300], Loss: 1.9374\n",
      "Epoch [90/300], Loss: 1.9355\n",
      "Epoch [90/300], Loss: 1.9379\n",
      "Epoch [90/300], Loss: 1.9392\n",
      "Epoch [90/300], Loss: 1.9497\n",
      "Epoch [90/300], Loss: 1.9439\n",
      "Epoch [90/300], Loss: 1.9562\n",
      "Epoch [90/300], Loss: 1.8921\n",
      "Epoch [100/300], Loss: 1.9443\n",
      "Epoch [100/300], Loss: 1.9224\n",
      "Epoch [100/300], Loss: 1.9503\n",
      "Epoch [100/300], Loss: 1.9407\n",
      "Epoch [100/300], Loss: 1.9476\n",
      "Epoch [100/300], Loss: 1.9405\n",
      "Epoch [100/300], Loss: 1.9476\n",
      "Epoch [100/300], Loss: 1.9418\n",
      "Epoch [100/300], Loss: 1.9277\n",
      "Epoch [100/300], Loss: 1.9467\n",
      "Epoch [100/300], Loss: 1.9372\n",
      "Epoch [100/300], Loss: 1.9377\n",
      "Epoch [100/300], Loss: 1.9748\n",
      "Epoch [110/300], Loss: 1.9484\n",
      "Epoch [110/300], Loss: 1.9399\n",
      "Epoch [110/300], Loss: 1.9450\n",
      "Epoch [110/300], Loss: 1.9454\n",
      "Epoch [110/300], Loss: 1.9396\n",
      "Epoch [110/300], Loss: 1.9406\n",
      "Epoch [110/300], Loss: 1.9326\n",
      "Epoch [110/300], Loss: 1.9348\n",
      "Epoch [110/300], Loss: 1.9442\n",
      "Epoch [110/300], Loss: 1.9372\n",
      "Epoch [110/300], Loss: 1.9363\n",
      "Epoch [110/300], Loss: 1.9446\n",
      "Epoch [110/300], Loss: 1.7973\n",
      "Epoch [120/300], Loss: 1.9462\n",
      "Epoch [120/300], Loss: 1.9514\n",
      "Epoch [120/300], Loss: 1.9358\n",
      "Epoch [120/300], Loss: 1.9262\n",
      "Epoch [120/300], Loss: 1.9462\n",
      "Epoch [120/300], Loss: 1.9344\n",
      "Epoch [120/300], Loss: 1.9417\n",
      "Epoch [120/300], Loss: 1.9301\n",
      "Epoch [120/300], Loss: 1.9380\n",
      "Epoch [120/300], Loss: 1.9573\n",
      "Epoch [120/300], Loss: 1.9279\n",
      "Epoch [120/300], Loss: 1.9401\n",
      "Epoch [120/300], Loss: 1.9718\n",
      "Epoch [130/300], Loss: 1.9441\n",
      "Epoch [130/300], Loss: 1.9289\n",
      "Epoch [130/300], Loss: 1.9456\n",
      "Epoch [130/300], Loss: 1.9379\n",
      "Epoch [130/300], Loss: 1.9322\n",
      "Epoch [130/300], Loss: 1.9473\n",
      "Epoch [130/300], Loss: 1.9432\n",
      "Epoch [130/300], Loss: 1.9577\n",
      "Epoch [130/300], Loss: 1.9375\n",
      "Epoch [130/300], Loss: 1.9405\n",
      "Epoch [130/300], Loss: 1.9425\n",
      "Epoch [130/300], Loss: 1.9354\n",
      "Epoch [130/300], Loss: 1.9201\n",
      "Epoch [140/300], Loss: 1.9410\n",
      "Epoch [140/300], Loss: 1.9323\n",
      "Epoch [140/300], Loss: 1.9535\n",
      "Epoch [140/300], Loss: 1.9456\n",
      "Epoch [140/300], Loss: 1.9297\n",
      "Epoch [140/300], Loss: 1.9487\n",
      "Epoch [140/300], Loss: 1.9368\n",
      "Epoch [140/300], Loss: 1.9360\n",
      "Epoch [140/300], Loss: 1.9359\n",
      "Epoch [140/300], Loss: 1.9376\n",
      "Epoch [140/300], Loss: 1.9421\n",
      "Epoch [140/300], Loss: 1.9443\n",
      "Epoch [140/300], Loss: 1.9643\n",
      "Epoch [150/300], Loss: 1.9472\n",
      "Epoch [150/300], Loss: 1.9360\n",
      "Epoch [150/300], Loss: 1.9462\n",
      "Epoch [150/300], Loss: 1.9485\n",
      "Epoch [150/300], Loss: 1.9452\n",
      "Epoch [150/300], Loss: 1.9404\n",
      "Epoch [150/300], Loss: 1.9282\n",
      "Epoch [150/300], Loss: 1.9365\n",
      "Epoch [150/300], Loss: 1.9316\n",
      "Epoch [150/300], Loss: 1.9480\n",
      "Epoch [150/300], Loss: 1.9329\n",
      "Epoch [150/300], Loss: 1.9431\n",
      "Epoch [150/300], Loss: 1.8928\n",
      "Epoch [160/300], Loss: 1.9368\n",
      "Epoch [160/300], Loss: 1.9387\n",
      "Epoch [160/300], Loss: 1.9322\n",
      "Epoch [160/300], Loss: 1.9367\n",
      "Epoch [160/300], Loss: 1.9312\n",
      "Epoch [160/300], Loss: 1.9416\n",
      "Epoch [160/300], Loss: 1.9484\n",
      "Epoch [160/300], Loss: 1.9437\n",
      "Epoch [160/300], Loss: 1.9360\n",
      "Epoch [160/300], Loss: 1.9590\n",
      "Epoch [160/300], Loss: 1.9425\n",
      "Epoch [160/300], Loss: 1.9345\n",
      "Epoch [160/300], Loss: 1.8543\n",
      "Epoch [170/300], Loss: 1.9340\n",
      "Epoch [170/300], Loss: 1.9402\n",
      "Epoch [170/300], Loss: 1.9313\n",
      "Epoch [170/300], Loss: 1.9472\n",
      "Epoch [170/300], Loss: 1.9500\n",
      "Epoch [170/300], Loss: 1.9292\n",
      "Epoch [170/300], Loss: 1.9431\n",
      "Epoch [170/300], Loss: 1.9286\n",
      "Epoch [170/300], Loss: 1.9439\n",
      "Epoch [170/300], Loss: 1.9384\n",
      "Epoch [170/300], Loss: 1.9494\n",
      "Epoch [170/300], Loss: 1.9473\n",
      "Epoch [170/300], Loss: 1.9970\n",
      "Epoch [180/300], Loss: 1.9455\n",
      "Epoch [180/300], Loss: 1.9414\n",
      "Epoch [180/300], Loss: 1.9441\n",
      "Epoch [180/300], Loss: 1.9451\n",
      "Epoch [180/300], Loss: 1.9453\n",
      "Epoch [180/300], Loss: 1.9308\n",
      "Epoch [180/300], Loss: 1.9361\n",
      "Epoch [180/300], Loss: 1.9403\n",
      "Epoch [180/300], Loss: 1.9460\n",
      "Epoch [180/300], Loss: 1.9335\n",
      "Epoch [180/300], Loss: 1.9381\n",
      "Epoch [180/300], Loss: 1.9401\n",
      "Epoch [180/300], Loss: 1.9374\n",
      "Epoch [190/300], Loss: 1.9548\n",
      "Epoch [190/300], Loss: 1.9411\n",
      "Epoch [190/300], Loss: 1.9437\n",
      "Epoch [190/300], Loss: 1.9327\n",
      "Epoch [190/300], Loss: 1.9294\n",
      "Epoch [190/300], Loss: 1.9425\n",
      "Epoch [190/300], Loss: 1.9407\n",
      "Epoch [190/300], Loss: 1.9364\n",
      "Epoch [190/300], Loss: 1.9341\n",
      "Epoch [190/300], Loss: 1.9307\n",
      "Epoch [190/300], Loss: 1.9495\n",
      "Epoch [190/300], Loss: 1.9511\n",
      "Epoch [190/300], Loss: 1.9589\n",
      "Epoch [200/300], Loss: 1.9390\n",
      "Epoch [200/300], Loss: 1.9368\n",
      "Epoch [200/300], Loss: 1.9307\n",
      "Epoch [200/300], Loss: 1.9428\n",
      "Epoch [200/300], Loss: 1.9530\n",
      "Epoch [200/300], Loss: 1.9360\n",
      "Epoch [200/300], Loss: 1.9487\n",
      "Epoch [200/300], Loss: 1.9418\n",
      "Epoch [200/300], Loss: 1.9377\n",
      "Epoch [200/300], Loss: 1.9359\n",
      "Epoch [200/300], Loss: 1.9372\n",
      "Epoch [200/300], Loss: 1.9383\n",
      "Epoch [200/300], Loss: 1.9740\n",
      "Epoch [210/300], Loss: 1.9483\n",
      "Epoch [210/300], Loss: 1.9460\n",
      "Epoch [210/300], Loss: 1.9355\n",
      "Epoch [210/300], Loss: 1.9403\n",
      "Epoch [210/300], Loss: 1.9428\n",
      "Epoch [210/300], Loss: 1.9305\n",
      "Epoch [210/300], Loss: 1.9575\n",
      "Epoch [210/300], Loss: 1.9432\n",
      "Epoch [210/300], Loss: 1.9319\n",
      "Epoch [210/300], Loss: 1.9347\n",
      "Epoch [210/300], Loss: 1.9428\n",
      "Epoch [210/300], Loss: 1.9304\n",
      "Epoch [210/300], Loss: 1.8694\n",
      "Epoch [220/300], Loss: 1.9431\n",
      "Epoch [220/300], Loss: 1.9271\n",
      "Epoch [220/300], Loss: 1.9406\n",
      "Epoch [220/300], Loss: 1.9372\n",
      "Epoch [220/300], Loss: 1.9399\n",
      "Epoch [220/300], Loss: 1.9393\n",
      "Epoch [220/300], Loss: 1.9403\n",
      "Epoch [220/300], Loss: 1.9534\n",
      "Epoch [220/300], Loss: 1.9407\n",
      "Epoch [220/300], Loss: 1.9485\n",
      "Epoch [220/300], Loss: 1.9436\n",
      "Epoch [220/300], Loss: 1.9296\n",
      "Epoch [220/300], Loss: 1.9919\n",
      "Epoch [230/300], Loss: 1.9505\n",
      "Epoch [230/300], Loss: 1.9512\n",
      "Epoch [230/300], Loss: 1.9444\n",
      "Epoch [230/300], Loss: 1.9492\n",
      "Epoch [230/300], Loss: 1.9183\n",
      "Epoch [230/300], Loss: 1.9397\n",
      "Epoch [230/300], Loss: 1.9392\n",
      "Epoch [230/300], Loss: 1.9414\n",
      "Epoch [230/300], Loss: 1.9347\n",
      "Epoch [230/300], Loss: 1.9411\n",
      "Epoch [230/300], Loss: 1.9258\n",
      "Epoch [230/300], Loss: 1.9471\n",
      "Epoch [230/300], Loss: 1.9672\n",
      "Epoch [240/300], Loss: 1.9504\n",
      "Epoch [240/300], Loss: 1.9529\n",
      "Epoch [240/300], Loss: 1.9294\n",
      "Epoch [240/300], Loss: 1.9405\n",
      "Epoch [240/300], Loss: 1.9340\n",
      "Epoch [240/300], Loss: 1.9285\n",
      "Epoch [240/300], Loss: 1.9227\n",
      "Epoch [240/300], Loss: 1.9504\n",
      "Epoch [240/300], Loss: 1.9396\n",
      "Epoch [240/300], Loss: 1.9432\n",
      "Epoch [240/300], Loss: 1.9413\n",
      "Epoch [240/300], Loss: 1.9453\n",
      "Epoch [240/300], Loss: 1.9744\n",
      "Epoch [250/300], Loss: 1.9368\n",
      "Epoch [250/300], Loss: 1.9459\n",
      "Epoch [250/300], Loss: 1.9497\n",
      "Epoch [250/300], Loss: 1.9302\n",
      "Epoch [250/300], Loss: 1.9382\n",
      "Epoch [250/300], Loss: 1.9439\n",
      "Epoch [250/300], Loss: 1.9364\n",
      "Epoch [250/300], Loss: 1.9446\n",
      "Epoch [250/300], Loss: 1.9379\n",
      "Epoch [250/300], Loss: 1.9433\n",
      "Epoch [250/300], Loss: 1.9468\n",
      "Epoch [250/300], Loss: 1.9219\n",
      "Epoch [250/300], Loss: 1.9872\n",
      "Epoch [260/300], Loss: 1.9365\n",
      "Epoch [260/300], Loss: 1.9325\n",
      "Epoch [260/300], Loss: 1.9353\n",
      "Epoch [260/300], Loss: 1.9413\n",
      "Epoch [260/300], Loss: 1.9456\n",
      "Epoch [260/300], Loss: 1.9364\n",
      "Epoch [260/300], Loss: 1.9322\n",
      "Epoch [260/300], Loss: 1.9586\n",
      "Epoch [260/300], Loss: 1.9488\n",
      "Epoch [260/300], Loss: 1.9377\n",
      "Epoch [260/300], Loss: 1.9396\n",
      "Epoch [260/300], Loss: 1.9394\n",
      "Epoch [260/300], Loss: 1.8933\n",
      "Epoch [270/300], Loss: 1.9367\n",
      "Epoch [270/300], Loss: 1.9367\n",
      "Epoch [270/300], Loss: 1.9487\n",
      "Epoch [270/300], Loss: 1.9418\n",
      "Epoch [270/300], Loss: 1.9432\n",
      "Epoch [270/300], Loss: 1.9416\n",
      "Epoch [270/300], Loss: 1.9313\n",
      "Epoch [270/300], Loss: 1.9345\n",
      "Epoch [270/300], Loss: 1.9477\n",
      "Epoch [270/300], Loss: 1.9398\n",
      "Epoch [270/300], Loss: 1.9542\n",
      "Epoch [270/300], Loss: 1.9277\n",
      "Epoch [270/300], Loss: 1.8547\n",
      "Epoch [280/300], Loss: 1.9480\n",
      "Epoch [280/300], Loss: 1.9368\n",
      "Epoch [280/300], Loss: 1.9356\n",
      "Epoch [280/300], Loss: 1.9502\n",
      "Epoch [280/300], Loss: 1.9388\n",
      "Epoch [280/300], Loss: 1.9400\n",
      "Epoch [280/300], Loss: 1.9411\n",
      "Epoch [280/300], Loss: 1.9378\n",
      "Epoch [280/300], Loss: 1.9507\n",
      "Epoch [280/300], Loss: 1.9285\n",
      "Epoch [280/300], Loss: 1.9362\n",
      "Epoch [280/300], Loss: 1.9424\n",
      "Epoch [280/300], Loss: 1.9017\n",
      "Epoch [290/300], Loss: 1.9390\n",
      "Epoch [290/300], Loss: 1.9371\n",
      "Epoch [290/300], Loss: 1.9427\n",
      "Epoch [290/300], Loss: 1.9401\n",
      "Epoch [290/300], Loss: 1.9368\n",
      "Epoch [290/300], Loss: 1.9363\n",
      "Epoch [290/300], Loss: 1.9326\n",
      "Epoch [290/300], Loss: 1.9479\n",
      "Epoch [290/300], Loss: 1.9328\n",
      "Epoch [290/300], Loss: 1.9352\n",
      "Epoch [290/300], Loss: 1.9454\n",
      "Epoch [290/300], Loss: 1.9498\n",
      "Epoch [290/300], Loss: 1.9815\n",
      "Epoch [300/300], Loss: 1.9351\n",
      "Epoch [300/300], Loss: 1.9477\n",
      "Epoch [300/300], Loss: 1.9241\n",
      "Epoch [300/300], Loss: 1.9451\n",
      "Epoch [300/300], Loss: 1.9319\n",
      "Epoch [300/300], Loss: 1.9376\n",
      "Epoch [300/300], Loss: 1.9568\n",
      "Epoch [300/300], Loss: 1.9412\n",
      "Epoch [300/300], Loss: 1.9489\n",
      "Epoch [300/300], Loss: 1.9256\n",
      "Epoch [300/300], Loss: 1.9298\n",
      "Epoch [300/300], Loss: 1.9589\n",
      "Epoch [300/300], Loss: 1.9413\n",
      "Epoch [10/300], Loss: 1.9443\n",
      "Epoch [10/300], Loss: 1.9491\n",
      "Epoch [10/300], Loss: 1.9342\n",
      "Epoch [10/300], Loss: 1.9221\n",
      "Epoch [10/300], Loss: 1.9440\n",
      "Epoch [10/300], Loss: 1.9329\n",
      "Epoch [10/300], Loss: 1.9308\n",
      "Epoch [10/300], Loss: 1.9530\n",
      "Epoch [10/300], Loss: 1.9325\n",
      "Epoch [10/300], Loss: 1.9407\n",
      "Epoch [10/300], Loss: 1.9502\n",
      "Epoch [10/300], Loss: 1.9476\n",
      "Epoch [10/300], Loss: 2.0037\n",
      "Epoch [20/300], Loss: 1.9412\n",
      "Epoch [20/300], Loss: 1.9404\n",
      "Epoch [20/300], Loss: 1.9361\n",
      "Epoch [20/300], Loss: 1.9321\n",
      "Epoch [20/300], Loss: 1.9438\n",
      "Epoch [20/300], Loss: 1.9407\n",
      "Epoch [20/300], Loss: 1.9369\n",
      "Epoch [20/300], Loss: 1.9411\n",
      "Epoch [20/300], Loss: 1.9479\n",
      "Epoch [20/300], Loss: 1.9334\n",
      "Epoch [20/300], Loss: 1.9391\n",
      "Epoch [20/300], Loss: 1.9484\n",
      "Epoch [20/300], Loss: 1.9633\n",
      "Epoch [30/300], Loss: 1.9454\n",
      "Epoch [30/300], Loss: 1.9367\n",
      "Epoch [30/300], Loss: 1.9445\n",
      "Epoch [30/300], Loss: 1.9306\n",
      "Epoch [30/300], Loss: 1.9436\n",
      "Epoch [30/300], Loss: 1.9415\n",
      "Epoch [30/300], Loss: 1.9426\n",
      "Epoch [30/300], Loss: 1.9378\n",
      "Epoch [30/300], Loss: 1.9372\n",
      "Epoch [30/300], Loss: 1.9359\n",
      "Epoch [30/300], Loss: 1.9335\n",
      "Epoch [30/300], Loss: 1.9436\n",
      "Epoch [30/300], Loss: 2.0065\n",
      "Epoch [40/300], Loss: 1.9338\n",
      "Epoch [40/300], Loss: 1.9359\n",
      "Epoch [40/300], Loss: 1.9469\n",
      "Epoch [40/300], Loss: 1.9404\n",
      "Epoch [40/300], Loss: 1.9414\n",
      "Epoch [40/300], Loss: 1.9269\n",
      "Epoch [40/300], Loss: 1.9316\n",
      "Epoch [40/300], Loss: 1.9321\n",
      "Epoch [40/300], Loss: 1.9528\n",
      "Epoch [40/300], Loss: 1.9468\n",
      "Epoch [40/300], Loss: 1.9476\n",
      "Epoch [40/300], Loss: 1.9378\n",
      "Epoch [40/300], Loss: 1.9056\n",
      "Epoch [50/300], Loss: 1.9280\n",
      "Epoch [50/300], Loss: 1.9452\n",
      "Epoch [50/300], Loss: 1.9518\n",
      "Epoch [50/300], Loss: 1.9366\n",
      "Epoch [50/300], Loss: 1.9357\n",
      "Epoch [50/300], Loss: 1.9420\n",
      "Epoch [50/300], Loss: 1.9450\n",
      "Epoch [50/300], Loss: 1.9306\n",
      "Epoch [50/300], Loss: 1.9314\n",
      "Epoch [50/300], Loss: 1.9501\n",
      "Epoch [50/300], Loss: 1.9441\n",
      "Epoch [50/300], Loss: 1.9340\n",
      "Epoch [50/300], Loss: 1.9978\n",
      "Epoch [60/300], Loss: 1.9330\n",
      "Epoch [60/300], Loss: 1.9333\n",
      "Epoch [60/300], Loss: 1.9348\n",
      "Epoch [60/300], Loss: 1.9487\n",
      "Epoch [60/300], Loss: 1.9426\n",
      "Epoch [60/300], Loss: 1.9496\n",
      "Epoch [60/300], Loss: 1.9519\n",
      "Epoch [60/300], Loss: 1.9375\n",
      "Epoch [60/300], Loss: 1.9235\n",
      "Epoch [60/300], Loss: 1.9380\n",
      "Epoch [60/300], Loss: 1.9466\n",
      "Epoch [60/300], Loss: 1.9367\n",
      "Epoch [60/300], Loss: 1.9767\n",
      "Epoch [70/300], Loss: 1.9345\n",
      "Epoch [70/300], Loss: 1.9390\n",
      "Epoch [70/300], Loss: 1.9378\n",
      "Epoch [70/300], Loss: 1.9454\n",
      "Epoch [70/300], Loss: 1.9443\n",
      "Epoch [70/300], Loss: 1.9328\n",
      "Epoch [70/300], Loss: 1.9328\n",
      "Epoch [70/300], Loss: 1.9444\n",
      "Epoch [70/300], Loss: 1.9509\n",
      "Epoch [70/300], Loss: 1.9373\n",
      "Epoch [70/300], Loss: 1.9422\n",
      "Epoch [70/300], Loss: 1.9324\n",
      "Epoch [70/300], Loss: 1.9659\n",
      "Epoch [80/300], Loss: 1.9334\n",
      "Epoch [80/300], Loss: 1.9512\n",
      "Epoch [80/300], Loss: 1.9524\n",
      "Epoch [80/300], Loss: 1.9413\n",
      "Epoch [80/300], Loss: 1.9340\n",
      "Epoch [80/300], Loss: 1.9453\n",
      "Epoch [80/300], Loss: 1.9382\n",
      "Epoch [80/300], Loss: 1.9397\n",
      "Epoch [80/300], Loss: 1.9343\n",
      "Epoch [80/300], Loss: 1.9469\n",
      "Epoch [80/300], Loss: 1.9389\n",
      "Epoch [80/300], Loss: 1.9170\n",
      "Epoch [80/300], Loss: 2.0345\n",
      "Epoch [90/300], Loss: 1.9324\n",
      "Epoch [90/300], Loss: 1.9380\n",
      "Epoch [90/300], Loss: 1.9514\n",
      "Epoch [90/300], Loss: 1.9547\n",
      "Epoch [90/300], Loss: 1.9377\n",
      "Epoch [90/300], Loss: 1.9370\n",
      "Epoch [90/300], Loss: 1.9294\n",
      "Epoch [90/300], Loss: 1.9526\n",
      "Epoch [90/300], Loss: 1.9289\n",
      "Epoch [90/300], Loss: 1.9291\n",
      "Epoch [90/300], Loss: 1.9388\n",
      "Epoch [90/300], Loss: 1.9431\n",
      "Epoch [90/300], Loss: 2.0320\n",
      "Epoch [100/300], Loss: 1.9412\n",
      "Epoch [100/300], Loss: 1.9442\n",
      "Epoch [100/300], Loss: 1.9357\n",
      "Epoch [100/300], Loss: 1.9413\n",
      "Epoch [100/300], Loss: 1.9360\n",
      "Epoch [100/300], Loss: 1.9273\n",
      "Epoch [100/300], Loss: 1.9425\n",
      "Epoch [100/300], Loss: 1.9356\n",
      "Epoch [100/300], Loss: 1.9306\n",
      "Epoch [100/300], Loss: 1.9568\n",
      "Epoch [100/300], Loss: 1.9390\n",
      "Epoch [100/300], Loss: 1.9415\n",
      "Epoch [100/300], Loss: 2.0222\n",
      "Epoch [110/300], Loss: 1.9475\n",
      "Epoch [110/300], Loss: 1.9512\n",
      "Epoch [110/300], Loss: 1.9425\n",
      "Epoch [110/300], Loss: 1.9431\n",
      "Epoch [110/300], Loss: 1.9417\n",
      "Epoch [110/300], Loss: 1.9233\n",
      "Epoch [110/300], Loss: 1.9396\n",
      "Epoch [110/300], Loss: 1.9370\n",
      "Epoch [110/300], Loss: 1.9402\n",
      "Epoch [110/300], Loss: 1.9361\n",
      "Epoch [110/300], Loss: 1.9389\n",
      "Epoch [110/300], Loss: 1.9419\n",
      "Epoch [110/300], Loss: 1.8917\n",
      "Epoch [120/300], Loss: 1.9319\n",
      "Epoch [120/300], Loss: 1.9333\n",
      "Epoch [120/300], Loss: 1.9459\n",
      "Epoch [120/300], Loss: 1.9370\n",
      "Epoch [120/300], Loss: 1.9506\n",
      "Epoch [120/300], Loss: 1.9417\n",
      "Epoch [120/300], Loss: 1.9511\n",
      "Epoch [120/300], Loss: 1.9269\n",
      "Epoch [120/300], Loss: 1.9278\n",
      "Epoch [120/300], Loss: 1.9391\n",
      "Epoch [120/300], Loss: 1.9476\n",
      "Epoch [120/300], Loss: 1.9460\n",
      "Epoch [120/300], Loss: 2.0362\n",
      "Epoch [130/300], Loss: 1.9469\n",
      "Epoch [130/300], Loss: 1.9413\n",
      "Epoch [130/300], Loss: 1.9190\n",
      "Epoch [130/300], Loss: 1.9361\n",
      "Epoch [130/300], Loss: 1.9449\n",
      "Epoch [130/300], Loss: 1.9304\n",
      "Epoch [130/300], Loss: 1.9452\n",
      "Epoch [130/300], Loss: 1.9366\n",
      "Epoch [130/300], Loss: 1.9448\n",
      "Epoch [130/300], Loss: 1.9480\n",
      "Epoch [130/300], Loss: 1.9394\n",
      "Epoch [130/300], Loss: 1.9430\n",
      "Epoch [130/300], Loss: 1.9614\n",
      "Epoch [140/300], Loss: 1.9526\n",
      "Epoch [140/300], Loss: 1.9327\n",
      "Epoch [140/300], Loss: 1.9452\n",
      "Epoch [140/300], Loss: 1.9414\n",
      "Epoch [140/300], Loss: 1.9356\n",
      "Epoch [140/300], Loss: 1.9441\n",
      "Epoch [140/300], Loss: 1.9459\n",
      "Epoch [140/300], Loss: 1.9402\n",
      "Epoch [140/300], Loss: 1.9217\n",
      "Epoch [140/300], Loss: 1.9316\n",
      "Epoch [140/300], Loss: 1.9404\n",
      "Epoch [140/300], Loss: 1.9428\n",
      "Epoch [140/300], Loss: 2.0362\n",
      "Epoch [150/300], Loss: 1.9552\n",
      "Epoch [150/300], Loss: 1.9349\n",
      "Epoch [150/300], Loss: 1.9327\n",
      "Epoch [150/300], Loss: 1.9407\n",
      "Epoch [150/300], Loss: 1.9416\n",
      "Epoch [150/300], Loss: 1.9508\n",
      "Epoch [150/300], Loss: 1.9356\n",
      "Epoch [150/300], Loss: 1.9222\n",
      "Epoch [150/300], Loss: 1.9418\n",
      "Epoch [150/300], Loss: 1.9422\n",
      "Epoch [150/300], Loss: 1.9372\n",
      "Epoch [150/300], Loss: 1.9370\n",
      "Epoch [150/300], Loss: 1.9648\n",
      "Epoch [160/300], Loss: 1.9421\n",
      "Epoch [160/300], Loss: 1.9301\n",
      "Epoch [160/300], Loss: 1.9486\n",
      "Epoch [160/300], Loss: 1.9417\n",
      "Epoch [160/300], Loss: 1.9447\n",
      "Epoch [160/300], Loss: 1.9458\n",
      "Epoch [160/300], Loss: 1.9416\n",
      "Epoch [160/300], Loss: 1.9330\n",
      "Epoch [160/300], Loss: 1.9352\n",
      "Epoch [160/300], Loss: 1.9244\n",
      "Epoch [160/300], Loss: 1.9366\n",
      "Epoch [160/300], Loss: 1.9467\n",
      "Epoch [160/300], Loss: 1.8544\n",
      "Epoch [170/300], Loss: 1.9291\n",
      "Epoch [170/300], Loss: 1.9372\n",
      "Epoch [170/300], Loss: 1.9389\n",
      "Epoch [170/300], Loss: 1.9428\n",
      "Epoch [170/300], Loss: 1.9475\n",
      "Epoch [170/300], Loss: 1.9475\n",
      "Epoch [170/300], Loss: 1.9379\n",
      "Epoch [170/300], Loss: 1.9388\n",
      "Epoch [170/300], Loss: 1.9360\n",
      "Epoch [170/300], Loss: 1.9451\n",
      "Epoch [170/300], Loss: 1.9406\n",
      "Epoch [170/300], Loss: 1.9337\n",
      "Epoch [170/300], Loss: 1.9200\n",
      "Epoch [180/300], Loss: 1.9364\n",
      "Epoch [180/300], Loss: 1.9445\n",
      "Epoch [180/300], Loss: 1.9390\n",
      "Epoch [180/300], Loss: 1.9497\n",
      "Epoch [180/300], Loss: 1.9391\n",
      "Epoch [180/300], Loss: 1.9409\n",
      "Epoch [180/300], Loss: 1.9381\n",
      "Epoch [180/300], Loss: 1.9391\n",
      "Epoch [180/300], Loss: 1.9438\n",
      "Epoch [180/300], Loss: 1.9213\n",
      "Epoch [180/300], Loss: 1.9446\n",
      "Epoch [180/300], Loss: 1.9345\n",
      "Epoch [180/300], Loss: 1.9166\n",
      "Epoch [190/300], Loss: 1.9388\n",
      "Epoch [190/300], Loss: 1.9347\n",
      "Epoch [190/300], Loss: 1.9350\n",
      "Epoch [190/300], Loss: 1.9453\n",
      "Epoch [190/300], Loss: 1.9366\n",
      "Epoch [190/300], Loss: 1.9359\n",
      "Epoch [190/300], Loss: 1.9449\n",
      "Epoch [190/300], Loss: 1.9328\n",
      "Epoch [190/300], Loss: 1.9435\n",
      "Epoch [190/300], Loss: 1.9405\n",
      "Epoch [190/300], Loss: 1.9425\n",
      "Epoch [190/300], Loss: 1.9412\n",
      "Epoch [190/300], Loss: 2.0045\n",
      "Epoch [200/300], Loss: 1.9344\n",
      "Epoch [200/300], Loss: 1.9350\n",
      "Epoch [200/300], Loss: 1.9380\n",
      "Epoch [200/300], Loss: 1.9371\n",
      "Epoch [200/300], Loss: 1.9390\n",
      "Epoch [200/300], Loss: 1.9411\n",
      "Epoch [200/300], Loss: 1.9410\n",
      "Epoch [200/300], Loss: 1.9335\n",
      "Epoch [200/300], Loss: 1.9568\n",
      "Epoch [200/300], Loss: 1.9355\n",
      "Epoch [200/300], Loss: 1.9440\n",
      "Epoch [200/300], Loss: 1.9408\n",
      "Epoch [200/300], Loss: 1.9597\n",
      "Epoch [210/300], Loss: 1.9409\n",
      "Epoch [210/300], Loss: 1.9382\n",
      "Epoch [210/300], Loss: 1.9517\n",
      "Epoch [210/300], Loss: 1.9469\n",
      "Epoch [210/300], Loss: 1.9310\n",
      "Epoch [210/300], Loss: 1.9409\n",
      "Epoch [210/300], Loss: 1.9418\n",
      "Epoch [210/300], Loss: 1.9323\n",
      "Epoch [210/300], Loss: 1.9316\n",
      "Epoch [210/300], Loss: 1.9333\n",
      "Epoch [210/300], Loss: 1.9440\n",
      "Epoch [210/300], Loss: 1.9463\n",
      "Epoch [210/300], Loss: 1.9088\n",
      "Epoch [220/300], Loss: 1.9408\n",
      "Epoch [220/300], Loss: 1.9481\n",
      "Epoch [220/300], Loss: 1.9332\n",
      "Epoch [220/300], Loss: 1.9361\n",
      "Epoch [220/300], Loss: 1.9450\n",
      "Epoch [220/300], Loss: 1.9485\n",
      "Epoch [220/300], Loss: 1.9368\n",
      "Epoch [220/300], Loss: 1.9383\n",
      "Epoch [220/300], Loss: 1.9396\n",
      "Epoch [220/300], Loss: 1.9312\n",
      "Epoch [220/300], Loss: 1.9370\n",
      "Epoch [220/300], Loss: 1.9328\n",
      "Epoch [220/300], Loss: 2.0388\n",
      "Epoch [230/300], Loss: 1.9313\n",
      "Epoch [230/300], Loss: 1.9410\n",
      "Epoch [230/300], Loss: 1.9397\n",
      "Epoch [230/300], Loss: 1.9389\n",
      "Epoch [230/300], Loss: 1.9322\n",
      "Epoch [230/300], Loss: 1.9323\n",
      "Epoch [230/300], Loss: 1.9294\n",
      "Epoch [230/300], Loss: 1.9539\n",
      "Epoch [230/300], Loss: 1.9404\n",
      "Epoch [230/300], Loss: 1.9421\n",
      "Epoch [230/300], Loss: 1.9484\n",
      "Epoch [230/300], Loss: 1.9380\n",
      "Epoch [230/300], Loss: 1.9169\n",
      "Epoch [240/300], Loss: 1.9348\n",
      "Epoch [240/300], Loss: 1.9349\n",
      "Epoch [240/300], Loss: 1.9398\n",
      "Epoch [240/300], Loss: 1.9437\n",
      "Epoch [240/300], Loss: 1.9419\n",
      "Epoch [240/300], Loss: 1.9417\n",
      "Epoch [240/300], Loss: 1.9323\n",
      "Epoch [240/300], Loss: 1.9401\n",
      "Epoch [240/300], Loss: 1.9467\n",
      "Epoch [240/300], Loss: 1.9458\n",
      "Epoch [240/300], Loss: 1.9446\n",
      "Epoch [240/300], Loss: 1.9286\n",
      "Epoch [240/300], Loss: 1.9093\n",
      "Epoch [250/300], Loss: 1.9368\n",
      "Epoch [250/300], Loss: 1.9382\n",
      "Epoch [250/300], Loss: 1.9433\n",
      "Epoch [250/300], Loss: 1.9240\n",
      "Epoch [250/300], Loss: 1.9448\n",
      "Epoch [250/300], Loss: 1.9279\n",
      "Epoch [250/300], Loss: 1.9441\n",
      "Epoch [250/300], Loss: 1.9354\n",
      "Epoch [250/300], Loss: 1.9352\n",
      "Epoch [250/300], Loss: 1.9512\n",
      "Epoch [250/300], Loss: 1.9545\n",
      "Epoch [250/300], Loss: 1.9356\n",
      "Epoch [250/300], Loss: 1.9147\n",
      "Epoch [260/300], Loss: 1.9388\n",
      "Epoch [260/300], Loss: 1.9262\n",
      "Epoch [260/300], Loss: 1.9434\n",
      "Epoch [260/300], Loss: 1.9398\n",
      "Epoch [260/300], Loss: 1.9483\n",
      "Epoch [260/300], Loss: 1.9338\n",
      "Epoch [260/300], Loss: 1.9481\n",
      "Epoch [260/300], Loss: 1.9497\n",
      "Epoch [260/300], Loss: 1.9341\n",
      "Epoch [260/300], Loss: 1.9303\n",
      "Epoch [260/300], Loss: 1.9414\n",
      "Epoch [260/300], Loss: 1.9345\n",
      "Epoch [260/300], Loss: 1.9742\n",
      "Epoch [270/300], Loss: 1.9293\n",
      "Epoch [270/300], Loss: 1.9287\n",
      "Epoch [270/300], Loss: 1.9305\n",
      "Epoch [270/300], Loss: 1.9473\n",
      "Epoch [270/300], Loss: 1.9379\n",
      "Epoch [270/300], Loss: 1.9487\n",
      "Epoch [270/300], Loss: 1.9430\n",
      "Epoch [270/300], Loss: 1.9405\n",
      "Epoch [270/300], Loss: 1.9463\n",
      "Epoch [270/300], Loss: 1.9466\n",
      "Epoch [270/300], Loss: 1.9338\n",
      "Epoch [270/300], Loss: 1.9428\n",
      "Epoch [270/300], Loss: 1.7575\n",
      "Epoch [280/300], Loss: 1.9327\n",
      "Epoch [280/300], Loss: 1.9412\n",
      "Epoch [280/300], Loss: 1.9277\n",
      "Epoch [280/300], Loss: 1.9364\n",
      "Epoch [280/300], Loss: 1.9479\n",
      "Epoch [280/300], Loss: 1.9359\n",
      "Epoch [280/300], Loss: 1.9392\n",
      "Epoch [280/300], Loss: 1.9360\n",
      "Epoch [280/300], Loss: 1.9406\n",
      "Epoch [280/300], Loss: 1.9501\n",
      "Epoch [280/300], Loss: 1.9476\n",
      "Epoch [280/300], Loss: 1.9387\n",
      "Epoch [280/300], Loss: 1.9694\n",
      "Epoch [290/300], Loss: 1.9462\n",
      "Epoch [290/300], Loss: 1.9265\n",
      "Epoch [290/300], Loss: 1.9439\n",
      "Epoch [290/300], Loss: 1.9398\n",
      "Epoch [290/300], Loss: 1.9605\n",
      "Epoch [290/300], Loss: 1.9306\n",
      "Epoch [290/300], Loss: 1.9222\n",
      "Epoch [290/300], Loss: 1.9370\n",
      "Epoch [290/300], Loss: 1.9485\n",
      "Epoch [290/300], Loss: 1.9428\n",
      "Epoch [290/300], Loss: 1.9295\n",
      "Epoch [290/300], Loss: 1.9414\n",
      "Epoch [290/300], Loss: 1.9793\n",
      "Epoch [300/300], Loss: 1.9425\n",
      "Epoch [300/300], Loss: 1.9349\n",
      "Epoch [300/300], Loss: 1.9178\n",
      "Epoch [300/300], Loss: 1.9389\n",
      "Epoch [300/300], Loss: 1.9499\n",
      "Epoch [300/300], Loss: 1.9363\n",
      "Epoch [300/300], Loss: 1.9199\n",
      "Epoch [300/300], Loss: 1.9551\n",
      "Epoch [300/300], Loss: 1.9448\n",
      "Epoch [300/300], Loss: 1.9353\n",
      "Epoch [300/300], Loss: 1.9472\n",
      "Epoch [300/300], Loss: 1.9494\n",
      "Epoch [300/300], Loss: 1.9330\n",
      "Epoch [10/300], Loss: 1.9471\n",
      "Epoch [10/300], Loss: 1.9389\n",
      "Epoch [10/300], Loss: 1.9403\n",
      "Epoch [10/300], Loss: 1.9321\n",
      "Epoch [10/300], Loss: 1.9481\n",
      "Epoch [10/300], Loss: 1.9571\n",
      "Epoch [10/300], Loss: 1.9391\n",
      "Epoch [10/300], Loss: 1.9416\n",
      "Epoch [10/300], Loss: 1.9377\n",
      "Epoch [10/300], Loss: 1.9414\n",
      "Epoch [10/300], Loss: 1.9433\n",
      "Epoch [10/300], Loss: 1.9438\n",
      "Epoch [10/300], Loss: 1.9670\n",
      "Epoch [20/300], Loss: 1.9331\n",
      "Epoch [20/300], Loss: 1.9468\n",
      "Epoch [20/300], Loss: 1.9487\n",
      "Epoch [20/300], Loss: 1.9459\n",
      "Epoch [20/300], Loss: 1.9363\n",
      "Epoch [20/300], Loss: 1.9537\n",
      "Epoch [20/300], Loss: 1.9416\n",
      "Epoch [20/300], Loss: 1.9527\n",
      "Epoch [20/300], Loss: 1.9514\n",
      "Epoch [20/300], Loss: 1.9281\n",
      "Epoch [20/300], Loss: 1.9415\n",
      "Epoch [20/300], Loss: 1.9269\n",
      "Epoch [20/300], Loss: 1.8707\n",
      "Epoch [30/300], Loss: 1.9288\n",
      "Epoch [30/300], Loss: 1.9311\n",
      "Epoch [30/300], Loss: 1.9447\n",
      "Epoch [30/300], Loss: 1.9453\n",
      "Epoch [30/300], Loss: 1.9451\n",
      "Epoch [30/300], Loss: 1.9527\n",
      "Epoch [30/300], Loss: 1.9491\n",
      "Epoch [30/300], Loss: 1.9394\n",
      "Epoch [30/300], Loss: 1.9400\n",
      "Epoch [30/300], Loss: 1.9405\n",
      "Epoch [30/300], Loss: 1.9503\n",
      "Epoch [30/300], Loss: 1.9361\n",
      "Epoch [30/300], Loss: 1.9270\n",
      "Epoch [40/300], Loss: 1.9334\n",
      "Epoch [40/300], Loss: 1.9390\n",
      "Epoch [40/300], Loss: 1.9447\n",
      "Epoch [40/300], Loss: 1.9478\n",
      "Epoch [40/300], Loss: 1.9452\n",
      "Epoch [40/300], Loss: 1.9474\n",
      "Epoch [40/300], Loss: 1.9419\n",
      "Epoch [40/300], Loss: 1.9412\n",
      "Epoch [40/300], Loss: 1.9415\n",
      "Epoch [40/300], Loss: 1.9350\n",
      "Epoch [40/300], Loss: 1.9463\n",
      "Epoch [40/300], Loss: 1.9404\n",
      "Epoch [40/300], Loss: 1.9097\n",
      "Epoch [50/300], Loss: 1.9384\n",
      "Epoch [50/300], Loss: 1.9460\n",
      "Epoch [50/300], Loss: 1.9502\n",
      "Epoch [50/300], Loss: 1.9472\n",
      "Epoch [50/300], Loss: 1.9360\n",
      "Epoch [50/300], Loss: 1.9336\n",
      "Epoch [50/300], Loss: 1.9473\n",
      "Epoch [50/300], Loss: 1.9505\n",
      "Epoch [50/300], Loss: 1.9281\n",
      "Epoch [50/300], Loss: 1.9384\n",
      "Epoch [50/300], Loss: 1.9464\n",
      "Epoch [50/300], Loss: 1.9393\n",
      "Epoch [50/300], Loss: 2.0250\n",
      "Epoch [60/300], Loss: 1.9367\n",
      "Epoch [60/300], Loss: 1.9414\n",
      "Epoch [60/300], Loss: 1.9336\n",
      "Epoch [60/300], Loss: 1.9531\n",
      "Epoch [60/300], Loss: 1.9523\n",
      "Epoch [60/300], Loss: 1.9453\n",
      "Epoch [60/300], Loss: 1.9261\n",
      "Epoch [60/300], Loss: 1.9460\n",
      "Epoch [60/300], Loss: 1.9386\n",
      "Epoch [60/300], Loss: 1.9385\n",
      "Epoch [60/300], Loss: 1.9520\n",
      "Epoch [60/300], Loss: 1.9337\n",
      "Epoch [60/300], Loss: 2.0049\n",
      "Epoch [70/300], Loss: 1.9391\n",
      "Epoch [70/300], Loss: 1.9534\n",
      "Epoch [70/300], Loss: 1.9397\n",
      "Epoch [70/300], Loss: 1.9483\n",
      "Epoch [70/300], Loss: 1.9352\n",
      "Epoch [70/300], Loss: 1.9377\n",
      "Epoch [70/300], Loss: 1.9382\n",
      "Epoch [70/300], Loss: 1.9336\n",
      "Epoch [70/300], Loss: 1.9438\n",
      "Epoch [70/300], Loss: 1.9468\n",
      "Epoch [70/300], Loss: 1.9368\n",
      "Epoch [70/300], Loss: 1.9455\n",
      "Epoch [70/300], Loss: 1.9895\n",
      "Epoch [80/300], Loss: 1.9353\n",
      "Epoch [80/300], Loss: 1.9348\n",
      "Epoch [80/300], Loss: 1.9371\n",
      "Epoch [80/300], Loss: 1.9532\n",
      "Epoch [80/300], Loss: 1.9556\n",
      "Epoch [80/300], Loss: 1.9404\n",
      "Epoch [80/300], Loss: 1.9401\n",
      "Epoch [80/300], Loss: 1.9424\n",
      "Epoch [80/300], Loss: 1.9427\n",
      "Epoch [80/300], Loss: 1.9345\n",
      "Epoch [80/300], Loss: 1.9471\n",
      "Epoch [80/300], Loss: 1.9365\n",
      "Epoch [80/300], Loss: 1.9042\n",
      "Epoch [90/300], Loss: 1.9405\n",
      "Epoch [90/300], Loss: 1.9326\n",
      "Epoch [90/300], Loss: 1.9411\n",
      "Epoch [90/300], Loss: 1.9383\n",
      "Epoch [90/300], Loss: 1.9466\n",
      "Epoch [90/300], Loss: 1.9404\n",
      "Epoch [90/300], Loss: 1.9350\n",
      "Epoch [90/300], Loss: 1.9430\n",
      "Epoch [90/300], Loss: 1.9520\n",
      "Epoch [90/300], Loss: 1.9411\n",
      "Epoch [90/300], Loss: 1.9395\n",
      "Epoch [90/300], Loss: 1.9491\n",
      "Epoch [90/300], Loss: 1.9160\n",
      "Epoch [100/300], Loss: 1.9371\n",
      "Epoch [100/300], Loss: 1.9472\n",
      "Epoch [100/300], Loss: 1.9424\n",
      "Epoch [100/300], Loss: 1.9396\n",
      "Epoch [100/300], Loss: 1.9400\n",
      "Epoch [100/300], Loss: 1.9401\n",
      "Epoch [100/300], Loss: 1.9398\n",
      "Epoch [100/300], Loss: 1.9512\n",
      "Epoch [100/300], Loss: 1.9388\n",
      "Epoch [100/300], Loss: 1.9477\n",
      "Epoch [100/300], Loss: 1.9304\n",
      "Epoch [100/300], Loss: 1.9445\n",
      "Epoch [100/300], Loss: 1.8593\n",
      "Epoch [110/300], Loss: 1.9477\n",
      "Epoch [110/300], Loss: 1.9446\n",
      "Epoch [110/300], Loss: 1.9389\n",
      "Epoch [110/300], Loss: 1.9451\n",
      "Epoch [110/300], Loss: 1.9421\n",
      "Epoch [110/300], Loss: 1.9398\n",
      "Epoch [110/300], Loss: 1.9362\n",
      "Epoch [110/300], Loss: 1.9469\n",
      "Epoch [110/300], Loss: 1.9412\n",
      "Epoch [110/300], Loss: 1.9443\n",
      "Epoch [110/300], Loss: 1.9372\n",
      "Epoch [110/300], Loss: 1.9344\n",
      "Epoch [110/300], Loss: 1.8523\n",
      "Epoch [120/300], Loss: 1.9444\n",
      "Epoch [120/300], Loss: 1.9360\n",
      "Epoch [120/300], Loss: 1.9321\n",
      "Epoch [120/300], Loss: 1.9404\n",
      "Epoch [120/300], Loss: 1.9517\n",
      "Epoch [120/300], Loss: 1.9454\n",
      "Epoch [120/300], Loss: 1.9329\n",
      "Epoch [120/300], Loss: 1.9401\n",
      "Epoch [120/300], Loss: 1.9489\n",
      "Epoch [120/300], Loss: 1.9428\n",
      "Epoch [120/300], Loss: 1.9401\n",
      "Epoch [120/300], Loss: 1.9481\n",
      "Epoch [120/300], Loss: 1.8824\n",
      "Epoch [130/300], Loss: 1.9379\n",
      "Epoch [130/300], Loss: 1.9305\n",
      "Epoch [130/300], Loss: 1.9388\n",
      "Epoch [130/300], Loss: 1.9410\n",
      "Epoch [130/300], Loss: 1.9575\n",
      "Epoch [130/300], Loss: 1.9458\n",
      "Epoch [130/300], Loss: 1.9507\n",
      "Epoch [130/300], Loss: 1.9348\n",
      "Epoch [130/300], Loss: 1.9444\n",
      "Epoch [130/300], Loss: 1.9321\n",
      "Epoch [130/300], Loss: 1.9398\n",
      "Epoch [130/300], Loss: 1.9418\n",
      "Epoch [130/300], Loss: 2.0048\n",
      "Epoch [140/300], Loss: 1.9212\n",
      "Epoch [140/300], Loss: 1.9422\n",
      "Epoch [140/300], Loss: 1.9528\n",
      "Epoch [140/300], Loss: 1.9400\n",
      "Epoch [140/300], Loss: 1.9398\n",
      "Epoch [140/300], Loss: 1.9569\n",
      "Epoch [140/300], Loss: 1.9503\n",
      "Epoch [140/300], Loss: 1.9355\n",
      "Epoch [140/300], Loss: 1.9366\n",
      "Epoch [140/300], Loss: 1.9329\n",
      "Epoch [140/300], Loss: 1.9409\n",
      "Epoch [140/300], Loss: 1.9528\n",
      "Epoch [140/300], Loss: 1.9195\n",
      "Epoch [150/300], Loss: 1.9501\n",
      "Epoch [150/300], Loss: 1.9334\n",
      "Epoch [150/300], Loss: 1.9435\n",
      "Epoch [150/300], Loss: 1.9358\n",
      "Epoch [150/300], Loss: 1.9382\n",
      "Epoch [150/300], Loss: 1.9457\n",
      "Epoch [150/300], Loss: 1.9509\n",
      "Epoch [150/300], Loss: 1.9350\n",
      "Epoch [150/300], Loss: 1.9357\n",
      "Epoch [150/300], Loss: 1.9512\n",
      "Epoch [150/300], Loss: 1.9330\n",
      "Epoch [150/300], Loss: 1.9480\n",
      "Epoch [150/300], Loss: 1.9696\n",
      "Epoch [160/300], Loss: 1.9510\n",
      "Epoch [160/300], Loss: 1.9371\n",
      "Epoch [160/300], Loss: 1.9353\n",
      "Epoch [160/300], Loss: 1.9382\n",
      "Epoch [160/300], Loss: 1.9385\n",
      "Epoch [160/300], Loss: 1.9395\n",
      "Epoch [160/300], Loss: 1.9398\n",
      "Epoch [160/300], Loss: 1.9384\n",
      "Epoch [160/300], Loss: 1.9478\n",
      "Epoch [160/300], Loss: 1.9365\n",
      "Epoch [160/300], Loss: 1.9447\n",
      "Epoch [160/300], Loss: 1.9508\n",
      "Epoch [160/300], Loss: 1.9426\n",
      "Epoch [170/300], Loss: 1.9437\n",
      "Epoch [170/300], Loss: 1.9479\n",
      "Epoch [170/300], Loss: 1.9512\n",
      "Epoch [170/300], Loss: 1.9366\n",
      "Epoch [170/300], Loss: 1.9351\n",
      "Epoch [170/300], Loss: 1.9391\n",
      "Epoch [170/300], Loss: 1.9496\n",
      "Epoch [170/300], Loss: 1.9451\n",
      "Epoch [170/300], Loss: 1.9442\n",
      "Epoch [170/300], Loss: 1.9342\n",
      "Epoch [170/300], Loss: 1.9338\n",
      "Epoch [170/300], Loss: 1.9365\n",
      "Epoch [170/300], Loss: 2.0233\n",
      "Epoch [180/300], Loss: 1.9419\n",
      "Epoch [180/300], Loss: 1.9440\n",
      "Epoch [180/300], Loss: 1.9480\n",
      "Epoch [180/300], Loss: 1.9372\n",
      "Epoch [180/300], Loss: 1.9344\n",
      "Epoch [180/300], Loss: 1.9373\n",
      "Epoch [180/300], Loss: 1.9481\n",
      "Epoch [180/300], Loss: 1.9350\n",
      "Epoch [180/300], Loss: 1.9356\n",
      "Epoch [180/300], Loss: 1.9470\n",
      "Epoch [180/300], Loss: 1.9461\n",
      "Epoch [180/300], Loss: 1.9462\n",
      "Epoch [180/300], Loss: 1.9452\n",
      "Epoch [190/300], Loss: 1.9330\n",
      "Epoch [190/300], Loss: 1.9431\n",
      "Epoch [190/300], Loss: 1.9472\n",
      "Epoch [190/300], Loss: 1.9446\n",
      "Epoch [190/300], Loss: 1.9461\n",
      "Epoch [190/300], Loss: 1.9422\n",
      "Epoch [190/300], Loss: 1.9333\n",
      "Epoch [190/300], Loss: 1.9428\n",
      "Epoch [190/300], Loss: 1.9359\n",
      "Epoch [190/300], Loss: 1.9446\n",
      "Epoch [190/300], Loss: 1.9397\n",
      "Epoch [190/300], Loss: 1.9468\n",
      "Epoch [190/300], Loss: 1.8519\n",
      "Epoch [200/300], Loss: 1.9466\n",
      "Epoch [200/300], Loss: 1.9428\n",
      "Epoch [200/300], Loss: 1.9410\n",
      "Epoch [200/300], Loss: 1.9460\n",
      "Epoch [200/300], Loss: 1.9430\n",
      "Epoch [200/300], Loss: 1.9441\n",
      "Epoch [200/300], Loss: 1.9338\n",
      "Epoch [200/300], Loss: 1.9356\n",
      "Epoch [200/300], Loss: 1.9448\n",
      "Epoch [200/300], Loss: 1.9452\n",
      "Epoch [200/300], Loss: 1.9402\n",
      "Epoch [200/300], Loss: 1.9366\n",
      "Epoch [200/300], Loss: 1.9600\n",
      "Epoch [210/300], Loss: 1.9467\n",
      "Epoch [210/300], Loss: 1.9365\n",
      "Epoch [210/300], Loss: 1.9400\n",
      "Epoch [210/300], Loss: 1.9374\n",
      "Epoch [210/300], Loss: 1.9439\n",
      "Epoch [210/300], Loss: 1.9381\n",
      "Epoch [210/300], Loss: 1.9427\n",
      "Epoch [210/300], Loss: 1.9458\n",
      "Epoch [210/300], Loss: 1.9362\n",
      "Epoch [210/300], Loss: 1.9448\n",
      "Epoch [210/300], Loss: 1.9455\n",
      "Epoch [210/300], Loss: 1.9405\n",
      "Epoch [210/300], Loss: 1.9437\n",
      "Epoch [220/300], Loss: 1.9419\n",
      "Epoch [220/300], Loss: 1.9463\n",
      "Epoch [220/300], Loss: 1.9441\n",
      "Epoch [220/300], Loss: 1.9307\n",
      "Epoch [220/300], Loss: 1.9502\n",
      "Epoch [220/300], Loss: 1.9381\n",
      "Epoch [220/300], Loss: 1.9431\n",
      "Epoch [220/300], Loss: 1.9416\n",
      "Epoch [220/300], Loss: 1.9466\n",
      "Epoch [220/300], Loss: 1.9395\n",
      "Epoch [220/300], Loss: 1.9373\n",
      "Epoch [220/300], Loss: 1.9383\n",
      "Epoch [220/300], Loss: 1.8693\n",
      "Epoch [230/300], Loss: 1.9419\n",
      "Epoch [230/300], Loss: 1.9438\n",
      "Epoch [230/300], Loss: 1.9478\n",
      "Epoch [230/300], Loss: 1.9381\n",
      "Epoch [230/300], Loss: 1.9408\n",
      "Epoch [230/300], Loss: 1.9423\n",
      "Epoch [230/300], Loss: 1.9400\n",
      "Epoch [230/300], Loss: 1.9421\n",
      "Epoch [230/300], Loss: 1.9323\n",
      "Epoch [230/300], Loss: 1.9414\n",
      "Epoch [230/300], Loss: 1.9417\n",
      "Epoch [230/300], Loss: 1.9492\n",
      "Epoch [230/300], Loss: 1.8440\n",
      "Epoch [240/300], Loss: 1.9451\n",
      "Epoch [240/300], Loss: 1.9346\n",
      "Epoch [240/300], Loss: 1.9447\n",
      "Epoch [240/300], Loss: 1.9391\n",
      "Epoch [240/300], Loss: 1.9362\n",
      "Epoch [240/300], Loss: 1.9455\n",
      "Epoch [240/300], Loss: 1.9424\n",
      "Epoch [240/300], Loss: 1.9459\n",
      "Epoch [240/300], Loss: 1.9392\n",
      "Epoch [240/300], Loss: 1.9411\n",
      "Epoch [240/300], Loss: 1.9386\n",
      "Epoch [240/300], Loss: 1.9454\n",
      "Epoch [240/300], Loss: 1.9288\n",
      "Epoch [250/300], Loss: 1.9481\n",
      "Epoch [250/300], Loss: 1.9269\n",
      "Epoch [250/300], Loss: 1.9434\n",
      "Epoch [250/300], Loss: 1.9334\n",
      "Epoch [250/300], Loss: 1.9419\n",
      "Epoch [250/300], Loss: 1.9411\n",
      "Epoch [250/300], Loss: 1.9559\n",
      "Epoch [250/300], Loss: 1.9525\n",
      "Epoch [250/300], Loss: 1.9447\n",
      "Epoch [250/300], Loss: 1.9359\n",
      "Epoch [250/300], Loss: 1.9395\n",
      "Epoch [250/300], Loss: 1.9354\n",
      "Epoch [250/300], Loss: 1.9281\n",
      "Epoch [260/300], Loss: 1.9476\n",
      "Epoch [260/300], Loss: 1.9386\n",
      "Epoch [260/300], Loss: 1.9375\n",
      "Epoch [260/300], Loss: 1.9310\n",
      "Epoch [260/300], Loss: 1.9405\n",
      "Epoch [260/300], Loss: 1.9380\n",
      "Epoch [260/300], Loss: 1.9420\n",
      "Epoch [260/300], Loss: 1.9386\n",
      "Epoch [260/300], Loss: 1.9417\n",
      "Epoch [260/300], Loss: 1.9402\n",
      "Epoch [260/300], Loss: 1.9582\n",
      "Epoch [260/300], Loss: 1.9414\n",
      "Epoch [260/300], Loss: 1.9783\n",
      "Epoch [270/300], Loss: 1.9446\n",
      "Epoch [270/300], Loss: 1.9415\n",
      "Epoch [270/300], Loss: 1.9437\n",
      "Epoch [270/300], Loss: 1.9415\n",
      "Epoch [270/300], Loss: 1.9422\n",
      "Epoch [270/300], Loss: 1.9339\n",
      "Epoch [270/300], Loss: 1.9493\n",
      "Epoch [270/300], Loss: 1.9370\n",
      "Epoch [270/300], Loss: 1.9472\n",
      "Epoch [270/300], Loss: 1.9341\n",
      "Epoch [270/300], Loss: 1.9352\n",
      "Epoch [270/300], Loss: 1.9440\n",
      "Epoch [270/300], Loss: 1.9867\n",
      "Epoch [280/300], Loss: 1.9444\n",
      "Epoch [280/300], Loss: 1.9418\n",
      "Epoch [280/300], Loss: 1.9407\n",
      "Epoch [280/300], Loss: 1.9350\n",
      "Epoch [280/300], Loss: 1.9356\n",
      "Epoch [280/300], Loss: 1.9472\n",
      "Epoch [280/300], Loss: 1.9476\n",
      "Epoch [280/300], Loss: 1.9324\n",
      "Epoch [280/300], Loss: 1.9420\n",
      "Epoch [280/300], Loss: 1.9470\n",
      "Epoch [280/300], Loss: 1.9407\n",
      "Epoch [280/300], Loss: 1.9423\n",
      "Epoch [280/300], Loss: 1.8915\n",
      "Epoch [290/300], Loss: 1.9399\n",
      "Epoch [290/300], Loss: 1.9351\n",
      "Epoch [290/300], Loss: 1.9468\n",
      "Epoch [290/300], Loss: 1.9455\n",
      "Epoch [290/300], Loss: 1.9403\n",
      "Epoch [290/300], Loss: 1.9384\n",
      "Epoch [290/300], Loss: 1.9435\n",
      "Epoch [290/300], Loss: 1.9493\n",
      "Epoch [290/300], Loss: 1.9307\n",
      "Epoch [290/300], Loss: 1.9347\n",
      "Epoch [290/300], Loss: 1.9424\n",
      "Epoch [290/300], Loss: 1.9498\n",
      "Epoch [290/300], Loss: 1.9392\n",
      "Epoch [300/300], Loss: 1.9465\n",
      "Epoch [300/300], Loss: 1.9383\n",
      "Epoch [300/300], Loss: 1.9360\n",
      "Epoch [300/300], Loss: 1.9489\n",
      "Epoch [300/300], Loss: 1.9404\n",
      "Epoch [300/300], Loss: 1.9422\n",
      "Epoch [300/300], Loss: 1.9428\n",
      "Epoch [300/300], Loss: 1.9418\n",
      "Epoch [300/300], Loss: 1.9455\n",
      "Epoch [300/300], Loss: 1.9448\n",
      "Epoch [300/300], Loss: 1.9338\n",
      "Epoch [300/300], Loss: 1.9337\n",
      "Epoch [300/300], Loss: 1.9282\n",
      "Epoch [10/300], Loss: 1.9402\n",
      "Epoch [10/300], Loss: 1.9420\n",
      "Epoch [10/300], Loss: 1.9369\n",
      "Epoch [10/300], Loss: 1.9453\n",
      "Epoch [10/300], Loss: 1.9454\n",
      "Epoch [10/300], Loss: 1.9495\n",
      "Epoch [10/300], Loss: 1.9402\n",
      "Epoch [10/300], Loss: 1.9566\n",
      "Epoch [10/300], Loss: 1.9332\n",
      "Epoch [10/300], Loss: 1.9400\n",
      "Epoch [10/300], Loss: 1.9479\n",
      "Epoch [10/300], Loss: 1.9272\n",
      "Epoch [10/300], Loss: 1.9139\n",
      "Epoch [20/300], Loss: 1.9491\n",
      "Epoch [20/300], Loss: 1.9435\n",
      "Epoch [20/300], Loss: 1.9361\n",
      "Epoch [20/300], Loss: 1.9434\n",
      "Epoch [20/300], Loss: 1.9361\n",
      "Epoch [20/300], Loss: 1.9496\n",
      "Epoch [20/300], Loss: 1.9463\n",
      "Epoch [20/300], Loss: 1.9480\n",
      "Epoch [20/300], Loss: 1.9339\n",
      "Epoch [20/300], Loss: 1.9444\n",
      "Epoch [20/300], Loss: 1.9389\n",
      "Epoch [20/300], Loss: 1.9355\n",
      "Epoch [20/300], Loss: 1.9848\n",
      "Epoch [30/300], Loss: 1.9357\n",
      "Epoch [30/300], Loss: 1.9437\n",
      "Epoch [30/300], Loss: 1.9396\n",
      "Epoch [30/300], Loss: 1.9357\n",
      "Epoch [30/300], Loss: 1.9380\n",
      "Epoch [30/300], Loss: 1.9425\n",
      "Epoch [30/300], Loss: 1.9450\n",
      "Epoch [30/300], Loss: 1.9326\n",
      "Epoch [30/300], Loss: 1.9581\n",
      "Epoch [30/300], Loss: 1.9407\n",
      "Epoch [30/300], Loss: 1.9442\n",
      "Epoch [30/300], Loss: 1.9423\n",
      "Epoch [30/300], Loss: 1.9020\n",
      "Epoch [40/300], Loss: 1.9440\n",
      "Epoch [40/300], Loss: 1.9291\n",
      "Epoch [40/300], Loss: 1.9372\n",
      "Epoch [40/300], Loss: 1.9455\n",
      "Epoch [40/300], Loss: 1.9542\n",
      "Epoch [40/300], Loss: 1.9509\n",
      "Epoch [40/300], Loss: 1.9458\n",
      "Epoch [40/300], Loss: 1.9460\n",
      "Epoch [40/300], Loss: 1.9374\n",
      "Epoch [40/300], Loss: 1.9315\n",
      "Epoch [40/300], Loss: 1.9473\n",
      "Epoch [40/300], Loss: 1.9252\n",
      "Epoch [40/300], Loss: 1.9522\n",
      "Epoch [50/300], Loss: 1.9357\n",
      "Epoch [50/300], Loss: 1.9437\n",
      "Epoch [50/300], Loss: 1.9525\n",
      "Epoch [50/300], Loss: 1.9420\n",
      "Epoch [50/300], Loss: 1.9307\n",
      "Epoch [50/300], Loss: 1.9462\n",
      "Epoch [50/300], Loss: 1.9294\n",
      "Epoch [50/300], Loss: 1.9446\n",
      "Epoch [50/300], Loss: 1.9457\n",
      "Epoch [50/300], Loss: 1.9425\n",
      "Epoch [50/300], Loss: 1.9389\n",
      "Epoch [50/300], Loss: 1.9478\n",
      "Epoch [50/300], Loss: 1.9132\n",
      "Epoch [60/300], Loss: 1.9439\n",
      "Epoch [60/300], Loss: 1.9439\n",
      "Epoch [60/300], Loss: 1.9366\n",
      "Epoch [60/300], Loss: 1.9487\n",
      "Epoch [60/300], Loss: 1.9333\n",
      "Epoch [60/300], Loss: 1.9448\n",
      "Epoch [60/300], Loss: 1.9444\n",
      "Epoch [60/300], Loss: 1.9355\n",
      "Epoch [60/300], Loss: 1.9478\n",
      "Epoch [60/300], Loss: 1.9390\n",
      "Epoch [60/300], Loss: 1.9325\n",
      "Epoch [60/300], Loss: 1.9354\n",
      "Epoch [60/300], Loss: 2.0149\n",
      "Epoch [70/300], Loss: 1.9294\n",
      "Epoch [70/300], Loss: 1.9446\n",
      "Epoch [70/300], Loss: 1.9367\n",
      "Epoch [70/300], Loss: 1.9515\n",
      "Epoch [70/300], Loss: 1.9470\n",
      "Epoch [70/300], Loss: 1.9466\n",
      "Epoch [70/300], Loss: 1.9365\n",
      "Epoch [70/300], Loss: 1.9425\n",
      "Epoch [70/300], Loss: 1.9451\n",
      "Epoch [70/300], Loss: 1.9347\n",
      "Epoch [70/300], Loss: 1.9437\n",
      "Epoch [70/300], Loss: 1.9320\n",
      "Epoch [70/300], Loss: 1.9559\n",
      "Epoch [80/300], Loss: 1.9365\n",
      "Epoch [80/300], Loss: 1.9337\n",
      "Epoch [80/300], Loss: 1.9404\n",
      "Epoch [80/300], Loss: 1.9422\n",
      "Epoch [80/300], Loss: 1.9477\n",
      "Epoch [80/300], Loss: 1.9368\n",
      "Epoch [80/300], Loss: 1.9330\n",
      "Epoch [80/300], Loss: 1.9366\n",
      "Epoch [80/300], Loss: 1.9375\n",
      "Epoch [80/300], Loss: 1.9504\n",
      "Epoch [80/300], Loss: 1.9412\n",
      "Epoch [80/300], Loss: 1.9495\n",
      "Epoch [80/300], Loss: 1.9496\n",
      "Epoch [90/300], Loss: 1.9401\n",
      "Epoch [90/300], Loss: 1.9496\n",
      "Epoch [90/300], Loss: 1.9358\n",
      "Epoch [90/300], Loss: 1.9407\n",
      "Epoch [90/300], Loss: 1.9370\n",
      "Epoch [90/300], Loss: 1.9494\n",
      "Epoch [90/300], Loss: 1.9361\n",
      "Epoch [90/300], Loss: 1.9393\n",
      "Epoch [90/300], Loss: 1.9203\n",
      "Epoch [90/300], Loss: 1.9400\n",
      "Epoch [90/300], Loss: 1.9492\n",
      "Epoch [90/300], Loss: 1.9425\n",
      "Epoch [90/300], Loss: 1.9588\n",
      "Epoch [100/300], Loss: 1.9391\n",
      "Epoch [100/300], Loss: 1.9409\n",
      "Epoch [100/300], Loss: 1.9465\n",
      "Epoch [100/300], Loss: 1.9280\n",
      "Epoch [100/300], Loss: 1.9413\n",
      "Epoch [100/300], Loss: 1.9502\n",
      "Epoch [100/300], Loss: 1.9396\n",
      "Epoch [100/300], Loss: 1.9275\n",
      "Epoch [100/300], Loss: 1.9386\n",
      "Epoch [100/300], Loss: 1.9431\n",
      "Epoch [100/300], Loss: 1.9532\n",
      "Epoch [100/300], Loss: 1.9421\n",
      "Epoch [100/300], Loss: 1.9441\n",
      "Epoch [110/300], Loss: 1.9364\n",
      "Epoch [110/300], Loss: 1.9388\n",
      "Epoch [110/300], Loss: 1.9394\n",
      "Epoch [110/300], Loss: 1.9480\n",
      "Epoch [110/300], Loss: 1.9534\n",
      "Epoch [110/300], Loss: 1.9335\n",
      "Epoch [110/300], Loss: 1.9456\n",
      "Epoch [110/300], Loss: 1.9255\n",
      "Epoch [110/300], Loss: 1.9500\n",
      "Epoch [110/300], Loss: 1.9298\n",
      "Epoch [110/300], Loss: 1.9395\n",
      "Epoch [110/300], Loss: 1.9432\n",
      "Epoch [110/300], Loss: 2.0069\n",
      "Epoch [120/300], Loss: 1.9248\n",
      "Epoch [120/300], Loss: 1.9550\n",
      "Epoch [120/300], Loss: 1.9362\n",
      "Epoch [120/300], Loss: 1.9464\n",
      "Epoch [120/300], Loss: 1.9344\n",
      "Epoch [120/300], Loss: 1.9330\n",
      "Epoch [120/300], Loss: 1.9455\n",
      "Epoch [120/300], Loss: 1.9414\n",
      "Epoch [120/300], Loss: 1.9482\n",
      "Epoch [120/300], Loss: 1.9486\n",
      "Epoch [120/300], Loss: 1.9344\n",
      "Epoch [120/300], Loss: 1.9397\n",
      "Epoch [120/300], Loss: 1.9632\n",
      "Epoch [130/300], Loss: 1.9573\n",
      "Epoch [130/300], Loss: 1.9255\n",
      "Epoch [130/300], Loss: 1.9547\n",
      "Epoch [130/300], Loss: 1.9257\n",
      "Epoch [130/300], Loss: 1.9392\n",
      "Epoch [130/300], Loss: 1.9427\n",
      "Epoch [130/300], Loss: 1.9388\n",
      "Epoch [130/300], Loss: 1.9469\n",
      "Epoch [130/300], Loss: 1.9471\n",
      "Epoch [130/300], Loss: 1.9473\n",
      "Epoch [130/300], Loss: 1.9295\n",
      "Epoch [130/300], Loss: 1.9320\n",
      "Epoch [130/300], Loss: 1.8850\n",
      "Epoch [140/300], Loss: 1.9333\n",
      "Epoch [140/300], Loss: 1.9370\n",
      "Epoch [140/300], Loss: 1.9284\n",
      "Epoch [140/300], Loss: 1.9362\n",
      "Epoch [140/300], Loss: 1.9502\n",
      "Epoch [140/300], Loss: 1.9466\n",
      "Epoch [140/300], Loss: 1.9309\n",
      "Epoch [140/300], Loss: 1.9529\n",
      "Epoch [140/300], Loss: 1.9324\n",
      "Epoch [140/300], Loss: 1.9449\n",
      "Epoch [140/300], Loss: 1.9393\n",
      "Epoch [140/300], Loss: 1.9485\n",
      "Epoch [140/300], Loss: 1.9509\n",
      "Epoch [150/300], Loss: 1.9413\n",
      "Epoch [150/300], Loss: 1.9316\n",
      "Epoch [150/300], Loss: 1.9241\n",
      "Epoch [150/300], Loss: 1.9442\n",
      "Epoch [150/300], Loss: 1.9475\n",
      "Epoch [150/300], Loss: 1.9488\n",
      "Epoch [150/300], Loss: 1.9329\n",
      "Epoch [150/300], Loss: 1.9349\n",
      "Epoch [150/300], Loss: 1.9387\n",
      "Epoch [150/300], Loss: 1.9406\n",
      "Epoch [150/300], Loss: 1.9547\n",
      "Epoch [150/300], Loss: 1.9407\n",
      "Epoch [150/300], Loss: 2.0184\n",
      "Epoch [160/300], Loss: 1.9370\n",
      "Epoch [160/300], Loss: 1.9381\n",
      "Epoch [160/300], Loss: 1.9224\n",
      "Epoch [160/300], Loss: 1.9308\n",
      "Epoch [160/300], Loss: 1.9395\n",
      "Epoch [160/300], Loss: 1.9513\n",
      "Epoch [160/300], Loss: 1.9386\n",
      "Epoch [160/300], Loss: 1.9429\n",
      "Epoch [160/300], Loss: 1.9382\n",
      "Epoch [160/300], Loss: 1.9437\n",
      "Epoch [160/300], Loss: 1.9522\n",
      "Epoch [160/300], Loss: 1.9402\n",
      "Epoch [160/300], Loss: 1.9663\n",
      "Epoch [170/300], Loss: 1.9445\n",
      "Epoch [170/300], Loss: 1.9362\n",
      "Epoch [170/300], Loss: 1.9403\n",
      "Epoch [170/300], Loss: 1.9329\n",
      "Epoch [170/300], Loss: 1.9396\n",
      "Epoch [170/300], Loss: 1.9432\n",
      "Epoch [170/300], Loss: 1.9400\n",
      "Epoch [170/300], Loss: 1.9318\n",
      "Epoch [170/300], Loss: 1.9500\n",
      "Epoch [170/300], Loss: 1.9347\n",
      "Epoch [170/300], Loss: 1.9383\n",
      "Epoch [170/300], Loss: 1.9459\n",
      "Epoch [170/300], Loss: 1.8973\n",
      "Epoch [180/300], Loss: 1.9429\n",
      "Epoch [180/300], Loss: 1.9448\n",
      "Epoch [180/300], Loss: 1.9169\n",
      "Epoch [180/300], Loss: 1.9356\n",
      "Epoch [180/300], Loss: 1.9436\n",
      "Epoch [180/300], Loss: 1.9349\n",
      "Epoch [180/300], Loss: 1.9524\n",
      "Epoch [180/300], Loss: 1.9387\n",
      "Epoch [180/300], Loss: 1.9426\n",
      "Epoch [180/300], Loss: 1.9485\n",
      "Epoch [180/300], Loss: 1.9439\n",
      "Epoch [180/300], Loss: 1.9365\n",
      "Epoch [180/300], Loss: 1.8249\n",
      "Epoch [190/300], Loss: 1.9410\n",
      "Epoch [190/300], Loss: 1.9406\n",
      "Epoch [190/300], Loss: 1.9304\n",
      "Epoch [190/300], Loss: 1.9396\n",
      "Epoch [190/300], Loss: 1.9341\n",
      "Epoch [190/300], Loss: 1.9445\n",
      "Epoch [190/300], Loss: 1.9316\n",
      "Epoch [190/300], Loss: 1.9584\n",
      "Epoch [190/300], Loss: 1.9379\n",
      "Epoch [190/300], Loss: 1.9294\n",
      "Epoch [190/300], Loss: 1.9385\n",
      "Epoch [190/300], Loss: 1.9538\n",
      "Epoch [190/300], Loss: 1.8768\n",
      "Epoch [200/300], Loss: 1.9320\n",
      "Epoch [200/300], Loss: 1.9303\n",
      "Epoch [200/300], Loss: 1.9315\n",
      "Epoch [200/300], Loss: 1.9426\n",
      "Epoch [200/300], Loss: 1.9345\n",
      "Epoch [200/300], Loss: 1.9491\n",
      "Epoch [200/300], Loss: 1.9371\n",
      "Epoch [200/300], Loss: 1.9442\n",
      "Epoch [200/300], Loss: 1.9619\n",
      "Epoch [200/300], Loss: 1.9272\n",
      "Epoch [200/300], Loss: 1.9516\n",
      "Epoch [200/300], Loss: 1.9377\n",
      "Epoch [200/300], Loss: 1.9804\n",
      "Epoch [210/300], Loss: 1.9440\n",
      "Epoch [210/300], Loss: 1.9506\n",
      "Epoch [210/300], Loss: 1.9327\n",
      "Epoch [210/300], Loss: 1.9418\n",
      "Epoch [210/300], Loss: 1.9439\n",
      "Epoch [210/300], Loss: 1.9355\n",
      "Epoch [210/300], Loss: 1.9440\n",
      "Epoch [210/300], Loss: 1.9429\n",
      "Epoch [210/300], Loss: 1.9438\n",
      "Epoch [210/300], Loss: 1.9460\n",
      "Epoch [210/300], Loss: 1.9285\n",
      "Epoch [210/300], Loss: 1.9249\n",
      "Epoch [210/300], Loss: 1.7281\n",
      "Epoch [220/300], Loss: 1.9425\n",
      "Epoch [220/300], Loss: 1.9542\n",
      "Epoch [220/300], Loss: 1.9373\n",
      "Epoch [220/300], Loss: 1.9272\n",
      "Epoch [220/300], Loss: 1.9286\n",
      "Epoch [220/300], Loss: 1.9330\n",
      "Epoch [220/300], Loss: 1.9361\n",
      "Epoch [220/300], Loss: 1.9443\n",
      "Epoch [220/300], Loss: 1.9441\n",
      "Epoch [220/300], Loss: 1.9335\n",
      "Epoch [220/300], Loss: 1.9456\n",
      "Epoch [220/300], Loss: 1.9414\n",
      "Epoch [220/300], Loss: 1.9903\n",
      "Epoch [230/300], Loss: 1.9346\n",
      "Epoch [230/300], Loss: 1.9363\n",
      "Epoch [230/300], Loss: 1.9478\n",
      "Epoch [230/300], Loss: 1.9338\n",
      "Epoch [230/300], Loss: 1.9517\n",
      "Epoch [230/300], Loss: 1.9510\n",
      "Epoch [230/300], Loss: 1.9369\n",
      "Epoch [230/300], Loss: 1.9317\n",
      "Epoch [230/300], Loss: 1.9496\n",
      "Epoch [230/300], Loss: 1.9273\n",
      "Epoch [230/300], Loss: 1.9397\n",
      "Epoch [230/300], Loss: 1.9368\n",
      "Epoch [230/300], Loss: 1.9725\n",
      "Epoch [240/300], Loss: 1.9417\n",
      "Epoch [240/300], Loss: 1.9498\n",
      "Epoch [240/300], Loss: 1.9410\n",
      "Epoch [240/300], Loss: 1.9395\n",
      "Epoch [240/300], Loss: 1.9230\n",
      "Epoch [240/300], Loss: 1.9241\n",
      "Epoch [240/300], Loss: 1.9406\n",
      "Epoch [240/300], Loss: 1.9379\n",
      "Epoch [240/300], Loss: 1.9455\n",
      "Epoch [240/300], Loss: 1.9508\n",
      "Epoch [240/300], Loss: 1.9420\n",
      "Epoch [240/300], Loss: 1.9421\n",
      "Epoch [240/300], Loss: 2.0042\n",
      "Epoch [250/300], Loss: 1.9365\n",
      "Epoch [250/300], Loss: 1.9248\n",
      "Epoch [250/300], Loss: 1.9323\n",
      "Epoch [250/300], Loss: 1.9430\n",
      "Epoch [250/300], Loss: 1.9491\n",
      "Epoch [250/300], Loss: 1.9614\n",
      "Epoch [250/300], Loss: 1.9283\n",
      "Epoch [250/300], Loss: 1.9386\n",
      "Epoch [250/300], Loss: 1.9300\n",
      "Epoch [250/300], Loss: 1.9447\n",
      "Epoch [250/300], Loss: 1.9449\n",
      "Epoch [250/300], Loss: 1.9413\n",
      "Epoch [250/300], Loss: 2.0242\n",
      "Epoch [260/300], Loss: 1.9339\n",
      "Epoch [260/300], Loss: 1.9491\n",
      "Epoch [260/300], Loss: 1.9411\n",
      "Epoch [260/300], Loss: 1.9256\n",
      "Epoch [260/300], Loss: 1.9332\n",
      "Epoch [260/300], Loss: 1.9528\n",
      "Epoch [260/300], Loss: 1.9360\n",
      "Epoch [260/300], Loss: 1.9480\n",
      "Epoch [260/300], Loss: 1.9483\n",
      "Epoch [260/300], Loss: 1.9314\n",
      "Epoch [260/300], Loss: 1.9345\n",
      "Epoch [260/300], Loss: 1.9423\n",
      "Epoch [260/300], Loss: 1.8683\n",
      "Epoch [270/300], Loss: 1.9409\n",
      "Epoch [270/300], Loss: 1.9472\n",
      "Epoch [270/300], Loss: 1.9313\n",
      "Epoch [270/300], Loss: 1.9380\n",
      "Epoch [270/300], Loss: 1.9429\n",
      "Epoch [270/300], Loss: 1.9355\n",
      "Epoch [270/300], Loss: 1.9415\n",
      "Epoch [270/300], Loss: 1.9388\n",
      "Epoch [270/300], Loss: 1.9427\n",
      "Epoch [270/300], Loss: 1.9338\n",
      "Epoch [270/300], Loss: 1.9343\n",
      "Epoch [270/300], Loss: 1.9375\n",
      "Epoch [270/300], Loss: 1.8486\n",
      "Epoch [280/300], Loss: 1.9382\n",
      "Epoch [280/300], Loss: 1.9397\n",
      "Epoch [280/300], Loss: 1.9347\n",
      "Epoch [280/300], Loss: 1.9343\n",
      "Epoch [280/300], Loss: 1.9374\n",
      "Epoch [280/300], Loss: 1.9159\n",
      "Epoch [280/300], Loss: 1.9533\n",
      "Epoch [280/300], Loss: 1.9424\n",
      "Epoch [280/300], Loss: 1.9401\n",
      "Epoch [280/300], Loss: 1.9442\n",
      "Epoch [280/300], Loss: 1.9462\n",
      "Epoch [280/300], Loss: 1.9509\n",
      "Epoch [280/300], Loss: 1.9733\n",
      "Epoch [290/300], Loss: 1.9481\n",
      "Epoch [290/300], Loss: 1.9431\n",
      "Epoch [290/300], Loss: 1.9368\n",
      "Epoch [290/300], Loss: 1.9352\n",
      "Epoch [290/300], Loss: 1.9363\n",
      "Epoch [290/300], Loss: 1.9435\n",
      "Epoch [290/300], Loss: 1.9253\n",
      "Epoch [290/300], Loss: 1.9335\n",
      "Epoch [290/300], Loss: 1.9476\n",
      "Epoch [290/300], Loss: 1.9337\n",
      "Epoch [290/300], Loss: 1.9440\n",
      "Epoch [290/300], Loss: 1.9449\n",
      "Epoch [290/300], Loss: 1.8877\n",
      "Epoch [300/300], Loss: 1.9308\n",
      "Epoch [300/300], Loss: 1.9607\n",
      "Epoch [300/300], Loss: 1.9345\n",
      "Epoch [300/300], Loss: 1.9346\n",
      "Epoch [300/300], Loss: 1.9301\n",
      "Epoch [300/300], Loss: 1.9355\n",
      "Epoch [300/300], Loss: 1.9323\n",
      "Epoch [300/300], Loss: 1.9524\n",
      "Epoch [300/300], Loss: 1.9400\n",
      "Epoch [300/300], Loss: 1.9447\n",
      "Epoch [300/300], Loss: 1.9468\n",
      "Epoch [300/300], Loss: 1.9193\n",
      "Epoch [300/300], Loss: 1.9882\n",
      "Epoch [10/300], Loss: 1.9376\n",
      "Epoch [10/300], Loss: 1.9343\n",
      "Epoch [10/300], Loss: 1.9370\n",
      "Epoch [10/300], Loss: 1.9280\n",
      "Epoch [10/300], Loss: 1.9395\n",
      "Epoch [10/300], Loss: 1.9557\n",
      "Epoch [10/300], Loss: 1.9510\n",
      "Epoch [10/300], Loss: 1.9385\n",
      "Epoch [10/300], Loss: 1.9320\n",
      "Epoch [10/300], Loss: 1.9385\n",
      "Epoch [10/300], Loss: 1.9360\n",
      "Epoch [10/300], Loss: 1.9334\n",
      "Epoch [10/300], Loss: 1.8590\n",
      "Epoch [20/300], Loss: 1.9357\n",
      "Epoch [20/300], Loss: 1.9532\n",
      "Epoch [20/300], Loss: 1.9324\n",
      "Epoch [20/300], Loss: 1.9405\n",
      "Epoch [20/300], Loss: 1.9520\n",
      "Epoch [20/300], Loss: 1.9250\n",
      "Epoch [20/300], Loss: 1.9275\n",
      "Epoch [20/300], Loss: 1.9362\n",
      "Epoch [20/300], Loss: 1.9329\n",
      "Epoch [20/300], Loss: 1.9522\n",
      "Epoch [20/300], Loss: 1.9503\n",
      "Epoch [20/300], Loss: 1.9290\n",
      "Epoch [20/300], Loss: 1.8459\n",
      "Epoch [30/300], Loss: 1.9420\n",
      "Epoch [30/300], Loss: 1.9489\n",
      "Epoch [30/300], Loss: 1.9498\n",
      "Epoch [30/300], Loss: 1.9441\n",
      "Epoch [30/300], Loss: 1.9387\n",
      "Epoch [30/300], Loss: 1.9275\n",
      "Epoch [30/300], Loss: 1.9372\n",
      "Epoch [30/300], Loss: 1.9273\n",
      "Epoch [30/300], Loss: 1.9351\n",
      "Epoch [30/300], Loss: 1.9258\n",
      "Epoch [30/300], Loss: 1.9396\n",
      "Epoch [30/300], Loss: 1.9448\n",
      "Epoch [30/300], Loss: 1.8547\n",
      "Epoch [40/300], Loss: 1.9241\n",
      "Epoch [40/300], Loss: 1.9495\n",
      "Epoch [40/300], Loss: 1.9378\n",
      "Epoch [40/300], Loss: 1.9354\n",
      "Epoch [40/300], Loss: 1.9444\n",
      "Epoch [40/300], Loss: 1.9470\n",
      "Epoch [40/300], Loss: 1.9310\n",
      "Epoch [40/300], Loss: 1.9442\n",
      "Epoch [40/300], Loss: 1.9374\n",
      "Epoch [40/300], Loss: 1.9368\n",
      "Epoch [40/300], Loss: 1.9342\n",
      "Epoch [40/300], Loss: 1.9449\n",
      "Epoch [40/300], Loss: 1.9779\n",
      "Epoch [50/300], Loss: 1.9368\n",
      "Epoch [50/300], Loss: 1.9354\n",
      "Epoch [50/300], Loss: 1.9284\n",
      "Epoch [50/300], Loss: 1.9420\n",
      "Epoch [50/300], Loss: 1.9513\n",
      "Epoch [50/300], Loss: 1.9381\n",
      "Epoch [50/300], Loss: 1.9356\n",
      "Epoch [50/300], Loss: 1.9280\n",
      "Epoch [50/300], Loss: 1.9413\n",
      "Epoch [50/300], Loss: 1.9494\n",
      "Epoch [50/300], Loss: 1.9286\n",
      "Epoch [50/300], Loss: 1.9464\n",
      "Epoch [50/300], Loss: 1.8643\n",
      "Epoch [60/300], Loss: 1.9256\n",
      "Epoch [60/300], Loss: 1.9323\n",
      "Epoch [60/300], Loss: 1.9429\n",
      "Epoch [60/300], Loss: 1.9425\n",
      "Epoch [60/300], Loss: 1.9451\n",
      "Epoch [60/300], Loss: 1.9424\n",
      "Epoch [60/300], Loss: 1.9369\n",
      "Epoch [60/300], Loss: 1.9375\n",
      "Epoch [60/300], Loss: 1.9415\n",
      "Epoch [60/300], Loss: 1.9351\n",
      "Epoch [60/300], Loss: 1.9394\n",
      "Epoch [60/300], Loss: 1.9446\n",
      "Epoch [60/300], Loss: 1.9713\n",
      "Epoch [70/300], Loss: 1.9466\n",
      "Epoch [70/300], Loss: 1.9180\n",
      "Epoch [70/300], Loss: 1.9438\n",
      "Epoch [70/300], Loss: 1.9428\n",
      "Epoch [70/300], Loss: 1.9440\n",
      "Epoch [70/300], Loss: 1.9329\n",
      "Epoch [70/300], Loss: 1.9402\n",
      "Epoch [70/300], Loss: 1.9411\n",
      "Epoch [70/300], Loss: 1.9456\n",
      "Epoch [70/300], Loss: 1.9336\n",
      "Epoch [70/300], Loss: 1.9387\n",
      "Epoch [70/300], Loss: 1.9399\n",
      "Epoch [70/300], Loss: 1.9327\n",
      "Epoch [80/300], Loss: 1.9416\n",
      "Epoch [80/300], Loss: 1.9456\n",
      "Epoch [80/300], Loss: 1.9312\n",
      "Epoch [80/300], Loss: 1.9345\n",
      "Epoch [80/300], Loss: 1.9502\n",
      "Epoch [80/300], Loss: 1.9340\n",
      "Epoch [80/300], Loss: 1.9463\n",
      "Epoch [80/300], Loss: 1.9464\n",
      "Epoch [80/300], Loss: 1.9397\n",
      "Epoch [80/300], Loss: 1.9312\n",
      "Epoch [80/300], Loss: 1.9520\n",
      "Epoch [80/300], Loss: 1.9206\n",
      "Epoch [80/300], Loss: 1.8778\n",
      "Epoch [90/300], Loss: 1.9468\n",
      "Epoch [90/300], Loss: 1.9444\n",
      "Epoch [90/300], Loss: 1.9260\n",
      "Epoch [90/300], Loss: 1.9445\n",
      "Epoch [90/300], Loss: 1.9433\n",
      "Epoch [90/300], Loss: 1.9324\n",
      "Epoch [90/300], Loss: 1.9364\n",
      "Epoch [90/300], Loss: 1.9462\n",
      "Epoch [90/300], Loss: 1.9333\n",
      "Epoch [90/300], Loss: 1.9320\n",
      "Epoch [90/300], Loss: 1.9293\n",
      "Epoch [90/300], Loss: 1.9477\n",
      "Epoch [90/300], Loss: 1.8141\n",
      "Epoch [100/300], Loss: 1.9543\n",
      "Epoch [100/300], Loss: 1.9385\n",
      "Epoch [100/300], Loss: 1.9418\n",
      "Epoch [100/300], Loss: 1.9358\n",
      "Epoch [100/300], Loss: 1.9375\n",
      "Epoch [100/300], Loss: 1.9302\n",
      "Epoch [100/300], Loss: 1.9602\n",
      "Epoch [100/300], Loss: 1.9237\n",
      "Epoch [100/300], Loss: 1.9387\n",
      "Epoch [100/300], Loss: 1.9188\n",
      "Epoch [100/300], Loss: 1.9296\n",
      "Epoch [100/300], Loss: 1.9486\n",
      "Epoch [100/300], Loss: 2.0242\n",
      "Epoch [110/300], Loss: 1.9354\n",
      "Epoch [110/300], Loss: 1.9473\n",
      "Epoch [110/300], Loss: 1.9556\n",
      "Epoch [110/300], Loss: 1.9510\n",
      "Epoch [110/300], Loss: 1.9481\n",
      "Epoch [110/300], Loss: 1.9179\n",
      "Epoch [110/300], Loss: 1.9315\n",
      "Epoch [110/300], Loss: 1.9359\n",
      "Epoch [110/300], Loss: 1.9356\n",
      "Epoch [110/300], Loss: 1.9396\n",
      "Epoch [110/300], Loss: 1.9226\n",
      "Epoch [110/300], Loss: 1.9430\n",
      "Epoch [110/300], Loss: 2.0271\n",
      "Epoch [120/300], Loss: 1.9455\n",
      "Epoch [120/300], Loss: 1.9148\n",
      "Epoch [120/300], Loss: 1.9344\n",
      "Epoch [120/300], Loss: 1.9305\n",
      "Epoch [120/300], Loss: 1.9332\n",
      "Epoch [120/300], Loss: 1.9398\n",
      "Epoch [120/300], Loss: 1.9251\n",
      "Epoch [120/300], Loss: 1.9363\n",
      "Epoch [120/300], Loss: 1.9581\n",
      "Epoch [120/300], Loss: 1.9377\n",
      "Epoch [120/300], Loss: 1.9596\n",
      "Epoch [120/300], Loss: 1.9466\n",
      "Epoch [120/300], Loss: 2.0113\n",
      "Epoch [130/300], Loss: 1.9349\n",
      "Epoch [130/300], Loss: 1.9433\n",
      "Epoch [130/300], Loss: 1.9508\n",
      "Epoch [130/300], Loss: 1.9299\n",
      "Epoch [130/300], Loss: 1.9397\n",
      "Epoch [130/300], Loss: 1.9145\n",
      "Epoch [130/300], Loss: 1.9327\n",
      "Epoch [130/300], Loss: 1.9396\n",
      "Epoch [130/300], Loss: 1.9473\n",
      "Epoch [130/300], Loss: 1.9398\n",
      "Epoch [130/300], Loss: 1.9435\n",
      "Epoch [130/300], Loss: 1.9482\n",
      "Epoch [130/300], Loss: 1.8643\n",
      "Epoch [140/300], Loss: 1.9283\n",
      "Epoch [140/300], Loss: 1.9420\n",
      "Epoch [140/300], Loss: 1.9388\n",
      "Epoch [140/300], Loss: 1.9341\n",
      "Epoch [140/300], Loss: 1.9278\n",
      "Epoch [140/300], Loss: 1.9462\n",
      "Epoch [140/300], Loss: 1.9383\n",
      "Epoch [140/300], Loss: 1.9582\n",
      "Epoch [140/300], Loss: 1.9472\n",
      "Epoch [140/300], Loss: 1.9338\n",
      "Epoch [140/300], Loss: 1.9357\n",
      "Epoch [140/300], Loss: 1.9391\n",
      "Epoch [140/300], Loss: 1.8234\n",
      "Epoch [150/300], Loss: 1.9333\n",
      "Epoch [150/300], Loss: 1.9438\n",
      "Epoch [150/300], Loss: 1.9319\n",
      "Epoch [150/300], Loss: 1.9431\n",
      "Epoch [150/300], Loss: 1.9374\n",
      "Epoch [150/300], Loss: 1.9430\n",
      "Epoch [150/300], Loss: 1.9446\n",
      "Epoch [150/300], Loss: 1.9393\n",
      "Epoch [150/300], Loss: 1.9284\n",
      "Epoch [150/300], Loss: 1.9359\n",
      "Epoch [150/300], Loss: 1.9521\n",
      "Epoch [150/300], Loss: 1.9294\n",
      "Epoch [150/300], Loss: 1.9869\n",
      "Epoch [160/300], Loss: 1.9407\n",
      "Epoch [160/300], Loss: 1.9401\n",
      "Epoch [160/300], Loss: 1.9560\n",
      "Epoch [160/300], Loss: 1.9375\n",
      "Epoch [160/300], Loss: 1.9414\n",
      "Epoch [160/300], Loss: 1.9408\n",
      "Epoch [160/300], Loss: 1.9356\n",
      "Epoch [160/300], Loss: 1.9288\n",
      "Epoch [160/300], Loss: 1.9263\n",
      "Epoch [160/300], Loss: 1.9439\n",
      "Epoch [160/300], Loss: 1.9328\n",
      "Epoch [160/300], Loss: 1.9428\n",
      "Epoch [160/300], Loss: 1.8446\n",
      "Epoch [170/300], Loss: 1.9513\n",
      "Epoch [170/300], Loss: 1.9376\n",
      "Epoch [170/300], Loss: 1.9454\n",
      "Epoch [170/300], Loss: 1.9419\n",
      "Epoch [170/300], Loss: 1.9396\n",
      "Epoch [170/300], Loss: 1.9372\n",
      "Epoch [170/300], Loss: 1.9459\n",
      "Epoch [170/300], Loss: 1.9337\n",
      "Epoch [170/300], Loss: 1.9435\n",
      "Epoch [170/300], Loss: 1.9266\n",
      "Epoch [170/300], Loss: 1.9236\n",
      "Epoch [170/300], Loss: 1.9370\n",
      "Epoch [170/300], Loss: 2.0138\n",
      "Epoch [180/300], Loss: 1.9333\n",
      "Epoch [180/300], Loss: 1.9397\n",
      "Epoch [180/300], Loss: 1.9315\n",
      "Epoch [180/300], Loss: 1.9465\n",
      "Epoch [180/300], Loss: 1.9380\n",
      "Epoch [180/300], Loss: 1.9476\n",
      "Epoch [180/300], Loss: 1.9320\n",
      "Epoch [180/300], Loss: 1.9346\n",
      "Epoch [180/300], Loss: 1.9430\n",
      "Epoch [180/300], Loss: 1.9381\n",
      "Epoch [180/300], Loss: 1.9310\n",
      "Epoch [180/300], Loss: 1.9516\n",
      "Epoch [180/300], Loss: 2.0125\n",
      "Epoch [190/300], Loss: 1.9440\n",
      "Epoch [190/300], Loss: 1.9352\n",
      "Epoch [190/300], Loss: 1.9407\n",
      "Epoch [190/300], Loss: 1.9347\n",
      "Epoch [190/300], Loss: 1.9214\n",
      "Epoch [190/300], Loss: 1.9345\n",
      "Epoch [190/300], Loss: 1.9447\n",
      "Epoch [190/300], Loss: 1.9431\n",
      "Epoch [190/300], Loss: 1.9443\n",
      "Epoch [190/300], Loss: 1.9260\n",
      "Epoch [190/300], Loss: 1.9474\n",
      "Epoch [190/300], Loss: 1.9494\n",
      "Epoch [190/300], Loss: 1.7237\n",
      "Epoch [200/300], Loss: 1.9380\n",
      "Epoch [200/300], Loss: 1.9410\n",
      "Epoch [200/300], Loss: 1.9538\n",
      "Epoch [200/300], Loss: 1.9407\n",
      "Epoch [200/300], Loss: 1.9217\n",
      "Epoch [200/300], Loss: 1.9489\n",
      "Epoch [200/300], Loss: 1.9462\n",
      "Epoch [200/300], Loss: 1.9240\n",
      "Epoch [200/300], Loss: 1.9488\n",
      "Epoch [200/300], Loss: 1.9416\n",
      "Epoch [200/300], Loss: 1.9328\n",
      "Epoch [200/300], Loss: 1.9284\n",
      "Epoch [200/300], Loss: 1.8279\n",
      "Epoch [210/300], Loss: 1.9383\n",
      "Epoch [210/300], Loss: 1.9482\n",
      "Epoch [210/300], Loss: 1.9412\n",
      "Epoch [210/300], Loss: 1.9364\n",
      "Epoch [210/300], Loss: 1.9435\n",
      "Epoch [210/300], Loss: 1.9278\n",
      "Epoch [210/300], Loss: 1.9305\n",
      "Epoch [210/300], Loss: 1.9391\n",
      "Epoch [210/300], Loss: 1.9381\n",
      "Epoch [210/300], Loss: 1.9275\n",
      "Epoch [210/300], Loss: 1.9473\n",
      "Epoch [210/300], Loss: 1.9389\n",
      "Epoch [210/300], Loss: 1.9740\n",
      "Epoch [220/300], Loss: 1.9444\n",
      "Epoch [220/300], Loss: 1.9422\n",
      "Epoch [220/300], Loss: 1.9262\n",
      "Epoch [220/300], Loss: 1.9631\n",
      "Epoch [220/300], Loss: 1.9356\n",
      "Epoch [220/300], Loss: 1.9440\n",
      "Epoch [220/300], Loss: 1.9438\n",
      "Epoch [220/300], Loss: 1.9326\n",
      "Epoch [220/300], Loss: 1.9481\n",
      "Epoch [220/300], Loss: 1.9239\n",
      "Epoch [220/300], Loss: 1.9308\n",
      "Epoch [220/300], Loss: 1.9375\n",
      "Epoch [220/300], Loss: 1.9369\n",
      "Epoch [230/300], Loss: 1.9296\n",
      "Epoch [230/300], Loss: 1.9367\n",
      "Epoch [230/300], Loss: 1.9405\n",
      "Epoch [230/300], Loss: 1.9410\n",
      "Epoch [230/300], Loss: 1.9411\n",
      "Epoch [230/300], Loss: 1.9452\n",
      "Epoch [230/300], Loss: 1.9245\n",
      "Epoch [230/300], Loss: 1.9360\n",
      "Epoch [230/300], Loss: 1.9536\n",
      "Epoch [230/300], Loss: 1.9425\n",
      "Epoch [230/300], Loss: 1.9475\n",
      "Epoch [230/300], Loss: 1.9286\n",
      "Epoch [230/300], Loss: 2.0148\n",
      "Epoch [240/300], Loss: 1.9525\n",
      "Epoch [240/300], Loss: 1.9348\n",
      "Epoch [240/300], Loss: 1.9305\n",
      "Epoch [240/300], Loss: 1.9455\n",
      "Epoch [240/300], Loss: 1.9326\n",
      "Epoch [240/300], Loss: 1.9463\n",
      "Epoch [240/300], Loss: 1.9350\n",
      "Epoch [240/300], Loss: 1.9346\n",
      "Epoch [240/300], Loss: 1.9407\n",
      "Epoch [240/300], Loss: 1.9341\n",
      "Epoch [240/300], Loss: 1.9374\n",
      "Epoch [240/300], Loss: 1.9359\n",
      "Epoch [240/300], Loss: 2.0083\n",
      "Epoch [250/300], Loss: 1.9147\n",
      "Epoch [250/300], Loss: 1.9595\n",
      "Epoch [250/300], Loss: 1.9433\n",
      "Epoch [250/300], Loss: 1.9471\n",
      "Epoch [250/300], Loss: 1.9270\n",
      "Epoch [250/300], Loss: 1.9335\n",
      "Epoch [250/300], Loss: 1.9418\n",
      "Epoch [250/300], Loss: 1.9436\n",
      "Epoch [250/300], Loss: 1.9383\n",
      "Epoch [250/300], Loss: 1.9367\n",
      "Epoch [250/300], Loss: 1.9354\n",
      "Epoch [250/300], Loss: 1.9502\n",
      "Epoch [250/300], Loss: 2.0330\n",
      "Epoch [260/300], Loss: 1.9320\n",
      "Epoch [260/300], Loss: 1.9408\n",
      "Epoch [260/300], Loss: 1.9331\n",
      "Epoch [260/300], Loss: 1.9508\n",
      "Epoch [260/300], Loss: 1.9239\n",
      "Epoch [260/300], Loss: 1.9478\n",
      "Epoch [260/300], Loss: 1.9453\n",
      "Epoch [260/300], Loss: 1.9207\n",
      "Epoch [260/300], Loss: 1.9450\n",
      "Epoch [260/300], Loss: 1.9196\n",
      "Epoch [260/300], Loss: 1.9523\n",
      "Epoch [260/300], Loss: 1.9495\n",
      "Epoch [260/300], Loss: 1.9680\n",
      "Epoch [270/300], Loss: 1.9461\n",
      "Epoch [270/300], Loss: 1.9380\n",
      "Epoch [270/300], Loss: 1.9409\n",
      "Epoch [270/300], Loss: 1.9447\n",
      "Epoch [270/300], Loss: 1.9441\n",
      "Epoch [270/300], Loss: 1.9544\n",
      "Epoch [270/300], Loss: 1.9229\n",
      "Epoch [270/300], Loss: 1.9214\n",
      "Epoch [270/300], Loss: 1.9393\n",
      "Epoch [270/300], Loss: 1.9380\n",
      "Epoch [270/300], Loss: 1.9290\n",
      "Epoch [270/300], Loss: 1.9455\n",
      "Epoch [270/300], Loss: 1.7197\n",
      "Epoch [280/300], Loss: 1.9469\n",
      "Epoch [280/300], Loss: 1.9222\n",
      "Epoch [280/300], Loss: 1.9462\n",
      "Epoch [280/300], Loss: 1.9346\n",
      "Epoch [280/300], Loss: 1.9199\n",
      "Epoch [280/300], Loss: 1.9280\n",
      "Epoch [280/300], Loss: 1.9373\n",
      "Epoch [280/300], Loss: 1.9431\n",
      "Epoch [280/300], Loss: 1.9605\n",
      "Epoch [280/300], Loss: 1.9417\n",
      "Epoch [280/300], Loss: 1.9470\n",
      "Epoch [280/300], Loss: 1.9369\n",
      "Epoch [280/300], Loss: 2.0103\n",
      "Epoch [290/300], Loss: 1.9305\n",
      "Epoch [290/300], Loss: 1.9459\n",
      "Epoch [290/300], Loss: 1.9345\n",
      "Epoch [290/300], Loss: 1.9483\n",
      "Epoch [290/300], Loss: 1.9196\n",
      "Epoch [290/300], Loss: 1.9528\n",
      "Epoch [290/300], Loss: 1.9445\n",
      "Epoch [290/300], Loss: 1.9231\n",
      "Epoch [290/300], Loss: 1.9461\n",
      "Epoch [290/300], Loss: 1.9445\n",
      "Epoch [290/300], Loss: 1.9437\n",
      "Epoch [290/300], Loss: 1.9287\n",
      "Epoch [290/300], Loss: 2.0234\n",
      "Epoch [300/300], Loss: 1.9276\n",
      "Epoch [300/300], Loss: 1.9426\n",
      "Epoch [300/300], Loss: 1.9566\n",
      "Epoch [300/300], Loss: 1.9273\n",
      "Epoch [300/300], Loss: 1.9453\n",
      "Epoch [300/300], Loss: 1.9412\n",
      "Epoch [300/300], Loss: 1.9307\n",
      "Epoch [300/300], Loss: 1.9383\n",
      "Epoch [300/300], Loss: 1.9560\n",
      "Epoch [300/300], Loss: 1.9357\n",
      "Epoch [300/300], Loss: 1.9276\n",
      "Epoch [300/300], Loss: 1.9361\n",
      "Epoch [300/300], Loss: 1.8211\n",
      "Epoch [10/300], Loss: 1.9491\n",
      "Epoch [10/300], Loss: 1.9337\n",
      "Epoch [10/300], Loss: 1.9487\n",
      "Epoch [10/300], Loss: 1.9467\n",
      "Epoch [10/300], Loss: 1.9454\n",
      "Epoch [10/300], Loss: 1.9408\n",
      "Epoch [10/300], Loss: 1.9356\n",
      "Epoch [10/300], Loss: 1.9482\n",
      "Epoch [10/300], Loss: 1.9538\n",
      "Epoch [10/300], Loss: 1.9314\n",
      "Epoch [10/300], Loss: 1.9311\n",
      "Epoch [10/300], Loss: 1.9557\n",
      "Epoch [10/300], Loss: 1.8699\n",
      "Epoch [20/300], Loss: 1.9509\n",
      "Epoch [20/300], Loss: 1.9410\n",
      "Epoch [20/300], Loss: 1.9469\n",
      "Epoch [20/300], Loss: 1.9454\n",
      "Epoch [20/300], Loss: 1.9398\n",
      "Epoch [20/300], Loss: 1.9440\n",
      "Epoch [20/300], Loss: 1.9479\n",
      "Epoch [20/300], Loss: 1.9398\n",
      "Epoch [20/300], Loss: 1.9533\n",
      "Epoch [20/300], Loss: 1.9323\n",
      "Epoch [20/300], Loss: 1.9396\n",
      "Epoch [20/300], Loss: 1.9277\n",
      "Epoch [20/300], Loss: 1.9719\n",
      "Epoch [30/300], Loss: 1.9468\n",
      "Epoch [30/300], Loss: 1.9493\n",
      "Epoch [30/300], Loss: 1.9291\n",
      "Epoch [30/300], Loss: 1.9341\n",
      "Epoch [30/300], Loss: 1.9549\n",
      "Epoch [30/300], Loss: 1.9498\n",
      "Epoch [30/300], Loss: 1.9484\n",
      "Epoch [30/300], Loss: 1.9468\n",
      "Epoch [30/300], Loss: 1.9343\n",
      "Epoch [30/300], Loss: 1.9369\n",
      "Epoch [30/300], Loss: 1.9497\n",
      "Epoch [30/300], Loss: 1.9290\n",
      "Epoch [30/300], Loss: 1.8990\n",
      "Epoch [40/300], Loss: 1.9420\n",
      "Epoch [40/300], Loss: 1.9324\n",
      "Epoch [40/300], Loss: 1.9432\n",
      "Epoch [40/300], Loss: 1.9408\n",
      "Epoch [40/300], Loss: 1.9412\n",
      "Epoch [40/300], Loss: 1.9533\n",
      "Epoch [40/300], Loss: 1.9441\n",
      "Epoch [40/300], Loss: 1.9319\n",
      "Epoch [40/300], Loss: 1.9359\n",
      "Epoch [40/300], Loss: 1.9412\n",
      "Epoch [40/300], Loss: 1.9521\n",
      "Epoch [40/300], Loss: 1.9536\n",
      "Epoch [40/300], Loss: 2.0177\n",
      "Epoch [50/300], Loss: 1.9277\n",
      "Epoch [50/300], Loss: 1.9417\n",
      "Epoch [50/300], Loss: 1.9502\n",
      "Epoch [50/300], Loss: 1.9609\n",
      "Epoch [50/300], Loss: 1.9274\n",
      "Epoch [50/300], Loss: 1.9485\n",
      "Epoch [50/300], Loss: 1.9491\n",
      "Epoch [50/300], Loss: 1.9281\n",
      "Epoch [50/300], Loss: 1.9428\n",
      "Epoch [50/300], Loss: 1.9398\n",
      "Epoch [50/300], Loss: 1.9364\n",
      "Epoch [50/300], Loss: 1.9466\n",
      "Epoch [50/300], Loss: 2.0225\n",
      "Epoch [60/300], Loss: 1.9283\n",
      "Epoch [60/300], Loss: 1.9385\n",
      "Epoch [60/300], Loss: 1.9502\n",
      "Epoch [60/300], Loss: 1.9387\n",
      "Epoch [60/300], Loss: 1.9562\n",
      "Epoch [60/300], Loss: 1.9420\n",
      "Epoch [60/300], Loss: 1.9463\n",
      "Epoch [60/300], Loss: 1.9400\n",
      "Epoch [60/300], Loss: 1.9364\n",
      "Epoch [60/300], Loss: 1.9482\n",
      "Epoch [60/300], Loss: 1.9467\n",
      "Epoch [60/300], Loss: 1.9317\n",
      "Epoch [60/300], Loss: 1.9242\n",
      "Epoch [70/300], Loss: 1.9347\n",
      "Epoch [70/300], Loss: 1.9398\n",
      "Epoch [70/300], Loss: 1.9491\n",
      "Epoch [70/300], Loss: 1.9355\n",
      "Epoch [70/300], Loss: 1.9392\n",
      "Epoch [70/300], Loss: 1.9472\n",
      "Epoch [70/300], Loss: 1.9414\n",
      "Epoch [70/300], Loss: 1.9442\n",
      "Epoch [70/300], Loss: 1.9441\n",
      "Epoch [70/300], Loss: 1.9452\n",
      "Epoch [70/300], Loss: 1.9510\n",
      "Epoch [70/300], Loss: 1.9379\n",
      "Epoch [70/300], Loss: 1.8829\n",
      "Epoch [80/300], Loss: 1.9504\n",
      "Epoch [80/300], Loss: 1.9426\n",
      "Epoch [80/300], Loss: 1.9454\n",
      "Epoch [80/300], Loss: 1.9407\n",
      "Epoch [80/300], Loss: 1.9353\n",
      "Epoch [80/300], Loss: 1.9454\n",
      "Epoch [80/300], Loss: 1.9295\n",
      "Epoch [80/300], Loss: 1.9311\n",
      "Epoch [80/300], Loss: 1.9580\n",
      "Epoch [80/300], Loss: 1.9372\n",
      "Epoch [80/300], Loss: 1.9390\n",
      "Epoch [80/300], Loss: 1.9509\n",
      "Epoch [80/300], Loss: 2.0437\n",
      "Epoch [90/300], Loss: 1.9460\n",
      "Epoch [90/300], Loss: 1.9447\n",
      "Epoch [90/300], Loss: 1.9360\n",
      "Epoch [90/300], Loss: 1.9426\n",
      "Epoch [90/300], Loss: 1.9446\n",
      "Epoch [90/300], Loss: 1.9466\n",
      "Epoch [90/300], Loss: 1.9405\n",
      "Epoch [90/300], Loss: 1.9346\n",
      "Epoch [90/300], Loss: 1.9385\n",
      "Epoch [90/300], Loss: 1.9408\n",
      "Epoch [90/300], Loss: 1.9414\n",
      "Epoch [90/300], Loss: 1.9486\n",
      "Epoch [90/300], Loss: 1.8891\n",
      "Epoch [100/300], Loss: 1.9453\n",
      "Epoch [100/300], Loss: 1.9512\n",
      "Epoch [100/300], Loss: 1.9271\n",
      "Epoch [100/300], Loss: 1.9395\n",
      "Epoch [100/300], Loss: 1.9405\n",
      "Epoch [100/300], Loss: 1.9382\n",
      "Epoch [100/300], Loss: 1.9460\n",
      "Epoch [100/300], Loss: 1.9356\n",
      "Epoch [100/300], Loss: 1.9435\n",
      "Epoch [100/300], Loss: 1.9482\n",
      "Epoch [100/300], Loss: 1.9551\n",
      "Epoch [100/300], Loss: 1.9389\n",
      "Epoch [100/300], Loss: 1.8778\n",
      "Epoch [110/300], Loss: 1.9250\n",
      "Epoch [110/300], Loss: 1.9377\n",
      "Epoch [110/300], Loss: 1.9329\n",
      "Epoch [110/300], Loss: 1.9395\n",
      "Epoch [110/300], Loss: 1.9542\n",
      "Epoch [110/300], Loss: 1.9456\n",
      "Epoch [110/300], Loss: 1.9523\n",
      "Epoch [110/300], Loss: 1.9342\n",
      "Epoch [110/300], Loss: 1.9522\n",
      "Epoch [110/300], Loss: 1.9391\n",
      "Epoch [110/300], Loss: 1.9445\n",
      "Epoch [110/300], Loss: 1.9404\n",
      "Epoch [110/300], Loss: 1.9604\n",
      "Epoch [120/300], Loss: 1.9532\n",
      "Epoch [120/300], Loss: 1.9459\n",
      "Epoch [120/300], Loss: 1.9413\n",
      "Epoch [120/300], Loss: 1.9348\n",
      "Epoch [120/300], Loss: 1.9426\n",
      "Epoch [120/300], Loss: 1.9434\n",
      "Epoch [120/300], Loss: 1.9279\n",
      "Epoch [120/300], Loss: 1.9497\n",
      "Epoch [120/300], Loss: 1.9427\n",
      "Epoch [120/300], Loss: 1.9261\n",
      "Epoch [120/300], Loss: 1.9536\n",
      "Epoch [120/300], Loss: 1.9423\n",
      "Epoch [120/300], Loss: 1.8921\n",
      "Epoch [130/300], Loss: 1.9300\n",
      "Epoch [130/300], Loss: 1.9494\n",
      "Epoch [130/300], Loss: 1.9467\n",
      "Epoch [130/300], Loss: 1.9491\n",
      "Epoch [130/300], Loss: 1.9465\n",
      "Epoch [130/300], Loss: 1.9390\n",
      "Epoch [130/300], Loss: 1.9381\n",
      "Epoch [130/300], Loss: 1.9400\n",
      "Epoch [130/300], Loss: 1.9294\n",
      "Epoch [130/300], Loss: 1.9485\n",
      "Epoch [130/300], Loss: 1.9528\n",
      "Epoch [130/300], Loss: 1.9338\n",
      "Epoch [130/300], Loss: 1.8903\n",
      "Epoch [140/300], Loss: 1.9430\n",
      "Epoch [140/300], Loss: 1.9359\n",
      "Epoch [140/300], Loss: 1.9517\n",
      "Epoch [140/300], Loss: 1.9393\n",
      "Epoch [140/300], Loss: 1.9442\n",
      "Epoch [140/300], Loss: 1.9447\n",
      "Epoch [140/300], Loss: 1.9377\n",
      "Epoch [140/300], Loss: 1.9416\n",
      "Epoch [140/300], Loss: 1.9490\n",
      "Epoch [140/300], Loss: 1.9423\n",
      "Epoch [140/300], Loss: 1.9435\n",
      "Epoch [140/300], Loss: 1.9338\n",
      "Epoch [140/300], Loss: 1.9506\n",
      "Epoch [150/300], Loss: 1.9515\n",
      "Epoch [150/300], Loss: 1.9372\n",
      "Epoch [150/300], Loss: 1.9401\n",
      "Epoch [150/300], Loss: 1.9393\n",
      "Epoch [150/300], Loss: 1.9461\n",
      "Epoch [150/300], Loss: 1.9505\n",
      "Epoch [150/300], Loss: 1.9472\n",
      "Epoch [150/300], Loss: 1.9419\n",
      "Epoch [150/300], Loss: 1.9278\n",
      "Epoch [150/300], Loss: 1.9458\n",
      "Epoch [150/300], Loss: 1.9306\n",
      "Epoch [150/300], Loss: 1.9434\n",
      "Epoch [150/300], Loss: 1.9858\n",
      "Epoch [160/300], Loss: 1.9357\n",
      "Epoch [160/300], Loss: 1.9424\n",
      "Epoch [160/300], Loss: 1.9495\n",
      "Epoch [160/300], Loss: 1.9455\n",
      "Epoch [160/300], Loss: 1.9503\n",
      "Epoch [160/300], Loss: 1.9339\n",
      "Epoch [160/300], Loss: 1.9346\n",
      "Epoch [160/300], Loss: 1.9509\n",
      "Epoch [160/300], Loss: 1.9395\n",
      "Epoch [160/300], Loss: 1.9343\n",
      "Epoch [160/300], Loss: 1.9404\n",
      "Epoch [160/300], Loss: 1.9399\n",
      "Epoch [160/300], Loss: 1.8999\n",
      "Epoch [170/300], Loss: 1.9404\n",
      "Epoch [170/300], Loss: 1.9438\n",
      "Epoch [170/300], Loss: 1.9572\n",
      "Epoch [170/300], Loss: 1.9343\n",
      "Epoch [170/300], Loss: 1.9347\n",
      "Epoch [170/300], Loss: 1.9522\n",
      "Epoch [170/300], Loss: 1.9312\n",
      "Epoch [170/300], Loss: 1.9428\n",
      "Epoch [170/300], Loss: 1.9355\n",
      "Epoch [170/300], Loss: 1.9412\n",
      "Epoch [170/300], Loss: 1.9411\n",
      "Epoch [170/300], Loss: 1.9426\n",
      "Epoch [170/300], Loss: 1.9943\n",
      "Epoch [180/300], Loss: 1.9390\n",
      "Epoch [180/300], Loss: 1.9509\n",
      "Epoch [180/300], Loss: 1.9388\n",
      "Epoch [180/300], Loss: 1.9542\n",
      "Epoch [180/300], Loss: 1.9352\n",
      "Epoch [180/300], Loss: 1.9348\n",
      "Epoch [180/300], Loss: 1.9454\n",
      "Epoch [180/300], Loss: 1.9362\n",
      "Epoch [180/300], Loss: 1.9368\n",
      "Epoch [180/300], Loss: 1.9341\n",
      "Epoch [180/300], Loss: 1.9404\n",
      "Epoch [180/300], Loss: 1.9511\n",
      "Epoch [180/300], Loss: 1.9056\n",
      "Epoch [190/300], Loss: 1.9277\n",
      "Epoch [190/300], Loss: 1.9428\n",
      "Epoch [190/300], Loss: 1.9458\n",
      "Epoch [190/300], Loss: 1.9379\n",
      "Epoch [190/300], Loss: 1.9365\n",
      "Epoch [190/300], Loss: 1.9478\n",
      "Epoch [190/300], Loss: 1.9524\n",
      "Epoch [190/300], Loss: 1.9459\n",
      "Epoch [190/300], Loss: 1.9302\n",
      "Epoch [190/300], Loss: 1.9462\n",
      "Epoch [190/300], Loss: 1.9402\n",
      "Epoch [190/300], Loss: 1.9448\n",
      "Epoch [190/300], Loss: 1.9904\n",
      "Epoch [200/300], Loss: 1.9276\n",
      "Epoch [200/300], Loss: 1.9334\n",
      "Epoch [200/300], Loss: 1.9563\n",
      "Epoch [200/300], Loss: 1.9400\n",
      "Epoch [200/300], Loss: 1.9453\n",
      "Epoch [200/300], Loss: 1.9524\n",
      "Epoch [200/300], Loss: 1.9436\n",
      "Epoch [200/300], Loss: 1.9425\n",
      "Epoch [200/300], Loss: 1.9378\n",
      "Epoch [200/300], Loss: 1.9411\n",
      "Epoch [200/300], Loss: 1.9369\n",
      "Epoch [200/300], Loss: 1.9364\n",
      "Epoch [200/300], Loss: 1.9814\n",
      "Epoch [210/300], Loss: 1.9457\n",
      "Epoch [210/300], Loss: 1.9402\n",
      "Epoch [210/300], Loss: 1.9464\n",
      "Epoch [210/300], Loss: 1.9429\n",
      "Epoch [210/300], Loss: 1.9341\n",
      "Epoch [210/300], Loss: 1.9374\n",
      "Epoch [210/300], Loss: 1.9419\n",
      "Epoch [210/300], Loss: 1.9379\n",
      "Epoch [210/300], Loss: 1.9425\n",
      "Epoch [210/300], Loss: 1.9396\n",
      "Epoch [210/300], Loss: 1.9490\n",
      "Epoch [210/300], Loss: 1.9406\n",
      "Epoch [210/300], Loss: 1.9148\n",
      "Epoch [220/300], Loss: 1.9391\n",
      "Epoch [220/300], Loss: 1.9427\n",
      "Epoch [220/300], Loss: 1.9482\n",
      "Epoch [220/300], Loss: 1.9453\n",
      "Epoch [220/300], Loss: 1.9316\n",
      "Epoch [220/300], Loss: 1.9412\n",
      "Epoch [220/300], Loss: 1.9499\n",
      "Epoch [220/300], Loss: 1.9369\n",
      "Epoch [220/300], Loss: 1.9465\n",
      "Epoch [220/300], Loss: 1.9404\n",
      "Epoch [220/300], Loss: 1.9456\n",
      "Epoch [220/300], Loss: 1.9259\n",
      "Epoch [220/300], Loss: 1.8285\n",
      "Epoch [230/300], Loss: 1.9286\n",
      "Epoch [230/300], Loss: 1.9460\n",
      "Epoch [230/300], Loss: 1.9333\n",
      "Epoch [230/300], Loss: 1.9439\n",
      "Epoch [230/300], Loss: 1.9408\n",
      "Epoch [230/300], Loss: 1.9454\n",
      "Epoch [230/300], Loss: 1.9416\n",
      "Epoch [230/300], Loss: 1.9396\n",
      "Epoch [230/300], Loss: 1.9442\n",
      "Epoch [230/300], Loss: 1.9320\n",
      "Epoch [230/300], Loss: 1.9507\n",
      "Epoch [230/300], Loss: 1.9473\n",
      "Epoch [230/300], Loss: 1.9883\n",
      "Epoch [240/300], Loss: 1.9415\n",
      "Epoch [240/300], Loss: 1.9397\n",
      "Epoch [240/300], Loss: 1.9427\n",
      "Epoch [240/300], Loss: 1.9360\n",
      "Epoch [240/300], Loss: 1.9449\n",
      "Epoch [240/300], Loss: 1.9408\n",
      "Epoch [240/300], Loss: 1.9468\n",
      "Epoch [240/300], Loss: 1.9399\n",
      "Epoch [240/300], Loss: 1.9303\n",
      "Epoch [240/300], Loss: 1.9464\n",
      "Epoch [240/300], Loss: 1.9485\n",
      "Epoch [240/300], Loss: 1.9368\n",
      "Epoch [240/300], Loss: 1.9288\n",
      "Epoch [250/300], Loss: 1.9393\n",
      "Epoch [250/300], Loss: 1.9380\n",
      "Epoch [250/300], Loss: 1.9494\n",
      "Epoch [250/300], Loss: 1.9387\n",
      "Epoch [250/300], Loss: 1.9427\n",
      "Epoch [250/300], Loss: 1.9365\n",
      "Epoch [250/300], Loss: 1.9299\n",
      "Epoch [250/300], Loss: 1.9431\n",
      "Epoch [250/300], Loss: 1.9442\n",
      "Epoch [250/300], Loss: 1.9421\n",
      "Epoch [250/300], Loss: 1.9420\n",
      "Epoch [250/300], Loss: 1.9463\n",
      "Epoch [250/300], Loss: 1.9891\n",
      "Epoch [260/300], Loss: 1.9299\n",
      "Epoch [260/300], Loss: 1.9356\n",
      "Epoch [260/300], Loss: 1.9357\n",
      "Epoch [260/300], Loss: 1.9448\n",
      "Epoch [260/300], Loss: 1.9478\n",
      "Epoch [260/300], Loss: 1.9430\n",
      "Epoch [260/300], Loss: 1.9564\n",
      "Epoch [260/300], Loss: 1.9366\n",
      "Epoch [260/300], Loss: 1.9319\n",
      "Epoch [260/300], Loss: 1.9380\n",
      "Epoch [260/300], Loss: 1.9496\n",
      "Epoch [260/300], Loss: 1.9450\n",
      "Epoch [260/300], Loss: 1.9912\n",
      "Epoch [270/300], Loss: 1.9400\n",
      "Epoch [270/300], Loss: 1.9337\n",
      "Epoch [270/300], Loss: 1.9351\n",
      "Epoch [270/300], Loss: 1.9379\n",
      "Epoch [270/300], Loss: 1.9321\n",
      "Epoch [270/300], Loss: 1.9383\n",
      "Epoch [270/300], Loss: 1.9387\n",
      "Epoch [270/300], Loss: 1.9477\n",
      "Epoch [270/300], Loss: 1.9474\n",
      "Epoch [270/300], Loss: 1.9515\n",
      "Epoch [270/300], Loss: 1.9427\n",
      "Epoch [270/300], Loss: 1.9482\n",
      "Epoch [270/300], Loss: 1.8975\n",
      "Epoch [280/300], Loss: 1.9422\n",
      "Epoch [280/300], Loss: 1.9459\n",
      "Epoch [280/300], Loss: 1.9385\n",
      "Epoch [280/300], Loss: 1.9506\n",
      "Epoch [280/300], Loss: 1.9446\n",
      "Epoch [280/300], Loss: 1.9363\n",
      "Epoch [280/300], Loss: 1.9394\n",
      "Epoch [280/300], Loss: 1.9427\n",
      "Epoch [280/300], Loss: 1.9440\n",
      "Epoch [280/300], Loss: 1.9361\n",
      "Epoch [280/300], Loss: 1.9306\n",
      "Epoch [280/300], Loss: 1.9416\n",
      "Epoch [280/300], Loss: 1.8663\n",
      "Epoch [290/300], Loss: 1.9425\n",
      "Epoch [290/300], Loss: 1.9381\n",
      "Epoch [290/300], Loss: 1.9353\n",
      "Epoch [290/300], Loss: 1.9535\n",
      "Epoch [290/300], Loss: 1.9447\n",
      "Epoch [290/300], Loss: 1.9305\n",
      "Epoch [290/300], Loss: 1.9449\n",
      "Epoch [290/300], Loss: 1.9399\n",
      "Epoch [290/300], Loss: 1.9450\n",
      "Epoch [290/300], Loss: 1.9426\n",
      "Epoch [290/300], Loss: 1.9332\n",
      "Epoch [290/300], Loss: 1.9405\n",
      "Epoch [290/300], Loss: 1.8974\n",
      "Epoch [300/300], Loss: 1.9424\n",
      "Epoch [300/300], Loss: 1.9448\n",
      "Epoch [300/300], Loss: 1.9403\n",
      "Epoch [300/300], Loss: 1.9393\n",
      "Epoch [300/300], Loss: 1.9226\n",
      "Epoch [300/300], Loss: 1.9401\n",
      "Epoch [300/300], Loss: 1.9447\n",
      "Epoch [300/300], Loss: 1.9393\n",
      "Epoch [300/300], Loss: 1.9493\n",
      "Epoch [300/300], Loss: 1.9411\n",
      "Epoch [300/300], Loss: 1.9377\n",
      "Epoch [300/300], Loss: 1.9477\n",
      "Epoch [300/300], Loss: 2.0075\n",
      "Epoch [10/300], Loss: 1.9395\n",
      "Epoch [10/300], Loss: 1.9371\n",
      "Epoch [10/300], Loss: 1.9485\n",
      "Epoch [10/300], Loss: 1.9383\n",
      "Epoch [10/300], Loss: 1.9511\n",
      "Epoch [10/300], Loss: 1.9408\n",
      "Epoch [10/300], Loss: 1.9526\n",
      "Epoch [10/300], Loss: 1.9640\n",
      "Epoch [10/300], Loss: 1.9349\n",
      "Epoch [10/300], Loss: 1.9355\n",
      "Epoch [10/300], Loss: 1.9380\n",
      "Epoch [10/300], Loss: 1.9406\n",
      "Epoch [10/300], Loss: 1.9292\n",
      "Epoch [20/300], Loss: 1.9471\n",
      "Epoch [20/300], Loss: 1.9460\n",
      "Epoch [20/300], Loss: 1.9479\n",
      "Epoch [20/300], Loss: 1.9482\n",
      "Epoch [20/300], Loss: 1.9323\n",
      "Epoch [20/300], Loss: 1.9406\n",
      "Epoch [20/300], Loss: 1.9416\n",
      "Epoch [20/300], Loss: 1.9472\n",
      "Epoch [20/300], Loss: 1.9385\n",
      "Epoch [20/300], Loss: 1.9457\n",
      "Epoch [20/300], Loss: 1.9421\n",
      "Epoch [20/300], Loss: 1.9430\n",
      "Epoch [20/300], Loss: 2.0094\n",
      "Epoch [30/300], Loss: 1.9445\n",
      "Epoch [30/300], Loss: 1.9354\n",
      "Epoch [30/300], Loss: 1.9543\n",
      "Epoch [30/300], Loss: 1.9383\n",
      "Epoch [30/300], Loss: 1.9460\n",
      "Epoch [30/300], Loss: 1.9453\n",
      "Epoch [30/300], Loss: 1.9474\n",
      "Epoch [30/300], Loss: 1.9423\n",
      "Epoch [30/300], Loss: 1.9385\n",
      "Epoch [30/300], Loss: 1.9348\n",
      "Epoch [30/300], Loss: 1.9438\n",
      "Epoch [30/300], Loss: 1.9487\n",
      "Epoch [30/300], Loss: 1.8543\n",
      "Epoch [40/300], Loss: 1.9394\n",
      "Epoch [40/300], Loss: 1.9441\n",
      "Epoch [40/300], Loss: 1.9412\n",
      "Epoch [40/300], Loss: 1.9432\n",
      "Epoch [40/300], Loss: 1.9416\n",
      "Epoch [40/300], Loss: 1.9492\n",
      "Epoch [40/300], Loss: 1.9399\n",
      "Epoch [40/300], Loss: 1.9467\n",
      "Epoch [40/300], Loss: 1.9437\n",
      "Epoch [40/300], Loss: 1.9393\n",
      "Epoch [40/300], Loss: 1.9515\n",
      "Epoch [40/300], Loss: 1.9400\n",
      "Epoch [40/300], Loss: 1.9138\n",
      "Epoch [50/300], Loss: 1.9402\n",
      "Epoch [50/300], Loss: 1.9464\n",
      "Epoch [50/300], Loss: 1.9364\n",
      "Epoch [50/300], Loss: 1.9419\n",
      "Epoch [50/300], Loss: 1.9380\n",
      "Epoch [50/300], Loss: 1.9498\n",
      "Epoch [50/300], Loss: 1.9504\n",
      "Epoch [50/300], Loss: 1.9465\n",
      "Epoch [50/300], Loss: 1.9452\n",
      "Epoch [50/300], Loss: 1.9383\n",
      "Epoch [50/300], Loss: 1.9533\n",
      "Epoch [50/300], Loss: 1.9330\n",
      "Epoch [50/300], Loss: 1.8608\n",
      "Epoch [60/300], Loss: 1.9406\n",
      "Epoch [60/300], Loss: 1.9389\n",
      "Epoch [60/300], Loss: 1.9526\n",
      "Epoch [60/300], Loss: 1.9328\n",
      "Epoch [60/300], Loss: 1.9495\n",
      "Epoch [60/300], Loss: 1.9465\n",
      "Epoch [60/300], Loss: 1.9510\n",
      "Epoch [60/300], Loss: 1.9324\n",
      "Epoch [60/300], Loss: 1.9537\n",
      "Epoch [60/300], Loss: 1.9356\n",
      "Epoch [60/300], Loss: 1.9435\n",
      "Epoch [60/300], Loss: 1.9425\n",
      "Epoch [60/300], Loss: 1.8853\n",
      "Epoch [70/300], Loss: 1.9493\n",
      "Epoch [70/300], Loss: 1.9343\n",
      "Epoch [70/300], Loss: 1.9467\n",
      "Epoch [70/300], Loss: 1.9504\n",
      "Epoch [70/300], Loss: 1.9448\n",
      "Epoch [70/300], Loss: 1.9367\n",
      "Epoch [70/300], Loss: 1.9522\n",
      "Epoch [70/300], Loss: 1.9350\n",
      "Epoch [70/300], Loss: 1.9334\n",
      "Epoch [70/300], Loss: 1.9483\n",
      "Epoch [70/300], Loss: 1.9450\n",
      "Epoch [70/300], Loss: 1.9427\n",
      "Epoch [70/300], Loss: 1.9258\n",
      "Epoch [80/300], Loss: 1.9463\n",
      "Epoch [80/300], Loss: 1.9429\n",
      "Epoch [80/300], Loss: 1.9425\n",
      "Epoch [80/300], Loss: 1.9472\n",
      "Epoch [80/300], Loss: 1.9338\n",
      "Epoch [80/300], Loss: 1.9399\n",
      "Epoch [80/300], Loss: 1.9519\n",
      "Epoch [80/300], Loss: 1.9437\n",
      "Epoch [80/300], Loss: 1.9383\n",
      "Epoch [80/300], Loss: 1.9421\n",
      "Epoch [80/300], Loss: 1.9452\n",
      "Epoch [80/300], Loss: 1.9418\n",
      "Epoch [80/300], Loss: 2.0109\n",
      "Epoch [90/300], Loss: 1.9398\n",
      "Epoch [90/300], Loss: 1.9263\n",
      "Epoch [90/300], Loss: 1.9418\n",
      "Epoch [90/300], Loss: 1.9498\n",
      "Epoch [90/300], Loss: 1.9367\n",
      "Epoch [90/300], Loss: 1.9502\n",
      "Epoch [90/300], Loss: 1.9484\n",
      "Epoch [90/300], Loss: 1.9470\n",
      "Epoch [90/300], Loss: 1.9521\n",
      "Epoch [90/300], Loss: 1.9435\n",
      "Epoch [90/300], Loss: 1.9447\n",
      "Epoch [90/300], Loss: 1.9394\n",
      "Epoch [90/300], Loss: 1.9442\n",
      "Epoch [100/300], Loss: 1.9401\n",
      "Epoch [100/300], Loss: 1.9448\n",
      "Epoch [100/300], Loss: 1.9416\n",
      "Epoch [100/300], Loss: 1.9454\n",
      "Epoch [100/300], Loss: 1.9338\n",
      "Epoch [100/300], Loss: 1.9414\n",
      "Epoch [100/300], Loss: 1.9383\n",
      "Epoch [100/300], Loss: 1.9494\n",
      "Epoch [100/300], Loss: 1.9451\n",
      "Epoch [100/300], Loss: 1.9442\n",
      "Epoch [100/300], Loss: 1.9532\n",
      "Epoch [100/300], Loss: 1.9388\n",
      "Epoch [100/300], Loss: 1.9882\n",
      "Epoch [110/300], Loss: 1.9445\n",
      "Epoch [110/300], Loss: 1.9380\n",
      "Epoch [110/300], Loss: 1.9428\n",
      "Epoch [110/300], Loss: 1.9424\n",
      "Epoch [110/300], Loss: 1.9402\n",
      "Epoch [110/300], Loss: 1.9487\n",
      "Epoch [110/300], Loss: 1.9493\n",
      "Epoch [110/300], Loss: 1.9456\n",
      "Epoch [110/300], Loss: 1.9403\n",
      "Epoch [110/300], Loss: 1.9375\n",
      "Epoch [110/300], Loss: 1.9462\n",
      "Epoch [110/300], Loss: 1.9396\n",
      "Epoch [110/300], Loss: 1.9696\n",
      "Epoch [120/300], Loss: 1.9427\n",
      "Epoch [120/300], Loss: 1.9432\n",
      "Epoch [120/300], Loss: 1.9484\n",
      "Epoch [120/300], Loss: 1.9433\n",
      "Epoch [120/300], Loss: 1.9349\n",
      "Epoch [120/300], Loss: 1.9391\n",
      "Epoch [120/300], Loss: 1.9506\n",
      "Epoch [120/300], Loss: 1.9488\n",
      "Epoch [120/300], Loss: 1.9468\n",
      "Epoch [120/300], Loss: 1.9373\n",
      "Epoch [120/300], Loss: 1.9432\n",
      "Epoch [120/300], Loss: 1.9367\n",
      "Epoch [120/300], Loss: 1.9245\n",
      "Epoch [130/300], Loss: 1.9497\n",
      "Epoch [130/300], Loss: 1.9433\n",
      "Epoch [130/300], Loss: 1.9405\n",
      "Epoch [130/300], Loss: 1.9473\n",
      "Epoch [130/300], Loss: 1.9378\n",
      "Epoch [130/300], Loss: 1.9436\n",
      "Epoch [130/300], Loss: 1.9384\n",
      "Epoch [130/300], Loss: 1.9406\n",
      "Epoch [130/300], Loss: 1.9493\n",
      "Epoch [130/300], Loss: 1.9363\n",
      "Epoch [130/300], Loss: 1.9454\n",
      "Epoch [130/300], Loss: 1.9436\n",
      "Epoch [130/300], Loss: 1.9626\n",
      "Epoch [140/300], Loss: 1.9458\n",
      "Epoch [140/300], Loss: 1.9382\n",
      "Epoch [140/300], Loss: 1.9467\n",
      "Epoch [140/300], Loss: 1.9422\n",
      "Epoch [140/300], Loss: 1.9432\n",
      "Epoch [140/300], Loss: 1.9360\n",
      "Epoch [140/300], Loss: 1.9385\n",
      "Epoch [140/300], Loss: 1.9467\n",
      "Epoch [140/300], Loss: 1.9512\n",
      "Epoch [140/300], Loss: 1.9427\n",
      "Epoch [140/300], Loss: 1.9382\n",
      "Epoch [140/300], Loss: 1.9437\n",
      "Epoch [140/300], Loss: 1.9810\n",
      "Epoch [150/300], Loss: 1.9501\n",
      "Epoch [150/300], Loss: 1.9415\n",
      "Epoch [150/300], Loss: 1.9370\n",
      "Epoch [150/300], Loss: 1.9451\n",
      "Epoch [150/300], Loss: 1.9415\n",
      "Epoch [150/300], Loss: 1.9359\n",
      "Epoch [150/300], Loss: 1.9375\n",
      "Epoch [150/300], Loss: 1.9441\n",
      "Epoch [150/300], Loss: 1.9537\n",
      "Epoch [150/300], Loss: 1.9493\n",
      "Epoch [150/300], Loss: 1.9356\n",
      "Epoch [150/300], Loss: 1.9465\n",
      "Epoch [150/300], Loss: 1.8698\n",
      "Epoch [160/300], Loss: 1.9467\n",
      "Epoch [160/300], Loss: 1.9447\n",
      "Epoch [160/300], Loss: 1.9458\n",
      "Epoch [160/300], Loss: 1.9453\n",
      "Epoch [160/300], Loss: 1.9418\n",
      "Epoch [160/300], Loss: 1.9485\n",
      "Epoch [160/300], Loss: 1.9359\n",
      "Epoch [160/300], Loss: 1.9412\n",
      "Epoch [160/300], Loss: 1.9520\n",
      "Epoch [160/300], Loss: 1.9376\n",
      "Epoch [160/300], Loss: 1.9443\n",
      "Epoch [160/300], Loss: 1.9317\n",
      "Epoch [160/300], Loss: 1.9666\n",
      "Epoch [170/300], Loss: 1.9391\n",
      "Epoch [170/300], Loss: 1.9469\n",
      "Epoch [170/300], Loss: 1.9391\n",
      "Epoch [170/300], Loss: 1.9455\n",
      "Epoch [170/300], Loss: 1.9491\n",
      "Epoch [170/300], Loss: 1.9430\n",
      "Epoch [170/300], Loss: 1.9443\n",
      "Epoch [170/300], Loss: 1.9385\n",
      "Epoch [170/300], Loss: 1.9432\n",
      "Epoch [170/300], Loss: 1.9434\n",
      "Epoch [170/300], Loss: 1.9432\n",
      "Epoch [170/300], Loss: 1.9387\n",
      "Epoch [170/300], Loss: 1.9459\n",
      "Epoch [180/300], Loss: 1.9400\n",
      "Epoch [180/300], Loss: 1.9529\n",
      "Epoch [180/300], Loss: 1.9555\n",
      "Epoch [180/300], Loss: 1.9432\n",
      "Epoch [180/300], Loss: 1.9423\n",
      "Epoch [180/300], Loss: 1.9365\n",
      "Epoch [180/300], Loss: 1.9328\n",
      "Epoch [180/300], Loss: 1.9455\n",
      "Epoch [180/300], Loss: 1.9402\n",
      "Epoch [180/300], Loss: 1.9398\n",
      "Epoch [180/300], Loss: 1.9322\n",
      "Epoch [180/300], Loss: 1.9542\n",
      "Epoch [180/300], Loss: 1.9834\n",
      "Epoch [190/300], Loss: 1.9549\n",
      "Epoch [190/300], Loss: 1.9422\n",
      "Epoch [190/300], Loss: 1.9423\n",
      "Epoch [190/300], Loss: 1.9442\n",
      "Epoch [190/300], Loss: 1.9443\n",
      "Epoch [190/300], Loss: 1.9366\n",
      "Epoch [190/300], Loss: 1.9433\n",
      "Epoch [190/300], Loss: 1.9372\n",
      "Epoch [190/300], Loss: 1.9433\n",
      "Epoch [190/300], Loss: 1.9391\n",
      "Epoch [190/300], Loss: 1.9475\n",
      "Epoch [190/300], Loss: 1.9385\n",
      "Epoch [190/300], Loss: 1.9357\n",
      "Epoch [200/300], Loss: 1.9543\n",
      "Epoch [200/300], Loss: 1.9339\n",
      "Epoch [200/300], Loss: 1.9412\n",
      "Epoch [200/300], Loss: 1.9469\n",
      "Epoch [200/300], Loss: 1.9437\n",
      "Epoch [200/300], Loss: 1.9472\n",
      "Epoch [200/300], Loss: 1.9432\n",
      "Epoch [200/300], Loss: 1.9397\n",
      "Epoch [200/300], Loss: 1.9494\n",
      "Epoch [200/300], Loss: 1.9424\n",
      "Epoch [200/300], Loss: 1.9346\n",
      "Epoch [200/300], Loss: 1.9395\n",
      "Epoch [200/300], Loss: 1.8667\n",
      "Epoch [210/300], Loss: 1.9454\n",
      "Epoch [210/300], Loss: 1.9401\n",
      "Epoch [210/300], Loss: 1.9494\n",
      "Epoch [210/300], Loss: 1.9439\n",
      "Epoch [210/300], Loss: 1.9448\n",
      "Epoch [210/300], Loss: 1.9438\n",
      "Epoch [210/300], Loss: 1.9345\n",
      "Epoch [210/300], Loss: 1.9270\n",
      "Epoch [210/300], Loss: 1.9444\n",
      "Epoch [210/300], Loss: 1.9480\n",
      "Epoch [210/300], Loss: 1.9432\n",
      "Epoch [210/300], Loss: 1.9514\n",
      "Epoch [210/300], Loss: 1.9051\n",
      "Epoch [220/300], Loss: 1.9336\n",
      "Epoch [220/300], Loss: 1.9464\n",
      "Epoch [220/300], Loss: 1.9320\n",
      "Epoch [220/300], Loss: 1.9436\n",
      "Epoch [220/300], Loss: 1.9401\n",
      "Epoch [220/300], Loss: 1.9452\n",
      "Epoch [220/300], Loss: 1.9448\n",
      "Epoch [220/300], Loss: 1.9354\n",
      "Epoch [220/300], Loss: 1.9515\n",
      "Epoch [220/300], Loss: 1.9530\n",
      "Epoch [220/300], Loss: 1.9468\n",
      "Epoch [220/300], Loss: 1.9406\n",
      "Epoch [220/300], Loss: 1.9681\n",
      "Epoch [230/300], Loss: 1.9485\n",
      "Epoch [230/300], Loss: 1.9427\n",
      "Epoch [230/300], Loss: 1.9402\n",
      "Epoch [230/300], Loss: 1.9452\n",
      "Epoch [230/300], Loss: 1.9379\n",
      "Epoch [230/300], Loss: 1.9429\n",
      "Epoch [230/300], Loss: 1.9423\n",
      "Epoch [230/300], Loss: 1.9408\n",
      "Epoch [230/300], Loss: 1.9408\n",
      "Epoch [230/300], Loss: 1.9510\n",
      "Epoch [230/300], Loss: 1.9397\n",
      "Epoch [230/300], Loss: 1.9394\n",
      "Epoch [230/300], Loss: 2.0038\n",
      "Epoch [240/300], Loss: 1.9380\n",
      "Epoch [240/300], Loss: 1.9392\n",
      "Epoch [240/300], Loss: 1.9503\n",
      "Epoch [240/300], Loss: 1.9352\n",
      "Epoch [240/300], Loss: 1.9514\n",
      "Epoch [240/300], Loss: 1.9393\n",
      "Epoch [240/300], Loss: 1.9440\n",
      "Epoch [240/300], Loss: 1.9402\n",
      "Epoch [240/300], Loss: 1.9507\n",
      "Epoch [240/300], Loss: 1.9336\n",
      "Epoch [240/300], Loss: 1.9450\n",
      "Epoch [240/300], Loss: 1.9454\n",
      "Epoch [240/300], Loss: 1.9985\n",
      "Epoch [250/300], Loss: 1.9512\n",
      "Epoch [250/300], Loss: 1.9410\n",
      "Epoch [250/300], Loss: 1.9414\n",
      "Epoch [250/300], Loss: 1.9372\n",
      "Epoch [250/300], Loss: 1.9370\n",
      "Epoch [250/300], Loss: 1.9523\n",
      "Epoch [250/300], Loss: 1.9421\n",
      "Epoch [250/300], Loss: 1.9399\n",
      "Epoch [250/300], Loss: 1.9422\n",
      "Epoch [250/300], Loss: 1.9391\n",
      "Epoch [250/300], Loss: 1.9458\n",
      "Epoch [250/300], Loss: 1.9444\n",
      "Epoch [250/300], Loss: 1.9790\n",
      "Epoch [260/300], Loss: 1.9403\n",
      "Epoch [260/300], Loss: 1.9346\n",
      "Epoch [260/300], Loss: 1.9455\n",
      "Epoch [260/300], Loss: 1.9510\n",
      "Epoch [260/300], Loss: 1.9458\n",
      "Epoch [260/300], Loss: 1.9444\n",
      "Epoch [260/300], Loss: 1.9439\n",
      "Epoch [260/300], Loss: 1.9404\n",
      "Epoch [260/300], Loss: 1.9421\n",
      "Epoch [260/300], Loss: 1.9394\n",
      "Epoch [260/300], Loss: 1.9461\n",
      "Epoch [260/300], Loss: 1.9416\n",
      "Epoch [260/300], Loss: 1.9637\n",
      "Epoch [270/300], Loss: 1.9411\n",
      "Epoch [270/300], Loss: 1.9470\n",
      "Epoch [270/300], Loss: 1.9446\n",
      "Epoch [270/300], Loss: 1.9472\n",
      "Epoch [270/300], Loss: 1.9438\n",
      "Epoch [270/300], Loss: 1.9301\n",
      "Epoch [270/300], Loss: 1.9419\n",
      "Epoch [270/300], Loss: 1.9410\n",
      "Epoch [270/300], Loss: 1.9482\n",
      "Epoch [270/300], Loss: 1.9376\n",
      "Epoch [270/300], Loss: 1.9490\n",
      "Epoch [270/300], Loss: 1.9391\n",
      "Epoch [270/300], Loss: 1.9428\n",
      "Epoch [280/300], Loss: 1.9446\n",
      "Epoch [280/300], Loss: 1.9393\n",
      "Epoch [280/300], Loss: 1.9375\n",
      "Epoch [280/300], Loss: 1.9551\n",
      "Epoch [280/300], Loss: 1.9349\n",
      "Epoch [280/300], Loss: 1.9455\n",
      "Epoch [280/300], Loss: 1.9351\n",
      "Epoch [280/300], Loss: 1.9386\n",
      "Epoch [280/300], Loss: 1.9502\n",
      "Epoch [280/300], Loss: 1.9441\n",
      "Epoch [280/300], Loss: 1.9450\n",
      "Epoch [280/300], Loss: 1.9455\n",
      "Epoch [280/300], Loss: 1.8997\n",
      "Epoch [290/300], Loss: 1.9463\n",
      "Epoch [290/300], Loss: 1.9447\n",
      "Epoch [290/300], Loss: 1.9412\n",
      "Epoch [290/300], Loss: 1.9390\n",
      "Epoch [290/300], Loss: 1.9368\n",
      "Epoch [290/300], Loss: 1.9518\n",
      "Epoch [290/300], Loss: 1.9501\n",
      "Epoch [290/300], Loss: 1.9338\n",
      "Epoch [290/300], Loss: 1.9410\n",
      "Epoch [290/300], Loss: 1.9409\n",
      "Epoch [290/300], Loss: 1.9412\n",
      "Epoch [290/300], Loss: 1.9455\n",
      "Epoch [290/300], Loss: 1.9733\n",
      "Epoch [300/300], Loss: 1.9457\n",
      "Epoch [300/300], Loss: 1.9389\n",
      "Epoch [300/300], Loss: 1.9566\n",
      "Epoch [300/300], Loss: 1.9388\n",
      "Epoch [300/300], Loss: 1.9366\n",
      "Epoch [300/300], Loss: 1.9402\n",
      "Epoch [300/300], Loss: 1.9455\n",
      "Epoch [300/300], Loss: 1.9450\n",
      "Epoch [300/300], Loss: 1.9378\n",
      "Epoch [300/300], Loss: 1.9466\n",
      "Epoch [300/300], Loss: 1.9420\n",
      "Epoch [300/300], Loss: 1.9400\n",
      "Epoch [300/300], Loss: 1.9474\n",
      "Epoch [10/300], Loss: 1.9444\n",
      "Epoch [10/300], Loss: 1.9376\n",
      "Epoch [10/300], Loss: 1.9366\n",
      "Epoch [10/300], Loss: 1.9381\n",
      "Epoch [10/300], Loss: 1.9368\n",
      "Epoch [10/300], Loss: 1.9352\n",
      "Epoch [10/300], Loss: 1.9467\n",
      "Epoch [10/300], Loss: 1.9434\n",
      "Epoch [10/300], Loss: 1.9394\n",
      "Epoch [10/300], Loss: 1.9429\n",
      "Epoch [10/300], Loss: 1.9379\n",
      "Epoch [10/300], Loss: 1.9411\n",
      "Epoch [10/300], Loss: 1.9466\n",
      "Epoch [20/300], Loss: 1.9281\n",
      "Epoch [20/300], Loss: 1.9371\n",
      "Epoch [20/300], Loss: 1.9417\n",
      "Epoch [20/300], Loss: 1.9454\n",
      "Epoch [20/300], Loss: 1.9390\n",
      "Epoch [20/300], Loss: 1.9320\n",
      "Epoch [20/300], Loss: 1.9378\n",
      "Epoch [20/300], Loss: 1.9472\n",
      "Epoch [20/300], Loss: 1.9383\n",
      "Epoch [20/300], Loss: 1.9460\n",
      "Epoch [20/300], Loss: 1.9395\n",
      "Epoch [20/300], Loss: 1.9430\n",
      "Epoch [20/300], Loss: 1.9645\n",
      "Epoch [30/300], Loss: 1.9451\n",
      "Epoch [30/300], Loss: 1.9530\n",
      "Epoch [30/300], Loss: 1.9224\n",
      "Epoch [30/300], Loss: 1.9428\n",
      "Epoch [30/300], Loss: 1.9413\n",
      "Epoch [30/300], Loss: 1.9451\n",
      "Epoch [30/300], Loss: 1.9381\n",
      "Epoch [30/300], Loss: 1.9394\n",
      "Epoch [30/300], Loss: 1.9404\n",
      "Epoch [30/300], Loss: 1.9373\n",
      "Epoch [30/300], Loss: 1.9385\n",
      "Epoch [30/300], Loss: 1.9368\n",
      "Epoch [30/300], Loss: 1.8911\n",
      "Epoch [40/300], Loss: 1.9352\n",
      "Epoch [40/300], Loss: 1.9464\n",
      "Epoch [40/300], Loss: 1.9368\n",
      "Epoch [40/300], Loss: 1.9354\n",
      "Epoch [40/300], Loss: 1.9433\n",
      "Epoch [40/300], Loss: 1.9316\n",
      "Epoch [40/300], Loss: 1.9354\n",
      "Epoch [40/300], Loss: 1.9374\n",
      "Epoch [40/300], Loss: 1.9378\n",
      "Epoch [40/300], Loss: 1.9415\n",
      "Epoch [40/300], Loss: 1.9515\n",
      "Epoch [40/300], Loss: 1.9411\n",
      "Epoch [40/300], Loss: 1.9573\n",
      "Epoch [50/300], Loss: 1.9441\n",
      "Epoch [50/300], Loss: 1.9490\n",
      "Epoch [50/300], Loss: 1.9450\n",
      "Epoch [50/300], Loss: 1.9390\n",
      "Epoch [50/300], Loss: 1.9391\n",
      "Epoch [50/300], Loss: 1.9430\n",
      "Epoch [50/300], Loss: 1.9439\n",
      "Epoch [50/300], Loss: 1.9396\n",
      "Epoch [50/300], Loss: 1.9274\n",
      "Epoch [50/300], Loss: 1.9333\n",
      "Epoch [50/300], Loss: 1.9370\n",
      "Epoch [50/300], Loss: 1.9333\n",
      "Epoch [50/300], Loss: 1.9656\n",
      "Epoch [60/300], Loss: 1.9335\n",
      "Epoch [60/300], Loss: 1.9459\n",
      "Epoch [60/300], Loss: 1.9439\n",
      "Epoch [60/300], Loss: 1.9316\n",
      "Epoch [60/300], Loss: 1.9386\n",
      "Epoch [60/300], Loss: 1.9364\n",
      "Epoch [60/300], Loss: 1.9432\n",
      "Epoch [60/300], Loss: 1.9409\n",
      "Epoch [60/300], Loss: 1.9410\n",
      "Epoch [60/300], Loss: 1.9392\n",
      "Epoch [60/300], Loss: 1.9364\n",
      "Epoch [60/300], Loss: 1.9399\n",
      "Epoch [60/300], Loss: 1.9009\n",
      "Epoch [70/300], Loss: 1.9368\n",
      "Epoch [70/300], Loss: 1.9439\n",
      "Epoch [70/300], Loss: 1.9485\n",
      "Epoch [70/300], Loss: 1.9341\n",
      "Epoch [70/300], Loss: 1.9381\n",
      "Epoch [70/300], Loss: 1.9347\n",
      "Epoch [70/300], Loss: 1.9430\n",
      "Epoch [70/300], Loss: 1.9277\n",
      "Epoch [70/300], Loss: 1.9331\n",
      "Epoch [70/300], Loss: 1.9444\n",
      "Epoch [70/300], Loss: 1.9362\n",
      "Epoch [70/300], Loss: 1.9468\n",
      "Epoch [70/300], Loss: 1.9620\n",
      "Epoch [80/300], Loss: 1.9342\n",
      "Epoch [80/300], Loss: 1.9421\n",
      "Epoch [80/300], Loss: 1.9418\n",
      "Epoch [80/300], Loss: 1.9358\n",
      "Epoch [80/300], Loss: 1.9384\n",
      "Epoch [80/300], Loss: 1.9395\n",
      "Epoch [80/300], Loss: 1.9420\n",
      "Epoch [80/300], Loss: 1.9431\n",
      "Epoch [80/300], Loss: 1.9356\n",
      "Epoch [80/300], Loss: 1.9457\n",
      "Epoch [80/300], Loss: 1.9375\n",
      "Epoch [80/300], Loss: 1.9294\n",
      "Epoch [80/300], Loss: 1.9595\n",
      "Epoch [90/300], Loss: 1.9433\n",
      "Epoch [90/300], Loss: 1.9300\n",
      "Epoch [90/300], Loss: 1.9408\n",
      "Epoch [90/300], Loss: 1.9460\n",
      "Epoch [90/300], Loss: 1.9455\n",
      "Epoch [90/300], Loss: 1.9480\n",
      "Epoch [90/300], Loss: 1.9426\n",
      "Epoch [90/300], Loss: 1.9381\n",
      "Epoch [90/300], Loss: 1.9200\n",
      "Epoch [90/300], Loss: 1.9462\n",
      "Epoch [90/300], Loss: 1.9214\n",
      "Epoch [90/300], Loss: 1.9452\n",
      "Epoch [90/300], Loss: 1.9305\n",
      "Epoch [100/300], Loss: 1.9402\n",
      "Epoch [100/300], Loss: 1.9436\n",
      "Epoch [100/300], Loss: 1.9338\n",
      "Epoch [100/300], Loss: 1.9273\n",
      "Epoch [100/300], Loss: 1.9446\n",
      "Epoch [100/300], Loss: 1.9382\n",
      "Epoch [100/300], Loss: 1.9395\n",
      "Epoch [100/300], Loss: 1.9437\n",
      "Epoch [100/300], Loss: 1.9325\n",
      "Epoch [100/300], Loss: 1.9548\n",
      "Epoch [100/300], Loss: 1.9331\n",
      "Epoch [100/300], Loss: 1.9348\n",
      "Epoch [100/300], Loss: 1.8439\n",
      "Epoch [110/300], Loss: 1.9336\n",
      "Epoch [110/300], Loss: 1.9414\n",
      "Epoch [110/300], Loss: 1.9152\n",
      "Epoch [110/300], Loss: 1.9311\n",
      "Epoch [110/300], Loss: 1.9360\n",
      "Epoch [110/300], Loss: 1.9528\n",
      "Epoch [110/300], Loss: 1.9458\n",
      "Epoch [110/300], Loss: 1.9356\n",
      "Epoch [110/300], Loss: 1.9361\n",
      "Epoch [110/300], Loss: 1.9424\n",
      "Epoch [110/300], Loss: 1.9550\n",
      "Epoch [110/300], Loss: 1.9394\n",
      "Epoch [110/300], Loss: 1.9787\n",
      "Epoch [120/300], Loss: 1.9408\n",
      "Epoch [120/300], Loss: 1.9384\n",
      "Epoch [120/300], Loss: 1.9262\n",
      "Epoch [120/300], Loss: 1.9448\n",
      "Epoch [120/300], Loss: 1.9505\n",
      "Epoch [120/300], Loss: 1.9407\n",
      "Epoch [120/300], Loss: 1.9362\n",
      "Epoch [120/300], Loss: 1.9293\n",
      "Epoch [120/300], Loss: 1.9299\n",
      "Epoch [120/300], Loss: 1.9389\n",
      "Epoch [120/300], Loss: 1.9446\n",
      "Epoch [120/300], Loss: 1.9406\n",
      "Epoch [120/300], Loss: 1.9669\n",
      "Epoch [130/300], Loss: 1.9343\n",
      "Epoch [130/300], Loss: 1.9372\n",
      "Epoch [130/300], Loss: 1.9383\n",
      "Epoch [130/300], Loss: 1.9428\n",
      "Epoch [130/300], Loss: 1.9383\n",
      "Epoch [130/300], Loss: 1.9437\n",
      "Epoch [130/300], Loss: 1.9505\n",
      "Epoch [130/300], Loss: 1.9398\n",
      "Epoch [130/300], Loss: 1.9280\n",
      "Epoch [130/300], Loss: 1.9300\n",
      "Epoch [130/300], Loss: 1.9249\n",
      "Epoch [130/300], Loss: 1.9489\n",
      "Epoch [130/300], Loss: 1.9250\n",
      "Epoch [140/300], Loss: 1.9315\n",
      "Epoch [140/300], Loss: 1.9413\n",
      "Epoch [140/300], Loss: 1.9323\n",
      "Epoch [140/300], Loss: 1.9284\n",
      "Epoch [140/300], Loss: 1.9460\n",
      "Epoch [140/300], Loss: 1.9273\n",
      "Epoch [140/300], Loss: 1.9424\n",
      "Epoch [140/300], Loss: 1.9323\n",
      "Epoch [140/300], Loss: 1.9527\n",
      "Epoch [140/300], Loss: 1.9445\n",
      "Epoch [140/300], Loss: 1.9361\n",
      "Epoch [140/300], Loss: 1.9432\n",
      "Epoch [140/300], Loss: 1.9631\n",
      "Epoch [150/300], Loss: 1.9278\n",
      "Epoch [150/300], Loss: 1.9416\n",
      "Epoch [150/300], Loss: 1.9368\n",
      "Epoch [150/300], Loss: 1.9412\n",
      "Epoch [150/300], Loss: 1.9388\n",
      "Epoch [150/300], Loss: 1.9555\n",
      "Epoch [150/300], Loss: 1.9305\n",
      "Epoch [150/300], Loss: 1.9339\n",
      "Epoch [150/300], Loss: 1.9286\n",
      "Epoch [150/300], Loss: 1.9449\n",
      "Epoch [150/300], Loss: 1.9392\n",
      "Epoch [150/300], Loss: 1.9366\n",
      "Epoch [150/300], Loss: 1.8323\n",
      "Epoch [160/300], Loss: 1.9480\n",
      "Epoch [160/300], Loss: 1.9316\n",
      "Epoch [160/300], Loss: 1.9452\n",
      "Epoch [160/300], Loss: 1.9303\n",
      "Epoch [160/300], Loss: 1.9347\n",
      "Epoch [160/300], Loss: 1.9258\n",
      "Epoch [160/300], Loss: 1.9307\n",
      "Epoch [160/300], Loss: 1.9369\n",
      "Epoch [160/300], Loss: 1.9342\n",
      "Epoch [160/300], Loss: 1.9363\n",
      "Epoch [160/300], Loss: 1.9475\n",
      "Epoch [160/300], Loss: 1.9489\n",
      "Epoch [160/300], Loss: 1.9560\n",
      "Epoch [170/300], Loss: 1.9520\n",
      "Epoch [170/300], Loss: 1.9421\n",
      "Epoch [170/300], Loss: 1.9478\n",
      "Epoch [170/300], Loss: 1.9243\n",
      "Epoch [170/300], Loss: 1.9237\n",
      "Epoch [170/300], Loss: 1.9348\n",
      "Epoch [170/300], Loss: 1.9311\n",
      "Epoch [170/300], Loss: 1.9404\n",
      "Epoch [170/300], Loss: 1.9238\n",
      "Epoch [170/300], Loss: 1.9373\n",
      "Epoch [170/300], Loss: 1.9431\n",
      "Epoch [170/300], Loss: 1.9496\n",
      "Epoch [170/300], Loss: 2.0165\n",
      "Epoch [180/300], Loss: 1.9256\n",
      "Epoch [180/300], Loss: 1.9520\n",
      "Epoch [180/300], Loss: 1.9349\n",
      "Epoch [180/300], Loss: 1.9340\n",
      "Epoch [180/300], Loss: 1.9381\n",
      "Epoch [180/300], Loss: 1.9208\n",
      "Epoch [180/300], Loss: 1.9415\n",
      "Epoch [180/300], Loss: 1.9441\n",
      "Epoch [180/300], Loss: 1.9383\n",
      "Epoch [180/300], Loss: 1.9333\n",
      "Epoch [180/300], Loss: 1.9487\n",
      "Epoch [180/300], Loss: 1.9417\n",
      "Epoch [180/300], Loss: 2.0045\n",
      "Epoch [190/300], Loss: 1.9258\n",
      "Epoch [190/300], Loss: 1.9274\n",
      "Epoch [190/300], Loss: 1.9365\n",
      "Epoch [190/300], Loss: 1.9482\n",
      "Epoch [190/300], Loss: 1.9397\n",
      "Epoch [190/300], Loss: 1.9396\n",
      "Epoch [190/300], Loss: 1.9287\n",
      "Epoch [190/300], Loss: 1.9461\n",
      "Epoch [190/300], Loss: 1.9460\n",
      "Epoch [190/300], Loss: 1.9292\n",
      "Epoch [190/300], Loss: 1.9371\n",
      "Epoch [190/300], Loss: 1.9470\n",
      "Epoch [190/300], Loss: 1.9380\n",
      "Epoch [200/300], Loss: 1.9410\n",
      "Epoch [200/300], Loss: 1.9296\n",
      "Epoch [200/300], Loss: 1.9397\n",
      "Epoch [200/300], Loss: 1.9473\n",
      "Epoch [200/300], Loss: 1.9359\n",
      "Epoch [200/300], Loss: 1.9353\n",
      "Epoch [200/300], Loss: 1.9417\n",
      "Epoch [200/300], Loss: 1.9322\n",
      "Epoch [200/300], Loss: 1.9388\n",
      "Epoch [200/300], Loss: 1.9457\n",
      "Epoch [200/300], Loss: 1.9353\n",
      "Epoch [200/300], Loss: 1.9290\n",
      "Epoch [200/300], Loss: 1.9688\n",
      "Epoch [210/300], Loss: 1.9375\n",
      "Epoch [210/300], Loss: 1.9522\n",
      "Epoch [210/300], Loss: 1.9327\n",
      "Epoch [210/300], Loss: 1.9398\n",
      "Epoch [210/300], Loss: 1.9273\n",
      "Epoch [210/300], Loss: 1.9399\n",
      "Epoch [210/300], Loss: 1.9446\n",
      "Epoch [210/300], Loss: 1.9365\n",
      "Epoch [210/300], Loss: 1.9399\n",
      "Epoch [210/300], Loss: 1.9338\n",
      "Epoch [210/300], Loss: 1.9204\n",
      "Epoch [210/300], Loss: 1.9470\n",
      "Epoch [210/300], Loss: 1.7786\n",
      "Epoch [220/300], Loss: 1.9600\n",
      "Epoch [220/300], Loss: 1.9333\n",
      "Epoch [220/300], Loss: 1.9384\n",
      "Epoch [220/300], Loss: 1.9158\n",
      "Epoch [220/300], Loss: 1.9615\n",
      "Epoch [220/300], Loss: 1.9298\n",
      "Epoch [220/300], Loss: 1.9441\n",
      "Epoch [220/300], Loss: 1.9189\n",
      "Epoch [220/300], Loss: 1.9545\n",
      "Epoch [220/300], Loss: 1.9308\n",
      "Epoch [220/300], Loss: 1.9302\n",
      "Epoch [220/300], Loss: 1.9371\n",
      "Epoch [220/300], Loss: 1.8883\n",
      "Epoch [230/300], Loss: 1.9483\n",
      "Epoch [230/300], Loss: 1.9405\n",
      "Epoch [230/300], Loss: 1.9359\n",
      "Epoch [230/300], Loss: 1.9195\n",
      "Epoch [230/300], Loss: 1.9456\n",
      "Epoch [230/300], Loss: 1.9393\n",
      "Epoch [230/300], Loss: 1.9320\n",
      "Epoch [230/300], Loss: 1.9459\n",
      "Epoch [230/300], Loss: 1.9386\n",
      "Epoch [230/300], Loss: 1.9326\n",
      "Epoch [230/300], Loss: 1.9259\n",
      "Epoch [230/300], Loss: 1.9406\n",
      "Epoch [230/300], Loss: 1.8347\n",
      "Epoch [240/300], Loss: 1.9358\n",
      "Epoch [240/300], Loss: 1.9326\n",
      "Epoch [240/300], Loss: 1.9336\n",
      "Epoch [240/300], Loss: 1.9415\n",
      "Epoch [240/300], Loss: 1.9342\n",
      "Epoch [240/300], Loss: 1.9189\n",
      "Epoch [240/300], Loss: 1.9520\n",
      "Epoch [240/300], Loss: 1.9306\n",
      "Epoch [240/300], Loss: 1.9431\n",
      "Epoch [240/300], Loss: 1.9400\n",
      "Epoch [240/300], Loss: 1.9386\n",
      "Epoch [240/300], Loss: 1.9458\n",
      "Epoch [240/300], Loss: 1.9860\n",
      "Epoch [250/300], Loss: 1.9312\n",
      "Epoch [250/300], Loss: 1.9316\n",
      "Epoch [250/300], Loss: 1.9229\n",
      "Epoch [250/300], Loss: 1.9336\n",
      "Epoch [250/300], Loss: 1.9365\n",
      "Epoch [250/300], Loss: 1.9397\n",
      "Epoch [250/300], Loss: 1.9582\n",
      "Epoch [250/300], Loss: 1.9452\n",
      "Epoch [250/300], Loss: 1.9294\n",
      "Epoch [250/300], Loss: 1.9345\n",
      "Epoch [250/300], Loss: 1.9300\n",
      "Epoch [250/300], Loss: 1.9557\n",
      "Epoch [250/300], Loss: 2.0159\n",
      "Epoch [260/300], Loss: 1.9545\n",
      "Epoch [260/300], Loss: 1.9259\n",
      "Epoch [260/300], Loss: 1.9329\n",
      "Epoch [260/300], Loss: 1.9288\n",
      "Epoch [260/300], Loss: 1.9427\n",
      "Epoch [260/300], Loss: 1.9407\n",
      "Epoch [260/300], Loss: 1.9382\n",
      "Epoch [260/300], Loss: 1.9505\n",
      "Epoch [260/300], Loss: 1.9123\n",
      "Epoch [260/300], Loss: 1.9431\n",
      "Epoch [260/300], Loss: 1.9392\n",
      "Epoch [260/300], Loss: 1.9413\n",
      "Epoch [260/300], Loss: 1.9178\n",
      "Epoch [270/300], Loss: 1.9251\n",
      "Epoch [270/300], Loss: 1.9165\n",
      "Epoch [270/300], Loss: 1.9321\n",
      "Epoch [270/300], Loss: 1.9317\n",
      "Epoch [270/300], Loss: 1.9444\n",
      "Epoch [270/300], Loss: 1.9429\n",
      "Epoch [270/300], Loss: 1.9345\n",
      "Epoch [270/300], Loss: 1.9601\n",
      "Epoch [270/300], Loss: 1.9332\n",
      "Epoch [270/300], Loss: 1.9553\n",
      "Epoch [270/300], Loss: 1.9435\n",
      "Epoch [270/300], Loss: 1.9300\n",
      "Epoch [270/300], Loss: 1.8465\n",
      "Epoch [280/300], Loss: 1.9408\n",
      "Epoch [280/300], Loss: 1.9260\n",
      "Epoch [280/300], Loss: 1.9363\n",
      "Epoch [280/300], Loss: 1.9445\n",
      "Epoch [280/300], Loss: 1.9467\n",
      "Epoch [280/300], Loss: 1.9367\n",
      "Epoch [280/300], Loss: 1.9348\n",
      "Epoch [280/300], Loss: 1.9401\n",
      "Epoch [280/300], Loss: 1.9345\n",
      "Epoch [280/300], Loss: 1.9307\n",
      "Epoch [280/300], Loss: 1.9345\n",
      "Epoch [280/300], Loss: 1.9418\n",
      "Epoch [280/300], Loss: 1.8549\n",
      "Epoch [290/300], Loss: 1.9452\n",
      "Epoch [290/300], Loss: 1.9410\n",
      "Epoch [290/300], Loss: 1.9236\n",
      "Epoch [290/300], Loss: 1.9345\n",
      "Epoch [290/300], Loss: 1.9512\n",
      "Epoch [290/300], Loss: 1.9478\n",
      "Epoch [290/300], Loss: 1.9390\n",
      "Epoch [290/300], Loss: 1.9354\n",
      "Epoch [290/300], Loss: 1.9378\n",
      "Epoch [290/300], Loss: 1.9332\n",
      "Epoch [290/300], Loss: 1.9388\n",
      "Epoch [290/300], Loss: 1.9208\n",
      "Epoch [290/300], Loss: 2.0181\n",
      "Epoch [300/300], Loss: 1.9448\n",
      "Epoch [300/300], Loss: 1.9225\n",
      "Epoch [300/300], Loss: 1.9454\n",
      "Epoch [300/300], Loss: 1.9209\n",
      "Epoch [300/300], Loss: 1.9393\n",
      "Epoch [300/300], Loss: 1.9460\n",
      "Epoch [300/300], Loss: 1.9356\n",
      "Epoch [300/300], Loss: 1.9437\n",
      "Epoch [300/300], Loss: 1.9427\n",
      "Epoch [300/300], Loss: 1.9286\n",
      "Epoch [300/300], Loss: 1.9481\n",
      "Epoch [300/300], Loss: 1.9358\n",
      "Epoch [300/300], Loss: 1.9084\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "#Define Dataset\n",
    "# Encode y_train using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "\n",
    "# Convert X_train_selected and X_test_selected to a tensor\n",
    "# X_train_np = X_train_norm\n",
    "# X_test_np = X_test_norm\n",
    "# X_train_np = X_train.values\n",
    "# X_test_np = X_test.values\n",
    "X_train_np = X_train_selected\n",
    "X_test_np = X_test_selected\n",
    "X_train_torch = torch.tensor(X_train_np).float().requires_grad_(True)\n",
    "X_test_torch = torch.tensor(X_test_np).float().requires_grad_(True)\n",
    "m,n=X_train_torch.shape\n",
    "\n",
    "# Convert y_train_encoded and y_test_encoded to a tensor\n",
    "y_train_torch = torch.tensor(y_train_encoded).long()\n",
    "y_test_torch = torch.tensor(y_test_encoded).long()\n",
    "\n",
    "\n",
    "# Define the DNN model\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, n, n_hidden, n_output, dropout_rate, l2_lambda):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(self.n, self.n_hidden)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.fc3 = nn.Linear(self.n_hidden, self.n_output)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        l2_reg = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)**2\n",
    "        return l2_reg\n",
    "\n",
    "# Define the bootstrapping function\n",
    "def bootstrap(X, y, n_bootstraps):\n",
    "    X_bootstraps = []\n",
    "    y_bootstraps = []\n",
    "    for i in range(n_bootstraps):\n",
    "        idxs = np.random.choice(range(len(X)), size=len(X), replace=True)\n",
    "        X_bootstraps.append(X[idxs])\n",
    "        y_bootstraps.append(y[idxs])\n",
    "    return X_bootstraps, y_bootstraps\n",
    "\n",
    "# Define hyperparameters\n",
    "n_inputs = n\n",
    "n_hidden = 30\n",
    "n_output = 7\n",
    "dropout_rate = 0.2\n",
    "l2_lambda = 0.001\n",
    "n_epochs = 300\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "# Define the model, criterion, optimizer, and learning rate\n",
    "model = DNNModel(n_inputs, n_hidden, n_output, dropout_rate, l2_lambda)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "# Print model summary\n",
    "summary(model, (n_inputs,))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Train the model using bootstrapping\n",
    "n_bootstraps = 10\n",
    "X_bootstraps, y_bootstraps = bootstrap(X_train_torch, y_train_torch, n_bootstraps)\n",
    "\n",
    "# Define empty lists to store F1 scores and epochs\n",
    "f1_scores = []\n",
    "epochs = []\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    X_train_boot = X_bootstraps[i]\n",
    "    y_train_boot = y_bootstraps[i]\n",
    "    dataset = TensorDataset(X_train_boot, y_train_boot)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        # Calculate F1 score for this epoch\n",
    "        y_pred = model(X_test_torch).detach().numpy()\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        f1 = f1_score(y_test_torch, y_pred, average='weighted')\n",
    "        \n",
    "        # Add F1 score and epoch to lists\n",
    "        f1_scores.append(f1)\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss += l2_lambda * model.l2_regularization()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            # Print training loss every 10 epochs\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))\n",
    "\n",
    "# Plot F1 scores against epochs\n",
    "plt.plot(epochs, f1_scores)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Epochs')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "46c04212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        64\n",
      "           1       0.00      0.00      0.00        49\n",
      "           2       0.00      0.00      0.00        43\n",
      "           3       0.00      0.00      0.00        50\n",
      "           4       0.00      0.00      0.00        47\n",
      "           5       0.19      1.00      0.32        74\n",
      "           6       0.00      0.00      0.00        58\n",
      "\n",
      "    accuracy                           0.19       385\n",
      "   macro avg       0.03      0.14      0.05       385\n",
      "weighted avg       0.04      0.19      0.06       385\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mervin\\Desktop\\Python\\Assignment2\\.conda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Mervin\\Desktop\\Python\\Assignment2\\.conda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Mervin\\Desktop\\Python\\Assignment2\\.conda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_torch)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    print(classification_report(y_test_torch.numpy(), predicted.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc51c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "\n",
    "writer = SummaryWriter('runs')\n",
    "writer.add_graph(model, (torch.zeros([1, n_inputs]), ))\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "92c823986b3fa953f4b86b7349397e3d2be04485601d5422eb9395e8cc70ddba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
